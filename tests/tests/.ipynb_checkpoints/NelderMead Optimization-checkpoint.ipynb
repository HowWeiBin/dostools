{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84565b74",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdaeca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:24:02.618640Z",
     "start_time": "2023-02-07T17:23:57.625194Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import time\n",
    "torch.set_default_dtype(torch.float64) \n",
    "%matplotlib notebook\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 10)\n",
    "sys.modules['dostools.src'] = dostools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccbf26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:24:07.538086Z",
     "start_time": "2023-02-07T17:24:02.622076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldos shape is torch.Size([1039, 778])\n",
      "mean dos shape is torch.Size([778])\n",
      "Variance covered with 10 PCs is = 0.9871211778950163\n"
     ]
    }
   ],
   "source": [
    "import dostools.datasets.data as data\n",
    "import dostools.utils.utils as utils\n",
    "\n",
    "n_structures = 1039\n",
    "np.random.seed(0)\n",
    "n_train = int(0.8 * n_structures)\n",
    "train_index = np.arange(n_structures)\n",
    "np.random.shuffle(train_index)\n",
    "test_index = train_index[n_train:]\n",
    "train_index = train_index[:n_train]\n",
    "\n",
    "with torch.no_grad():\n",
    "    structures = data.load_structures(\":\")\n",
    "    n_structures = len(structures) #total number of structures\n",
    "    for structure in structures:#implement periodicity\n",
    "        structure.wrap(eps = 1e-12) \n",
    "    n_atoms = np.zeros(n_structures, dtype = int) #stores number of atoms in each structures\n",
    "    for i in range(n_structures):\n",
    "        n_atoms[i] = len(structures[i])\n",
    "\n",
    "    #eigenergies, emin, emax = dostools.src.datasets.data.load_eigenenergies(unpack = True, n_structures = len(structures))\n",
    "    xdos = torch.tensor(data.load_xdos())\n",
    "    ldos = torch.tensor(data.load_ldos())\n",
    "    ldos *= 2\n",
    "\n",
    "    print (\"ldos shape is {}\".format(ldos.shape))\n",
    "    mean_dos_per_atom = ldos[train_index].mean(axis = 0) #only calculated for train set to prevent data leakage\n",
    "    print (\"mean dos shape is {}\".format(mean_dos_per_atom.shape))\n",
    "    \n",
    "    \n",
    "    y_pw = ldos - mean_dos_per_atom\n",
    "    y_lcdf = torch.cumsum(y_pw, dim = 1)\n",
    "    _, pc_vectors = utils.build_pc(ldos[train_index], mean_dos_per_atom[None,:], n_pc = 10)\n",
    "    y_pc = utils.build_coeffs(ldos - mean_dos_per_atom[None,:], pc_vectors)\n",
    "    Silicon = data.load_features()\n",
    "    kMM = data.load_kMM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c7af2",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfe1751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:24:07.562900Z",
     "start_time": "2023-02-07T17:24:07.540281Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.evaluation.evaluation as evaluation\n",
    "importlib.reload(evaluation)\n",
    "import dostools.models.training as training\n",
    "importlib.reload(training)\n",
    "\n",
    "targets = {\n",
    "    'pw' : ldos,\n",
    "    'lcdf' : y_lcdf,\n",
    "    'pc' : y_pc\n",
    "}\n",
    "evaluator = evaluation.Evaluator(targets, xdos, mean_dos_per_atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec309322",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a2a74",
   "metadata": {},
   "source": [
    "Data Generation will consist of two steps  \n",
    "1. Generate a random alignment \n",
    "2. Use scikitlearn Ridge to determine optimal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab90e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:35:55.386565Z",
     "start_time": "2023-02-07T17:35:55.334331Z"
    }
   },
   "outputs": [],
   "source": [
    "### Define limits for alignment to prevent complete shifts (shifting the spectrum out of the data)\n",
    "\n",
    "def determine_bounds(input_dos, threshold):\n",
    "    boo_dos = input_dos > threshold\n",
    "    left_bound = torch.nonzero(boo_dos)[0]\n",
    "    right_bound = torch.nonzero(boo_dos)[-1]\n",
    "        \n",
    "    return (0-left_bound), (788-right_bound)\n",
    "\n",
    "bounds = []\n",
    "for i in ldos[train_index]:\n",
    "    bounds.append(determine_bounds(i, 1e-10))\n",
    "\n",
    "bounds = torch.tensor(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13b24a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:47:34.306901Z",
     "start_time": "2023-02-07T17:47:34.295849Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import dostools.src.loss.loss as loss\n",
    "import dostools.consistency.consistency as consistency\n",
    "importlib.reload(loss)\n",
    "\n",
    "xdos_step = xdos[1] - xdos[0]\n",
    "#Since it should be alright to overfit shifts\n",
    "def determine_error(normalized_dos):\n",
    "    model = Ridge(alpha = 0, fit_intercept = False, solver = 'svd')\n",
    "    model.fit(Silicon.Features['structure_avekerneldescriptors'][train_index,:], normalized_dos)\n",
    "    \n",
    "    preds = model.predict(Silicon.Features['structure_avekerneldescriptors'][train_index,:])\n",
    "    with torch.no_grad():\n",
    "        rmse = loss.t_get_rmse(torch.tensor(preds), normalized_dos, xdos)\n",
    "        \n",
    "    return rmse\n",
    "        \n",
    "\n",
    "def normalize(ldos, alignment):\n",
    "    shifted_dos = consistency.shifted_ldos(ldos, xdos, alignment)\n",
    "    mean = torch.mean(shifted_dos, dim = 0)\n",
    "    normalized_dos = shifted_dos - mean\n",
    "    \n",
    "    return normalized_dos\n",
    "\n",
    "def generate_sample(bounds, ldos, batch_size):\n",
    "    x_sample = []\n",
    "    for bound in bounds:\n",
    "        col_i = torch.randint(bound[0], bound[1], (batch_size,1))\n",
    "        x_sample.append(col_i)\n",
    "    \n",
    "    x_sample = torch.hstack(x_sample)\n",
    "    \n",
    "    y_sample = []\n",
    "    for x in x_sample:\n",
    "        normalized_dos = normalize(ldos, x)\n",
    "        y = determine_error(normalized_dos)\n",
    "        y_sample.append(y)\n",
    "        \n",
    "        \n",
    "    y_sample = torch.tensor(y_sample)\n",
    "    \n",
    "    return (x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f02a18c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:54:34.758225Z",
     "start_time": "2023-02-07T17:54:32.827342Z"
    }
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'lowest_rmse' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe error is currently :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(rmse))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rmse\n\u001b[0;32m---> 25\u001b[0m rmin \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshift_rmse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m831\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxdos_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNelder-Mead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/_minimize.py:611\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     constraints \u001b[38;5;241m=\u001b[39m standardize_constraints(constraints, x0, meth)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnelder-mead\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_minimize_neldermead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py:750\u001b[0m, in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m fsim \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 750\u001b[0m     fsim[k] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(fsim)\n\u001b[1;32m    753\u001b[0m fsim \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake(fsim, ind, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/optimize/optimize.py:464\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.function_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction_wrapper\u001b[39m(x, \u001b[38;5;241m*\u001b[39mwrapper_args):\n\u001b[1;32m    463\u001b[0m     ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36mshift_rmse\u001b[0;34m(alignment)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mt_get_rmse(torch\u001b[38;5;241m.\u001b[39mtensor(preds), normalized_dos, xdos)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rmse \u001b[38;5;241m<\u001b[39m lowest_rmse:\n\u001b[1;32m     19\u001b[0m         lowest_rmse \u001b[38;5;241m=\u001b[39m rmse\n\u001b[1;32m     20\u001b[0m         best_alignment \u001b[38;5;241m=\u001b[39m alignment\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'lowest_rmse' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "lowest_rmse = 10\n",
    "global lowest_rmse\n",
    "best_alignment = 0\n",
    "global best_alignment\n",
    "\n",
    "def shift_rmse(alignment):\n",
    "    alignment = torch.tensor(alignment)\n",
    "    normalized_dos = normalize(ldos[train_index], alignment) \n",
    "    model = Ridge(alpha = 0, fit_intercept = False, solver = 'lsqr')\n",
    "    model.fit(Silicon.Features['structure_avekerneldescriptors'][train_index,:], normalized_dos)\n",
    "    preds = model.predict(Silicon.Features['structure_avekerneldescriptors'][train_index,:])\n",
    "    with torch.no_grad():\n",
    "        rmse = loss.t_get_rmse(torch.tensor(preds), normalized_dos, xdos)\n",
    "        if rmse < lowest_rmse:\n",
    "            lowest_rmse = rmse\n",
    "            best_alignment = alignment\n",
    "            \n",
    "    print (\"The error is currently :{}\".format(rmse))\n",
    "    return rmse\n",
    "\n",
    "rmin = minimize(fun = shift_rmse, x0 = np.zeros(831), bounds = bounds * xdos_step, method = \"Nelder-Mead\", options ={\"maxiter\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e88878f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:49:09.671583Z",
     "start_time": "2023-02-07T17:49:09.654080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "       20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(831)+20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e18f4629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:40:28.187193Z",
     "start_time": "2023-02-07T17:40:27.442752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6779e-09)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_rmse(np.zeros(831))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eaa249c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T17:48:15.901770Z",
     "start_time": "2023-02-07T17:48:15.895805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5546,  6.0578],\n",
       "        [-7.2594,  7.4596],\n",
       "        [-1.4018,  1.1014],\n",
       "        ...,\n",
       "        [-6.6085,  6.5084],\n",
       "        [-5.4070,  6.5084],\n",
       "        [-3.4545,  5.2067]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds * xdos_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a27e3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "088db521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dostools.src.models.models as models\n",
    "import dostools.src.models.training as training\n",
    "import dostools.src.models.architectures as architecture\n",
    "import dostools.src.loss.loss as loss\n",
    "import torch.nn as nn\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(training)\n",
    "importlib.reload(architecture)\n",
    "importlib.reload(loss)\n",
    "\n",
    "\n",
    "class ShiftErrorModel(nn.Module):\n",
    "    def __init__(self, inputSize, intermediateSize, outputSize):\n",
    "        super(ShiftErrorModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, intermediateSize, bias = True)\n",
    "        self.fc2 = nn.Linear(intermediateSize, intermediateSize)\n",
    "        self.fc3 = nn.Linear(intermediateSize,intermediateSize)\n",
    "        self.fc4 = nn.Linear(intermediateSize, outputSize)\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the transformations to the features based on the model\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): input features\n",
    "        \n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)\n",
    "        out = self.silu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.silu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.silu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1455252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShiftErrorModel(831, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b889b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 806:   1%|▊                                                                                                | 806/100000 [18:17:00<1910:26:00, 69.33s/it, lowest_loss=0.00173, pred_loss=0.00533, trigger=67]"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "batch_size = 16\n",
    "n_epochs = 100000\n",
    "weight_decay = 0\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "threshold = 1000\n",
    "scheduler_threshold = 100\n",
    "tol = 1e-4\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = scheduler_threshold)#0.5)\n",
    "best_state = copy.deepcopy(model.state_dict())\n",
    "lowest_loss = torch.tensor(9999)\n",
    "pred_loss = torch.tensor(0)\n",
    "trigger = 0\n",
    "loss_history =[]\n",
    "pbar = tqdm(range(n_epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(f\"Epoch: {epoch}\")\n",
    "    pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "    opt.zero_grad()\n",
    "    x_data, y_data = generate_sample(bounds, ldos[train_index], batch_size)\n",
    "    pred = model.forward(x_data.double())\n",
    "    pred_loss = loss.t_get_mse(pred.view(-1,1), y_data.view(-1,1))#, self.xdos, perc = True)\n",
    "    new_loss = pred_loss #*1E7\n",
    "    new_loss.backward()\n",
    "    opt.step()\n",
    "    if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "        print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "        model.load_state_dict(best_state)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    if epoch %1000 == 1:\n",
    "        loss_history.append(lowest_loss.item())\n",
    "    if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        lowest_loss = new_loss\n",
    "        trigger = 0\n",
    "    else:\n",
    "        trigger +=1\n",
    "\n",
    "        if trigger > threshold:\n",
    "            weight_decay.load_state_dict(best_state)\n",
    "            for g in opt.param_groups:\n",
    "                g['lr'] = lr\n",
    "            batch_size = batch_size * 8\n",
    "            print (\"Increasing batch_size: {}\".format(lowest_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ebc76",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da43ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b97464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
