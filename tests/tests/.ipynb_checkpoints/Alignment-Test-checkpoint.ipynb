{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84565b74",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdaeca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T11:03:47.884530Z",
     "start_time": "2023-02-06T11:03:45.843937Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "sys.modules['dostools.src'] = dostools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccbf26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T11:03:50.341966Z",
     "start_time": "2023-02-06T11:03:47.886641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldos shape is torch.Size([1039, 778])\n",
      "mean dos shape is torch.Size([778])\n",
      "Variance covered with 10 PCs is = 0.9871211778950163\n"
     ]
    }
   ],
   "source": [
    "import dostools.datasets.data as data\n",
    "import dostools.utils.utils as utils\n",
    "\n",
    "n_structures = 1039\n",
    "np.random.seed(0)\n",
    "n_train = int(0.8 * n_structures)\n",
    "train_index = np.arange(n_structures)\n",
    "np.random.shuffle(train_index)\n",
    "test_index = train_index[n_train:]\n",
    "train_index = train_index[:n_train]\n",
    "\n",
    "with torch.no_grad():\n",
    "    structures = data.load_structures(\":\")\n",
    "    n_structures = len(structures) #total number of structures\n",
    "    for structure in structures:#implement periodicity\n",
    "        structure.wrap(eps = 1e-12) \n",
    "    n_atoms = np.zeros(n_structures, dtype = int) #stores number of atoms in each structures\n",
    "    for i in range(n_structures):\n",
    "        n_atoms[i] = len(structures[i])\n",
    "\n",
    "    #eigenergies, emin, emax = dostools.src.datasets.data.load_eigenenergies(unpack = True, n_structures = len(structures))\n",
    "    xdos = torch.tensor(data.load_xdos())\n",
    "    ldos = torch.tensor(data.load_ldos())\n",
    "    ldos *= 2\n",
    "\n",
    "    print (\"ldos shape is {}\".format(ldos.shape))\n",
    "    mean_dos_per_atom = ldos[train_index].mean(axis = 0) #only calculated for train set to prevent data leakage\n",
    "    print (\"mean dos shape is {}\".format(mean_dos_per_atom.shape))\n",
    "    \n",
    "    \n",
    "    y_pw = ldos - mean_dos_per_atom\n",
    "    y_lcdf = torch.cumsum(y_pw, dim = 1)\n",
    "    _, pc_vectors = utils.build_pc(ldos[train_index], mean_dos_per_atom[None,:], n_pc = 10)\n",
    "    y_pc = utils.build_coeffs(ldos - mean_dos_per_atom[None,:], pc_vectors)\n",
    "    Silicon = data.load_features()\n",
    "    kMM = data.load_kMM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c7af2",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfe1751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T11:03:50.354069Z",
     "start_time": "2023-02-06T11:03:50.343708Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.evaluation.evaluation as evaluation\n",
    "importlib.reload(evaluation)\n",
    "import dostools.models.training as training\n",
    "importlib.reload(training)\n",
    "\n",
    "targets = {\n",
    "    'pw' : ldos,\n",
    "    'lcdf' : y_lcdf,\n",
    "    'pc' : y_pc\n",
    "}\n",
    "evaluator = evaluation.Evaluator(targets, xdos, mean_dos_per_atom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ebc76",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efa3cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T17:16:59.548164Z",
     "start_time": "2023-01-27T17:16:59.507360Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.datasets.dataset as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import dostools.consistency.consistency as consistency\n",
    "\n",
    "device = 'cpu'\n",
    "kwargs = {\"pin_memory\":True} if device == \"cuda:0\" else {}\n",
    "#Dataset\n",
    "y_shifted = y_pw\n",
    "#y_shifted[:100] = consistency.shifted_ldos(y_shifted[:100], xdos, torch.zeros(100)-10)\n",
    "train_data_soap = TensorDataset(Silicon.Features[\"structure_avedescriptors\"][train_index].double(), y_shifted[train_index].double())\n",
    "train_data_kernel = TensorDataset(Silicon.Features[\"structure_avekerneldescriptors\"][train_index].double(), y_shifted[train_index].double())\n",
    "\n",
    "test_data_soap = TensorDataset(Silicon.Features[\"structure_avedescriptors\"][test_index].double(), y_shifted[test_index].double())\n",
    "test_data_kernel = TensorDataset(Silicon.Features[\"structure_avekerneldescriptors\"][test_index].double(), y_shifted[test_index].double())\n",
    "\n",
    "#Dataloader\n",
    "\n",
    "train_dataloader_soap = DataLoader(train_data_soap, batch_size = n_train, shuffle = True, **kwargs)\n",
    "train_dataloader_kernel = DataLoader(train_data_kernel, batch_size = n_train, shuffle = True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7691fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T16:19:17.672020Z",
     "start_time": "2023-01-27T16:19:17.669361Z"
    }
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd3ea42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T17:30:43.824172Z",
     "start_time": "2023-01-27T17:30:43.801134Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.src.consistency.consistency as consistency\n",
    "import dostools.src.loss.loss as loss\n",
    "importlib.reload(loss)\n",
    "importlib.reload(consistency)\n",
    "\n",
    "def t_get_BF_shift_index_mse(prediction, true, shift_range, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        mse = torch.zeros(true.shape[0])\n",
    "        index = torch.zeros(true.shape[0])\n",
    "        for i, pred in enumerate((prediction)):\n",
    "            shifted_preds = consistency.shifted_ldos(pred.repeat(shift_range.shape[0],1), xdos, shift_range)\n",
    "            mse[i], index[i] = torch.min(loss.t_get_each_mse(shifted_preds, true[i].repeat(shift_range.shape[0],1)),0)\n",
    "        mse = torch.mean(mse, 0)\n",
    "        \n",
    "        return mse,index        \n",
    "    else:\n",
    "        raise ValueError(\"xdos not defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfaf523",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a54f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T17:32:53.309190Z",
     "start_time": "2023-01-27T17:32:53.298140Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_opt_BF_shift_rmse(prediction, true, opt_shift, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        rmse = torch.zeros(true.shape[0])\n",
    "        index = torch.zeros(true.shape[0])\n",
    "        shifted_preds = consistency.shifted_ldos(prediction, xdos, opt_shift)\n",
    "        rmse = torch.sqrt(torch.trapezoid((shifted_preds - true)**2, xdos, axis = 1)).mean()\n",
    "\n",
    "        if perc:\n",
    "            mean = true.mean(axis = 0)\n",
    "            std = torch.sqrt(torch.trapezoid((true - mean)**2, xdos, axis = 1)).mean()\n",
    "            loss = (100 * rmse/std)\n",
    "        return loss        \n",
    "    else:\n",
    "        raise ValueError(\"xdos not defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6462d",
   "metadata": {},
   "source": [
    "## Alignment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5bb960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T17:32:53.935041Z",
     "start_time": "2023-01-27T17:32:53.896654Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.src.models.models as models\n",
    "import dostools.src.models.training as training\n",
    "import dostools.src.models.architectures as architecture\n",
    "import dostools.src.loss.loss as loss\n",
    "import torch.nn as nn\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(training)\n",
    "importlib.reload(architecture)\n",
    "importlib.reload(loss)\n",
    "\n",
    "\n",
    "class AlignmentLinearModel(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, train_size, xdos, reg, opt, device):\n",
    "        super(AlignmentLinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(inputSize, outputSize, bias = False)\n",
    "        self.xdos = xdos\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "        self.reg = torch.tensor(reg, requires_grad = False).to(self.device)\n",
    "        self.alignment = torch.zeros(train_size, device = self.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the transformations to the features based on the model\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): input features\n",
    "        \n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "    def fit(self, traindata_loader, valdata_loader, loss, lr ,n_epochs):\n",
    "        \"\"\"\n",
    "        Fits the model based on the training data, early stopping is based on performance on training data (or validation data)\n",
    "        Returns the loss history \n",
    "        \n",
    "        Args:\n",
    "            traindata_loader (DataLoader): Train dataloader\n",
    "            valdata_loader (DataLoader): Validation dataloader\n",
    "            loss (function): Loss function\n",
    "            lr (float): Learning rate\n",
    "            n_epochs (int): Max number of epochs\n",
    "        \n",
    "        Returns:\n",
    "            list: Loss history of the training process\n",
    "        \"\"\"\n",
    "        if self.opt == \"Adam\":\n",
    "            opt = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = self.reg.item())\n",
    "            if valdata_loader is not None:\n",
    "                threshold = 1000\n",
    "                scheduler_threshold = 100\n",
    "            else:\n",
    "                threshold = 1000\n",
    "                scheduler_threshold = 1000\n",
    "            tol = 1e-4\n",
    "        if self.opt == \"LBFGS\":\n",
    "            opt = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            if valdata_loader is not None:\n",
    "                threshold = 2000\n",
    "                scheduler_threshold = 2000\n",
    "            else:\n",
    "                threshold = 30\n",
    "                scheduler_threshold = 5\n",
    "            tol = 1e-2\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = scheduler_threshold)#0.5)\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history =[]\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            if valdata_loader is not None:\n",
    "                pbar.set_postfix(val_loss = lowest_loss.item(), trigger = trigger, train_loss = pred_loss.item())\n",
    "            else:\n",
    "                pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "\n",
    "            for x_data, y_data in traindata_loader:\n",
    "                opt.zero_grad()\n",
    "                x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if self.opt == \"LBFGS\":\n",
    "                    def closure(predictions = False):\n",
    "                        \"\"\"\n",
    "                        Function is necessary for LBFGS, returns the total loss of the model\n",
    "                        \n",
    "                        Args:\n",
    "                            predictions (bool, optional): Returns prediction loss if true, returns total loss if False\n",
    "                        \n",
    "                        Returns:\n",
    "                            tensor: Loss\n",
    "                        \"\"\"\n",
    "                        opt.zero_grad()\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss, self.alignment = t_get_BF_shift_index_mse(_pred, y_data, shift_range, self.xdos)#, self.xdos), perc = True)       \n",
    "                        _pred_loss *= 1e7\n",
    "                        self.alignment = (self.alignment - 20) * (self.xdos[1] - self.xdos[0])\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = torch.sum(torch.pow(self.linear.weight,2))\n",
    "                        _reg_loss *= self.reg.item()\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        # global z \n",
    "                        # z = (torch.sum(abs(self.linear.weight.grad)))\n",
    "                        if predictions:\n",
    "                            return _pred_loss\n",
    "                        return _new_loss\n",
    "                    opt.step(closure)\n",
    "                    #print (z)\n",
    "                    with torch.no_grad():\n",
    "                        pred = self.forward(x_data)\n",
    "                        pred_loss = t_get_opt_BF_shift_rmse(pred, y_data, self.alignment, self.xdos, perc = True)\n",
    "                        reg_loss = torch.sum(torch.pow(self.linear.weight,2))\n",
    "                        reg_loss *= self.reg.item()\n",
    "                        new_loss = pred_loss + reg_loss\n",
    "                    if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                        print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                        self.load_state_dict(best_state)\n",
    "                        opt = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "                    if epoch %10 == 1:\n",
    "                        loss_history.append(lowest_loss.item())\n",
    "                        scheduler.step(new_loss)\n",
    "                elif self.opt == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss(pred, y_data)#, self.xdos, perc = True)\n",
    "                    new_loss = pred_loss\n",
    "                    pred_loss.backward()\n",
    "                    opt.step()\n",
    "                    if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                        print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                        self.load_state_dict(best_state)\n",
    "                        opt = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = self.reg.item())\n",
    "                    if epoch %1000 == 1:\n",
    "                        loss_history.append(lowest_loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if valdata_loader is not None:\n",
    "                    new_loss = torch.zeros(1, requires_grad = False).to(self.device)\n",
    "                    for x_val, y_val in valdata_loader:\n",
    "                        x_val, y_val = x_val.to(self.device), y_val.to(self.device)\n",
    "                        val_pred = self.forward(x_val)\n",
    "                        new_loss += loss(val_pred, y_val, self.xdos, perc = False)\n",
    "\n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss\n",
    "                    trigger = 0\n",
    "\n",
    "                else:\n",
    "                    trigger +=1\n",
    "                    \n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441e750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T17:37:38.673859Z",
     "start_time": "2023-01-27T17:37:26.800637Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 201:   1%|â–ˆ                                                                                                         | 201/20000 [41:19:34<5112:13:37, 929.54s/it, lowest_loss=19, pred_loss=18.9, trigger=1]"
     ]
    }
   ],
   "source": [
    "xdos_step = xdos[1] - xdos[0]\n",
    "shift_range = torch.tensor([x*xdos_step for x in range(-20,20)])\n",
    "M_soap = AlignmentLinearModel(448, 778, n_train, xdos, 1e-11, \"LBFGS\", \"cpu\")\n",
    "loss_history = M_soap.fit(train_dataloader_soap,None, None, 1, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73706139",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_soap.alignment/xdos_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b97464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
