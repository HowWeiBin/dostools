{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee53854f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:41:39.548542Z",
     "start_time": "2023-04-27T11:41:39.542949Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import scipy \n",
    "import copy\n",
    "import ase\n",
    "import ase.io\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "# %matplotlib notebook\n",
    "# matplotlib.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5f011",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af18caf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:40:06.671140Z",
     "start_time": "2023-04-27T11:40:06.558750Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "       \n",
    "    xdos = torch.load(\"./xdos.pt\")\n",
    "    \n",
    "    total_dos3 = torch.load(\"./total_ldos3.pt\")\n",
    "    total_dos1 = torch.load(\"./total_ldos1.pt\")\n",
    "    \n",
    "    surface_dos3 = torch.load(\"./surface_ldos3.pt\")\n",
    "    surface_dos1 = torch.load(\"./surface_ldos1.pt\")\n",
    "    \n",
    "    surface_aligned_dos3 = torch.load(\"./surface_aligned_dos3.pt\")\n",
    "    surface_aligned_dos1 = torch.load(\"./surface_aligned_dos1.pt\")\n",
    "    \n",
    "    bulk_dos3 = torch.load(\"./bulk_ldos3.pt\")\n",
    "    bulk_dos1 = torch.load(\"./bulk_ldos1.pt\")\n",
    "    \n",
    "    total_aligned_dos3 = torch.load(\"./total_aligned_dos3.pt\")\n",
    "    total_aligned_dos1 = torch.load(\"./total_aligned_dos1.pt\")\n",
    "    \n",
    "    surface_soap = torch.load(\"./surface_soap.pt\")\n",
    "    bulk_soap = torch.load(\"./bulk_soap.pt\")\n",
    "    total_soap = torch.load(\"./total_soap.pt\")\n",
    "    \n",
    "    surface_kernel_30 = torch.load(\"./surface_kernel_30.pt\")\n",
    "    surface_kMM_30 = torch.load(\"./surface_kMM_30.pt\")\n",
    "    \n",
    "    bulk_kernel_200 = torch.load(\"./bulk_kernel_200.pt\")\n",
    "    bulk_kMM_200 = torch.load(\"./bulk_kMM_200.pt\")\n",
    "    \n",
    "    bulk_kernel_100 = torch.load(\"./bulk_kernel_100.pt\")\n",
    "    bulk_kMM_100 = torch.load(\"./bulk_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_100 = torch.load(\"./total_kernel_100.pt\")\n",
    "    total_kMM_100 = torch.load(\"./total_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_150 = torch.load(\"./total_kernel_150.pt\")\n",
    "    total_kMM_150 = torch.load(\"./total_kMM_150.pt\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91165f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:40:06.686455Z",
     "start_time": "2023-04-27T11:40:06.673120Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train_test_split(n_samples):\n",
    "    n_structures = n_samples\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_structures)\n",
    "    train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(train_index)\n",
    "    test_index = train_index[n_train:]\n",
    "    train_index = train_index[:n_train]\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "def generate_biased_train_test_split(n_samples):\n",
    "    #Assumes 100 amorphous structures at the end\n",
    "    n_structures = n_samples\n",
    "    amorph_train = np.arange(n_samples-100, n_samples,1)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(amorph_train)\n",
    "    \n",
    "    amorph_test = amorph_train[:80]\n",
    "    amorph_train = amorph_train[80:]\n",
    "\n",
    "    n_structures = n_samples - 100\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_samples)-20\n",
    "    remaining_train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(remaining_train_index)\n",
    "\n",
    "    remaining_test_index = remaining_train_index[n_train:]\n",
    "    remaining_train_index = remaining_train_index[:n_train]\n",
    "\n",
    "    biased_train_index = np.concatenate([remaining_train_index, amorph_train])\n",
    "    biased_test_index = np.concatenate([remaining_test_index, amorph_test])\n",
    "    \n",
    "    return biased_train_index, biased_test_index\n",
    "\n",
    "def generate_surface_holdout_split(n_samples):\n",
    "    #Assumes that we are using the 110 surfaces for test which are located at 673 + 31st-57th index\n",
    "    #26 structures\n",
    "    \n",
    "    n_test = int(0.2 * n_samples) - 26\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    remaining_indexes = np.concatenate([np.arange(673+31), np.arange(673+57,n_samples,1)])\n",
    "    indexes_110 = np.arange(673+31, 673+57,1)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    np.random.shuffle(remaining_indexes)\n",
    "    \n",
    "    remaining_test_index = remaining_indexes[n_train:]\n",
    "    remaining_train_index = remaining_indexes[:n_train]\n",
    "    \n",
    "    total_train_index = remaining_train_index\n",
    "    total_test_index = np.concatenate([remaining_test_index, indexes_110])\n",
    "    \n",
    "    return total_train_index, total_test_index\n",
    "    \n",
    "def surface_holdout(n_samples):\n",
    "    test_index = np.arange(31,57,1)\n",
    "    train_index = np.concatenate([np.arange(31), np.arange(57, n_samples)])\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "n_surfaces = 154\n",
    "n_bulkstructures = 773\n",
    "n_total_structures = 773 + 154\n",
    "\n",
    "\n",
    "surface_train_index, surface_test_index = generate_train_test_split(n_surfaces)\n",
    "bulk_train_index, bulk_test_index = generate_train_test_split(n_bulkstructures)\n",
    "total_train_index, total_test_index = generate_train_test_split(n_total_structures)\n",
    "surface_holdout_train_index, surface_holdout_test_index = surface_holdout(n_surfaces)\n",
    "bulk_biased_train_index, bulk_biased_test_index = generate_biased_train_test_split(n_bulkstructures)\n",
    "total_biased_train_index, total_biased_test_index = generate_biased_train_test_split(n_total_structures)\n",
    "holdout_train_index, holdout_test_index = generate_surface_holdout_split(n_total_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ed621084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:22:54.212973Z",
     "start_time": "2023-04-29T19:22:54.205952Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve, correlate, correlation_lags\n",
    "def find_optimal_discrete_shift(prediction, true):\n",
    "    extension = true.shape[1] - prediction.shape[1]\n",
    "    correction = extension/2\n",
    "    if true.shape[0] == prediction.shape[0] and len(prediction.shape) == 2:\n",
    "        shift = []\n",
    "        for i in range(true.shape[0]):\n",
    "            corr = correlate(true[i], prediction[i], mode='full')\n",
    "            shift_i = np.argmax(corr) - len(true[i]) + correction + 1   \n",
    "            shift.append(shift_i)\n",
    "        \n",
    "        \n",
    "    elif true.shape[0] == prediction.shape[0] and len(prediction.shape) == 1:\n",
    "        corr = correlate(true, prediction, mode='full')\n",
    "        shift = np.argmax(corr) - len(true) + correction + 1   \n",
    "    else:\n",
    "        print (\"input shapes are not the same\")\n",
    "        raise Exception\n",
    "    return torch.tensor(shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3277c55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:14:19.718245Z",
     "start_time": "2023-04-29T19:14:19.622571Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing if i need to extend the bounds\n",
    "extended_target = \n",
    "original_target = shifted_ldos_discrete(shifted_target, xdos, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7afc58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:40:07.222042Z",
     "start_time": "2023-04-27T11:40:07.212353Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate shifted data\n",
    "def shifted_ldos_discrete(ldos, xdos, shift): \n",
    "    shifted_ldos = torch.zeros_like(ldos)\n",
    "    if len(ldos.shape) > 1:\n",
    "        xdos_shift = torch.round(shift).int()\n",
    "        for i in range(len(ldos)):\n",
    "            if xdos_shift[i] > 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,:-1*xdos_shift[i]], (xdos_shift[i],0))\n",
    "            elif xdos_shift[i] < 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,(-1*xdos_shift[i]):], (0,(-1*xdos_shift[i])))\n",
    "            else:\n",
    "                shifted_ldos[i] = ldos[i]\n",
    "    else:        \n",
    "        xdos_shift = int(torch.round(shift))\n",
    "        if xdos_shift > 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[:-1*xdos_shift], (xdos_shift,0))\n",
    "        elif xdos_shift < 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[(-1*xdos_shift):], (0,(-1*xdos_shift)))\n",
    "        else:\n",
    "            shifted_ldos = ldos\n",
    "    return shifted_ldos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "983908a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:29:49.658457Z",
     "start_time": "2023-04-29T19:29:49.650114Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_total_dos3 = torch.nn.functional.pad(total_dos3, (600,600), value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "741e19b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:29:50.857012Z",
     "start_time": "2023-04-29T19:29:50.196198Z"
    }
   },
   "outputs": [],
   "source": [
    "# true = torch.randint(low = -200, high = 200, size = (927,))\n",
    "true = torch.zeros(927) - 700\n",
    "shifted_target = shifted_ldos_discrete(total_dos3, xdos, true) \n",
    "shift = find_optimal_discrete_shift(shifted_target, reference_total_dos3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d286b4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:29:51.001658Z",
     "start_time": "2023-04-29T19:29:50.858926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff54c0d0370>]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEDCAYAAADZUdTgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWvklEQVR4nO3df6zd9X3f8efL1zYBAiGJbygDHNMJJSNdoOzKDSHiR7sQO0pqdeo0W/nVKMijCtPaaZlAnUDt/loyTVUTiGWlLksXQE0DiZU5QNRfRMlYuE6B2CQmDpDgOo0NtKHNjxHDe3+cr8PZ5ZzvPVyf63v8vc+HdHTP+Xx/vQ9cv/zx+3zP95uqQpK0PKxY6gIkScePoS9Jy4ihL0nLiKEvScuIoS9Jy4ihL0nLyMSGfpIdSQ4l2TOGfV2U5H8n2ZvkoST/pm/Zp5LsS7KnOeaqYz2eJE2qiQ194BZgw5j29SPgvVX1hmafv5/kjGbZp4DXA/8cOBm4ekzHlKSJM7GhX1X3Ak/3jyX5p0nuSrI7yZeSvH7EfT1SVd9qnh8EDgHTzetd1QC+Cpwz1jciSRNkYkN/iO3Av6uqfwH8R+Dml7qDJOuB1cC354yvAt4D3DWGOiVpIq1c6gJGleTlwJuBTyc5OnxSs+xfAb83YLO/qaq39e3jLOCPgfdV1fNz1r0ZuLeqvjTu2iVpUpwwoU/vXyV/X1UXzV1QVXcAd7RtnOR04H8B/7mq7puz7EZ67Z5/O7ZqJWkCnTDtnap6Bngsyb8GSM+Fo2ybZDVwJ/DJqvr0nGVXA28DtgyY/UtSp2RSr7KZ5DbgCmAN8H3gRuDPgY8DZwGrgNuralBbZ+6+3g38EbC3b/g3quqBJEeA7wD/0IzfMco+JelENLGhL0kavxOmvSNJOnYT+UHumjVrat26dUtdhiSdMHbv3v1kVU3Pt95Ehv66deuYnZ1d6jIk6YSR5DujrGd7R5KWEUNfkpYRQ1+SlhFDX5KWEUNfkpaRec/eSbIDeAdwqKp+YcDyDwHv6tvfPwOmq+rpJI/T+6brc8CRqpoZV+GSpJdulJn+LbTczKSqPlJVFzUXQrse+Kuq6r8O/pXNcgNfkpbYvKE/6GYmLbYAtx1TRcfgo3/2Lf7qkcNLdXhJmnhj6+knOYXevwg+0zdcwD3Nna62zrP91iSzSWYPH15YcN/8l9/my/ufXNC2krQcjPOD3HcCX57T2rm0qi4GNgIfTHLZsI2rantVzVTVzPT0vN8kHsoLyEnScOMM/c3Mae0096Olqg7Ru579+jEe70USMPMlabixhH6SVwCXA5/rGzs1yWlHnwNXAXvGcbyhdSzmziWpA0Y5ZfNnNzNJcoDezUxWAVTVtma1XwPuqaof9m16JnBncz/blcCtVbXoNx13oi9Jw80b+lW1ZYR1bqF3amf/2KPASLczHJcktnckqUWnvpFre0eS2nUq9AHKBo8kDdWt0PfsHUlq1anQt70jSe06FfqSpHadCv3m9FBJ0hCdCn3wMgyS1KZToZ/45SxJatOt0F/qAiRpwnUq9MFTNiWpTadCP4lfzpKkFt0K/aUuQJImXKdCH2zvSFKbToW+p+lLUrtOhT54yqYktelY6Hs9fUlq06nQt70jSe06Ffo9TvUlaZhOhX7w7B1JajNv6CfZkeRQkj1Dll+R5AdJHmgeN/Qt25BkX5L9Sa4bZ+GDa1nsI0jSiW2Umf4twIZ51vlSVV3UPH4PIMkUcBOwEbgA2JLkgmMpdhTO9CVpuHlDv6ruBZ5ewL7XA/ur6tGqeha4Hdi0gP2MLHgZBklqM66e/iVJHkzyhSRvaMbOBp7oW+dAMzZQkq1JZpPMHj58eEFF2N6RpHbjCP2vAa+tqguBjwKfbcYHRfDQaXhVba+qmaqamZ6eXnAxtnckabhjDv2qeqaq/rF5vgtYlWQNvZn9uX2rngMcPNbjtXGiL0ntjjn0k/xcmpvTJlnf7PMp4H7g/CTnJVkNbAZ2Huvx5uNEX5KGWznfCkluA64A1iQ5ANwIrAKoqm3ArwO/meQI8GNgc/VuVHskybXA3cAUsKOq9i7Ku3ihVts7ktRi3tCvqi3zLP8Y8LEhy3YBuxZWmiRp3Dr1jVzAUzYlqUWnQj/Bpr4ktehe6EuShupU6IMTfUlq06nQj2fqS1KrToU+QHnOpiQN1anQT2zvSFKbboX+UhcgSROuU6EPXnBNktp0KvST2N6RpBbdCv2lLkCSJlynQh88e0eS2nQr9D17R5JadSr0be9IUrtOhT7gVF+SWnQq9OMV1ySpVadCH7yeviS16VToB7+cJUltuhX6dnckqdW8oZ9kR5JDSfYMWf6uJA81j68kubBv2eNJvp7kgSSz4yx8GGf6kjTcKDP9W4ANLcsfAy6vqjcC/wXYPmf5lVV1UVXNLKzE0YXY05ekFivnW6Gq7k2yrmX5V/pe3gecM4a6FsT2jiS1G3dP/wPAF/peF3BPkt1Jto75WAPZ3pGk4ead6Y8qyZX0Qv8tfcOXVtXBJK8Bvpjkm1V175DttwJbAdauXTuusiRJfcYy00/yRuATwKaqeuroeFUdbH4eAu4E1g/bR1Vtr6qZqpqZnp5ecC1O9CVpuGMO/SRrgTuA91TVI33jpyY57ehz4Cpg4BlA45LE9o4ktZi3vZPkNuAKYE2SA8CNwCqAqtoG3AC8Gri5uQzCkeZMnTOBO5uxlcCtVXXXIryHF2pdzJ1LUgeMcvbOlnmWXw1cPWD8UeDCF2+x2JzqS9IwnftGru0dSRquc6EvSRquU6EPNnckqU2nQj/Ee+RKUotuhb7tHUlq1anQB9s7ktSmU6HvRF+S2nUq9MFTNiWpTbdCP7G9I0ktOhX6tnckqV2nQh/wlE1JatGp0PeUTUlq163QX+oCJGnCdSr0wbN3JKlNp0I/9nckqVWnQh+gPGlTkobqVOgH2zuS1KZboW93R5JadSr0wZm+JLXpVOiH2NOXpBbzhn6SHUkOJdkzZHmS/EGS/UkeSnJx37INSfY1y64bZ+GDi130I0jSCW2Umf4twIaW5RuB85vHVuDjAEmmgJua5RcAW5JccCzFjsL2jiQNN2/oV9W9wNMtq2wCPlk99wFnJDkLWA/sr6pHq+pZ4PZm3UUTvImKJLUZR0//bOCJvtcHmrFh4wMl2ZpkNsns4cOHF1SIZ+9IUrtxhP6gqK2W8YGqantVzVTVzPT09MKrcaovSUOtHMM+DgDn9r0+BzgIrB4yvmhig0eSWo1jpr8TeG9zFs+bgB9U1feA+4Hzk5yXZDWwuVl3UXnKpiQNN+9MP8ltwBXAmiQHgBuBVQBVtQ3YBbwd2A/8CHh/s+xIkmuBu4EpYEdV7V2E99BXq2fvSFKbeUO/qrbMs7yADw5ZtoveXwrHhR/kSlK7Tn0jF+zoS1KbToV+iPfIlaQW3Qp92zuS1KpToQ+2dySpTedCX5I0XOdC35a+JA3XqdBPYntHklp0K/SXugBJmnCdCn3A/o4ktehU6CeevSNJbboV+ktdgCRNuE6FPtjdkaQ2nQr93tk7pr4kDdOt0F/qAiRpwnUq9MH2jiS16VToe8E1SWrXqdAHZ/qS1KZjoe9lGCSpTadC3/aOJLUbKfSTbEiyL8n+JNcNWP6hJA80jz1JnkvyqmbZ40m+3iybHfcbmMs7Z0nScPPeGD3JFHAT8FbgAHB/kp1V9fDRdarqI8BHmvXfCfx2VT3dt5srq+rJsVY+qNbFPoAkneBGmemvB/ZX1aNV9SxwO7CpZf0twG3jKO6lsr0jSe1GCf2zgSf6Xh9oxl4kySnABuAzfcMF3JNkd5Ktww6SZGuS2SSzhw8fHqGswezuSNJwo4T+oPnzsGh9J/DlOa2dS6vqYmAj8MEklw3asKq2V9VMVc1MT0+PUNagQp3qS1KbUUL/AHBu3+tzgIND1t3MnNZOVR1sfh4C7qTXLlo0XntHkoYbJfTvB85Pcl6S1fSCfefclZK8Argc+Fzf2KlJTjv6HLgK2DOOwgdJbO9IUpt5z96pqiNJrgXuBqaAHVW1N8k1zfJtzaq/BtxTVT/s2/xM4M70PmFdCdxaVXeN8w3084NcSWo3b+gDVNUuYNecsW1zXt8C3DJn7FHgwmOq8CVyoi9Jw3XrG7nEL2dJUotOhb4n70hSu26FPrZ3JKlNp0I/YOpLUotuhb6n70hSq06FPjjRl6Q2nQp95/mS1K5ToQ9eT1+S2nQq9BPbO5LUpluhv9QFSNKE61Togxdck6Q2nQr9JF5aWZJadCv0l7oASZpwnQp9sL0jSW26FfpO9SWpVbdCH2f6ktSmU6HvjdElqV23Qt/Ml6RWnQp98DIMktRmpNBPsiHJviT7k1w3YPkVSX6Q5IHmccOo245T8DIMktRm3hujJ5kCbgLeChwA7k+ys6oenrPql6rqHQvcdixs70hSu1Fm+uuB/VX1aFU9C9wObBpx/8ey7YLY3ZGk4UYJ/bOBJ/peH2jG5rokyYNJvpDkDS9xW5JsTTKbZPbw4cMjlDVgH3gZBklqM0roD2qazE3WrwGvraoLgY8Cn30J2/YGq7ZX1UxVzUxPT49Q1oBCbe9IUqtRQv8AcG7f63OAg/0rVNUzVfWPzfNdwKoka0bZdtxs70jScKOE/v3A+UnOS7Ia2Azs7F8hyc+luSt5kvXNfp8aZdtxcqYvSe3mPXunqo4kuRa4G5gCdlTV3iTXNMu3Ab8O/GaSI8CPgc3VO2F+4LaL9F569S7mziXpBDdv6MPPWja75oxt63v+MeBjo267eGJ7R5JadOobubZ3JKldp0K/x6m+JA3TqdAPnr0jSW26Ffq2dySpVadCH2zuSFKbToW+N1GRpHadCn3wevqS1KZToZ/Y3pGkNt0K/aUuQJImXKdCHzxlU5LadCr0k9jTl6QWHQt9Z/qS1KZToT+V8JypL0lDdSv0V4Tnnjf0JWkYQ1+SlpHuhb7tHUkaqlOhvyK9m6h4Bo8kDdap0J9a0ft6li0eSRqsm6HvTF+SBhop9JNsSLIvyf4k1w1Y/q4kDzWPryS5sG/Z40m+nuSBJLPjLH6uFc0F9Z9/fjGPIkknrnlvjJ5kCrgJeCtwALg/yc6qerhvtceAy6vq75JsBLYDv9S3/MqqenKMdQ801fwV5kxfkgYbZaa/HthfVY9W1bPA7cCm/hWq6itV9XfNy/uAc8Zb5mimVvTeznPPGfqSNMgooX828ETf6wPN2DAfAL7Q97qAe5LsTrL1pZc4uqnmMpvO9CVpsHnbOwy+YvHAVE1yJb3Qf0vf8KVVdTDJa4AvJvlmVd07YNutwFaAtWvXjlDWi3n2jiS1G2WmfwA4t+/1OcDBuSsleSPwCWBTVT11dLyqDjY/DwF30msXvUhVba+qmaqamZ6eHv0d9FnRhP7zzvQlaaBRQv9+4Pwk5yVZDWwGdvavkGQtcAfwnqp6pG/81CSnHX0OXAXsGVfxc03Fmb4ktZm3vVNVR5JcC9wNTAE7qmpvkmua5duAG4BXAzenF7xHqmoGOBO4sxlbCdxaVXctyjvhhZm+oS9Jg43S06eqdgG75oxt63t+NXD1gO0eBS6cO75Yjs70be9I0mCd+kbuyiln+pLUplOhv8KeviS16lToe+0dSWrXqdB3pi9J7ToV+kdn+l5wTZIG61jo937a3pGkwToV+rZ3JKldp0J/ysswSFKrTob+ES+tLEkDdSv0/UauJLXqVuh77R1JatWp0F/hl7MkqVWnQv9n7R1n+pI0ULdC3/aOJLXqVOh7nr4ktetU6J+8egqAnxx5bokrkaTJ1KnQP+1lvXvCPPPjI0tciSRNpk6G/j/85KdLXIkkTaZOhf5JK6c4aeUKnvmJM31JGqRToQ9w+smrnOlL0hAjhX6SDUn2Jdmf5LoBy5PkD5rlDyW5eNRtx+30l620py9JQ8wb+kmmgJuAjcAFwJYkF8xZbSNwfvPYCnz8JWw7VmtefhLfffpHi3kISTphrRxhnfXA/qp6FCDJ7cAm4OG+dTYBn6yqAu5LckaSs4B1I2w7Vpe/bpoP37WPyz78F5y0snPdK0kd9spTVvMn11yyqMcYJfTPBp7oe30A+KUR1jl7xG0BSLKV3r8SWLt27QhlDfb+N5/HD//vEZ54+scc8b6Jkk4gp79s1aIfY5TQz4CxuV95HbbOKNv2Bqu2A9sBZmZmFvyV2pNXT/Ght71+oZtLUqeNEvoHgHP7Xp8DHBxxndUjbCtJOk5GaXrfD5yf5Lwkq4HNwM456+wE3tucxfMm4AdV9b0Rt5UkHSfzzvSr6kiSa4G7gSlgR1XtTXJNs3wbsAt4O7Af+BHw/rZtF+WdSJLmlZrAG47MzMzU7OzsUpchSSeMJLurama+9TynUZKWEUNfkpYRQ1+SlhFDX5KWkYn8IDfJYeA7C9x8DfDkGMsZJ2tbGGtbuEmuz9oWZlhtr62q6fk2nsjQPxZJZkf5BHspWNvCWNvCTXJ91rYwx1qb7R1JWkYMfUlaRroY+tuXuoAW1rYw1rZwk1yftS3MMdXWuZ6+JGm4Ls70JUlDGPqStIx0JvSP9w3Yh9SwI8mhJHv6xl6V5ItJvtX8fGXfsuubevcledsi1nVukr9I8o0ke5P8+wmq7WVJvprkwaa2352U2vqON5Xkr5N8fgJrezzJ15M8kGR2kuprbpv6p0m+2fzuXTIJtSV5XfPf6+jjmSS/NQm1Ncf67ebPwp4ktzV/RsZXW1Wd8A96l23+NvDz9G7c8iBwwRLUcRlwMbCnb+zDwHXN8+uA/9o8v6Cp8yTgvKb+qUWq6yzg4ub5acAjzfEnobYAL2+erwL+D/CmSaitr8b/ANwKfH5S/p/21fY4sGbO2ETUB/wP4Orm+WrgjEmpra/GKeBvgddOQm30bjH7GHBy8/pPgN8YZ22L+h/0eD2AS4C7+15fD1y/RLWs4/8P/X3AWc3zs4B9g2qkd8+BS45TjZ8D3jpptQGnAF+jdx/liaiN3t3e/gz4ZV4I/YmorTnG47w49Je8PuD0JrwyabXNqecq4MuTUhsv3Ff8VfTud/L5psax1daV9s6wG7NPgjOrdxcxmp+vacaXpOYk64BfpDejnojamvbJA8Ah4ItVNTG1Ab8P/Cfg+b6xSakNevecvifJ7iRbJ6i+nwcOA3/UtMY+keTUCamt32bgtub5ktdWVX8D/Dfgu8D36N2F8J5x1taV0B/5BuwT5LjXnOTlwGeA36qqZ9pWHTC2aLVV1XNVdRG9WfX6JL/Qsvpxqy3JO4BDVbV71E0GjC327+GlVXUxsBH4YJLLWtY9nvWtpNfq/HhV/SLwQ3ptiWGW4s/DauBXgU/Pt+qAscX6nXslsIleq+afAKcmefc4a+tK6I9y8/al8v0kZwE0Pw8148e15iSr6AX+p6rqjkmq7aiq+nvgL4ENE1LbpcCvJnkcuB345ST/c0JqA6CqDjY/DwF3AusnpL4DwIHmX20Af0rvL4FJqO2ojcDXqur7zetJqO1fAo9V1eGq+ilwB/DmcdbWldCf5Buw7wTe1zx/H71++tHxzUlOSnIecD7w1cUoIEmAPwS+UVX/fcJqm05yRvP8ZHq/9N+chNqq6vqqOqeq1tH7nfrzqnr3JNQGkOTUJKcdfU6v97tnEuqrqr8FnkjyumboV4CHJ6G2Plt4obVztIalru27wJuSnNL8uf0V4BtjrW2xPyg5Xg96N2Z/hN6n17+zRDXcRq8P91N6fwN/AHg1vQ8Cv9X8fFXf+r/T1LsP2LiIdb2F3j/5HgIeaB5vn5Da3gj8dVPbHuCGZnzJa5tT5xW88EHuRNRGr2/+YPPYe/T3foLquwiYbf7ffhZ45QTVdgrwFPCKvrFJqe136U189gB/TO/MnLHV5mUYJGkZ6Up7R5I0AkNfkpYRQ1+SlhFDX5KWEUNfkpYRQ1+SlhFDX5KWkf8H6y5LNXYiTOsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = torch.argmax(abs(true + shift))\n",
    "plt.plot(shifted_target[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8c385977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:29:51.538966Z",
     "start_time": "2023-04-29T19:29:51.535435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1445cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:40:07.245887Z",
     "start_time": "2023-04-27T11:40:07.224015Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_mse(a, b, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=1)).mean()\n",
    "        else:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=0)).mean()\n",
    "        if not perc:\n",
    "            return mse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.trapezoid((b - mean)**2, xdos, axis=1).mean()\n",
    "            return (100 * mse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = ((a - b)**2).mean(dim = 1)\n",
    "        else:\n",
    "            mse = ((a - b)**2).mean()\n",
    "        if len(mse.shape) > 1:\n",
    "            raise ValueError('Loss became 2D')\n",
    "        if not perc:\n",
    "            return torch.mean(mse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (mse / b.std(dim=0, unbiased = True)),0)\n",
    "        \n",
    "        \n",
    "def t_get_rmse(a, b, xdos=None, perc=False): #account for the fact that DOS is continuous but we are training them pointwise\n",
    "    \"\"\" computes  Root Mean Squared Error (RMSE) of array properties (DOS/aofd).\n",
    "         a=pred, b=target, xdos, perc: if False return RMSE else return %RMSE\"\"\"\n",
    "    #MIGHT NOT WORK FOR PC\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=1)).mean())\n",
    "        else:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=0)).mean())\n",
    "        if not perc:\n",
    "            return rmse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.sqrt((torch.trapezoid((b - mean)**2, xdos, axis=1)).mean())\n",
    "            return (100 * rmse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean(dim =0))\n",
    "        else:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean())\n",
    "        if not perc:\n",
    "            return torch.mean(rmse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (rmse / b.std(dim = 0,unbiased=True)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c110556b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:52:30.714126Z",
     "start_time": "2023-04-27T11:52:30.672805Z"
    }
   },
   "outputs": [],
   "source": [
    "from dostools.loss import loss\n",
    "\n",
    "def normal_reg_train_L(feat, target, train_index, test_index, regularization, n_epochs, lr):\n",
    "    \n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "    reg_features = torch.vstack([Features, reg])\n",
    "    reg_target = torch.vstack([Target, torch.zeros(n_col,Target.shape[1])])\n",
    "    \n",
    "    weights = torch.nn.Parameter(torch.rand(Features.shape[1], Target.shape[1])- 0.5)\n",
    "    opt = torch.optim.LBFGS([weights], lr = lr, line_search_fn = \"strong_wolfe\", tolerance_grad = 1e-20, tolerance_change = 1-20, history_size = 200)\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            pred_i = reg_features @ weights\n",
    "            opt_shift = find_optimal_discrete_shift(np.array(pred_i[:len(index)].detach()),np.array(reg_target[:len(index)].detach()))\n",
    "            pred_i[:len(index)] = shifted_ldos_discrete(pred_i[:len(index)], xdos, torch.tensor(opt_shift))\n",
    "            loss_i = loss.t_get_mse(pred_i, reg_target)\n",
    "            loss_i.backward()\n",
    "            return loss_i\n",
    "        opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            opt_shift = find_optimal_discrete_shift(np.array(preds),np.array(Target))\n",
    "            preds = shifted_ldos_discrete(preds, xdos, torch.tensor(opt_shift))\n",
    "            epoch_rmse = loss.t_get_rmse(preds, Target, xdos, perc = True)\n",
    "            epoch_mse = loss.t_get_mse(preds, Target, xdos)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            pred_loss = epoch_rmse\n",
    "\n",
    "            if epoch_mse < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            epoch_mse = prev_loss\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "        loss_dos = loss.t_get_rmse(final_preds, Target, xdos, perc = True)\n",
    "        test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target, xdos, perc = True)\n",
    "        return best_state, loss_dos, test_loss_dos\n",
    "        \n",
    "\n",
    "def normal_reg_train_Ad(feat, target, train_index, test_index, regularization, n_epochs, batch_size, lr):\n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "\n",
    "    Sampler = torch.utils.data.RandomSampler(index, replacement = False)\n",
    "    Batcher = torch.utils.data.BatchSampler(Sampler, batch_size, False)\n",
    "\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "\n",
    "\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "\n",
    "\n",
    "    # reg_features = torch.vstack([Features, reg])\n",
    "    # reg_target = torch.vstack([Target, torch.zeros(n_col,Target.shape[1])])\n",
    "\n",
    "\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "\n",
    "\n",
    "    weights = torch.nn.Parameter((torch.rand(Features.shape[1], Target.shape[1])- 0.5))\n",
    "    opt = torch.optim.Adam([weights], lr = lr, weight_decay = 0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = 500, threshold = 1e-7, min_lr = 1e-8)\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        for i_batch in Batcher:\n",
    "            def closure():\n",
    "                opt.zero_grad()\n",
    "                reg_features_i = torch.vstack([Features[i_batch], reg])\n",
    "                target_i = torch.vstack([Target[i_batch], torch.zeros(n_col, Target.shape[1])])\n",
    "                pred_i = reg_features_i @ weights\n",
    "                opt_shift = find_optimal_discrete_shift(np.array(pred_i[:len(i_batch)].detach()),np.array(target_i[:len(i_batch)].detach()))\n",
    "                pred_i[:len(i_batch)] = shifted_ldos_discrete(pred_i[:len(i_batch)], xdos, torch.tensor(opt_shift))\n",
    "                loss_i = loss.t_get_mse(pred_i, target_i)\n",
    "                loss_i.backward()\n",
    "                return loss_i\n",
    "            opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            opt_shift = find_optimal_discrete_shift(np.array(preds),np.array(Target))\n",
    "            preds = shifted_ldos_discrete(preds, xdos, torch.tensor(opt_shift))\n",
    "            epoch_rmse = loss.t_get_rmse(preds, Target, xdos, perc = True)\n",
    "            epoch_mse = loss.t_get_mse(preds, Target, xdos)\n",
    "\n",
    "\n",
    "            pred_loss = epoch_rmse\n",
    "\n",
    "            if epoch_mse < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            epoch_mse = prev_loss\n",
    "\n",
    "            scheduler.step(epoch_mse)\n",
    "\n",
    "            if Batcher.batch_size > 1024:\n",
    "                break\n",
    "\n",
    "            if opt.param_groups[0]['lr'] < 1e-4:\n",
    "                Batcher.batch_size *= 2\n",
    "                opt.param_groups[0]['lr'] = lr\n",
    "                print (\"The batch_size is now: \", Batcher.batch_size)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "        loss_dos = loss.t_get_rmse(final_preds, Target, xdos, perc = True)\n",
    "        test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target, xdos, perc = True)\n",
    "        return best_state, loss_dos, test_loss_dos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b0e0691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:06:30.488240Z",
     "start_time": "2023-04-29T19:06:30.472540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 125,  114,  123, -162,  257,  191, -262, -252,   18,  221,   53, -124,\n",
       "         101,  227, -262,  -71,  -25,  182,   -8, -247,  269,  -58,  208, -252,\n",
       "         -94,  208,  -69,    2,   79,   98,   90,  207,  289,  -42,  193,   58,\n",
       "          43,  -70, -246, -219, -193, -213,  268, -270,  281,  220,  263, -205,\n",
       "          83,  -50, -160, -180,  137,  141,  -70,  190,   42,   91,  144,   10,\n",
       "         -39,  113,  196,  -62,  -97,  -73,  100, -172,  -65,  234, -235, -168,\n",
       "         -84, -223,  192,  253,  -64, -195,  -75,   34,   85,   45, -101,  -57,\n",
       "          52,  154,  237,  -42,  -10,  115,  -35,  -67, -239,  -49,   46,  -27,\n",
       "        -138, -286, -268,  213, -216, -198,  167,  165,  -64, -102,  -60, -225,\n",
       "         244,  144, -242,  287, -162, -261,  102,  145,  151, -226,  220,   39,\n",
       "         -61,  264,  286,   76,  246,  155, -242,  134,  215,  298,  197,  186,\n",
       "         261,   67,  273,  242, -225,  282, -277, -142,  155,  -88,  -78, -116,\n",
       "          90,  -70,   60,  136, -103,  218,   85,  205, -239,  219,  -68, -193,\n",
       "         -15,  -90,   61, -235,  126,   67,   94,  -20,   80,   37,   51, -104,\n",
       "         -64,  251, -250, -219, -252, -240, -223,   58, -168,  -74, -201,  242,\n",
       "        -274,  -77,   20,  -85,    8,  -24,    3,  193, -277,   65,  251, -136,\n",
       "         -91,  296, -238,  279,  169, -272,   52,   70,  133, -249,   30, -134,\n",
       "         176,    4, -247,  255,  259,   62,  294,   41, -269,    7,   62,  -43,\n",
       "          72, -239,   94,  -67,  287,  193,  215,  -14,  -21,  -82, -231,  172,\n",
       "         170, -217, -207,   53, -246,  254,  -70,  263, -294,  267, -263, -161,\n",
       "         180,   36, -266, -263,  154,   97,  -92,  236, -177,    0, -253,  137,\n",
       "         139,   52,  297,  259,  294,  109,   84,  239,   50, -181, -128,    1,\n",
       "        -254,  191, -116, -290,  157, -200,  143, -103,  190, -234,   83, -234,\n",
       "        -162, -297, -251,  191, -131,  242, -253,  141, -288,  134, -186,  168,\n",
       "        -295, -106,   80,  232,   97,  266,  117,  173,   75,   97,  -97,  197,\n",
       "         -45, -153,  -16,  -80,  275, -236, -190,  116,  214, -294,  -81,  192,\n",
       "          98, -283,  274, -105,  -26, -131, -257,  -92, -297,   10,   67,   42,\n",
       "        -207, -125,   28,  292, -169,   11, -263,   48,   26,  225, -278, -278,\n",
       "         124, -211,  156, -153,  126,  271,   26, -174, -178,   68,  130,   98,\n",
       "        -197,  167,  -87,   43,   -2,   39,  152, -121,  287,   30,  166,  -81,\n",
       "        -247, -109,   48,   24, -168, -296,  124,   -9,   27, -251,  -63, -163,\n",
       "           2, -258,  128, -292,  216,  -51, -160,  173, -175,  -10,  -74,   77,\n",
       "        -237,   71, -240, -267, -128,  118, -151,   -6, -217,  246,  107, -280,\n",
       "         -85,  -72,  161,   52, -169, -112,  -79, -116,  180, -128, -171,  -94,\n",
       "        -178, -166,  -85,   85,  227,  166,   52,   16,  241, -133, -242,   87,\n",
       "           6, -214,   82,   83, -117,  250,   33, -127, -281, -252,  107,  -72,\n",
       "         287, -129, -215, -131,  -37,  265,  159,  193, -101,   25,  241,  283,\n",
       "         -53, -201,  246, -142,  208, -295,  120,   19, -194, -239,  -72,   21,\n",
       "          91, -159,  -80,    8,  244,  117,  -80, -184, -165, -260,  255,   15,\n",
       "         -85,  209,  251, -112,  132, -295, -267,  217,  158,  108,  181,  237,\n",
       "          54,  -13,  -50,  -74, -205,  -96, -278,  195,    1, -266, -172,  112,\n",
       "         236, -287,  255,  140,  -52,  126, -226,  133, -134,  185,  255, -113,\n",
       "          40,  156, -148,  204,  247,  236,  -96,  183,   50,  -64,   94,  275,\n",
       "        -204, -236,   87,  -32,  250, -266,   33,  240,   61,  147,  177, -134,\n",
       "         120,  274,   90, -181, -289,   62,  197, -124, -116,  169,  293,  -81,\n",
       "         -14,   -9,  171,  151, -168,  103,  298,  208,   47,  251, -182,  152,\n",
       "        -287, -265,   37,  -26, -147,  188,  121,  193,   78, -255,    6,  151,\n",
       "         228,  162,  156, -208,   48,  291, -202, -239,   16,  155,  -15,  -31,\n",
       "        -168,  -43,  222, -181,  273,   38,  191,   11,  147,  -97,  -84, -296,\n",
       "         112,  -69,  226,  274,  239, -231,  109,  -80,  212,  128,  -94, -230,\n",
       "          30,    4,  111,  176,  111, -263, -188, -141, -113,  -82,   47, -110,\n",
       "         171,  189,  299, -111, -169,  294,  253, -125,  111,  151,  -35, -137,\n",
       "          29, -148, -133, -287,  144, -154, -117, -178, -247, -199,   -8,  154,\n",
       "         265,  -58,   68, -283,   58,  208,  228,  256, -131,  202,  -31, -290,\n",
       "        -131,  250,    5, -210, -187, -151,  -83,  203,  220,   -6, -108,   52,\n",
       "        -221,   -4, -235,  197,   40,   66,  167,  263,  185,  -83,  135,  203,\n",
       "         -69,  181,  266,  -65,   18,  240,  235,  -54, -104, -139,   32,  265,\n",
       "         239,  271,  -42,  108,  214,    9,  201,   16,  -23, -132, -161,  -72,\n",
       "          53, -297, -139,  115,  145,  248, -238, -140,   58,  297, -205,  109,\n",
       "           3, -165, -229, -153, -164,  272, -168,   44,  -33,  226,   71,  159,\n",
       "         -56,  229,  245, -185,  151,    7, -253, -143,  240,  240,   42, -194,\n",
       "          36,  188,  159,   14,   68,   -6, -105,  -18,  124, -194,  295,  285,\n",
       "        -103, -155,  150, -236,   -4,   50,  265,   -6, -257, -275,  185,   -6,\n",
       "           1, -231,  207, -157,  234, -199,  -68,  169,   -8,   73, -157,  116,\n",
       "        -176,  211, -272,  153,  109,  155,   30,  -28,  130,  110,   44,  158,\n",
       "        -140, -132,  -16,  233, -188, -201, -194, -157,   46,  156,  -70, -279,\n",
       "         184,  -33,  227,   -3,   74,  -18,  107,   31, -132, -266,   59,  196,\n",
       "         140, -267, -164,   26, -122,  284, -293,   20, -217,  -46,  224,   70,\n",
       "        -276,  156,  221, -293,  189, -287,  157,  247,   -7, -218,  283,  -76,\n",
       "         -97,   -5, -248,  245, -207,  101, -241, -202,   -4,  167, -128, -253,\n",
       "         -38,  -36,  175, -276,   69,  222,  287, -265, -116,  210,   68,  147,\n",
       "         180,  292,  291,  193, -144, -158,  154,  277, -274,   57,   82,  228,\n",
       "          25,   99,   84, -251, -140,  -39,  145, -242,   75,  154, -131, -121,\n",
       "        -228,  -96, -224, -297, -264,   14,  -51,  267,  -10,  -56, -291,  268,\n",
       "         135,   79, -118,  173, -270,  227,  197,  217,  137,  144, -215,  150,\n",
       "         -19,  186, -165,  -14,   61,  268, -145,  215, -180,   34,  -71,   58,\n",
       "        -189, -116,   37,   60, -150, -141,  186,  279,  298,  123,  -57, -231,\n",
       "        -299, -128,  292])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "549ade0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:13:12.595426Z",
     "start_time": "2023-04-29T19:13:12.586686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-176,  -62,  115,  297,  275, -169,  101,  270, -184,  -79,   -8, -123,\n",
       "         174, -208, -225, -247, -114,   -2, -257,  214,  161, -116,    4, -142,\n",
       "         -40, -228,  238,   45, -290, -145,   76,  -50,  162,  -55,  220, -167,\n",
       "         296,  101, -176,  -20,  -63,   85, -278,  182,  169, -131,  169,  247,\n",
       "         243,  206, -271,  -61, -194,  289,  270,  175,  239, -267, -124, -103,\n",
       "         250,  166,  -48,  280,  150, -200,  143, -105,  173,  130,  -37,  143,\n",
       "         201,  249,  267,   95,   70, -180,  109,  280,  262,  181,   23,  128,\n",
       "         257,  270, -215,   14,  -25,  174,   89,    8, -213,  -19,  180,  292,\n",
       "           5, -294,  208,  174,  137,  194,  176,  177, -293,  265,  152, -140,\n",
       "         289,  238, -242,   78, -148,  280,  247, -221,  -95,    6, -263, -179,\n",
       "        -143, -173,   54,  254,   68, -115,   31,  234,  -93,  248,  -78,  257,\n",
       "         -69, -253,  290,  -82, -199, -202,  159, -299,  282,  239,  160, -151,\n",
       "         122,  130, -178,  183, -247,  211,  187,  214,  161,  243,  142, -124,\n",
       "         130, -122,  226,   90, -190,   72,  236,  180, -197,  -41, -260, -219,\n",
       "         198,  277,  190, -163,   93,  211, -101, -228, -133,   78,  268,  239,\n",
       "         -29,  -12,  -54,   77,  -32,  257,  129,   50,   19,   65,  244, -169,\n",
       "        -189,   11,   -9, -163,  -89, -193,  226, -238,   85,   93,  -20,  200,\n",
       "        -177,   50, -247, -107, -196, -121, -168, -217,  -38,  269,   78, -160,\n",
       "         250,  -30,   96,  -61,  163, -114,  -90,   88, -186, -180, -255, -276,\n",
       "         109,  -66,   24,  -36,  -45, -182,   42,  249,    8,  266,   31, -118,\n",
       "          45,  145, -142, -225,  189,  269,  264,  232, -194,  -59, -271, -110,\n",
       "         276, -231,    6,   82, -154,  133,  119,   34,  259,   66,  211,  -47,\n",
       "          71, -211,   60,  164, -218, -157,   36,  173, -116,  162,  269,  109,\n",
       "        -167,   59,  249,   93,   34,    3, -160,  -45, -247, -100,   41, -290,\n",
       "         253, -115,   27,  233,  204,  -41,   56,  123,   99, -188,   88,    9,\n",
       "         258, -264,  -49,  -60,   40,  189,  -79, -268,  285,  242,  275,  -73,\n",
       "         -26, -268,  283,  -88,   46,  224,  281,  198,  162, -227, -291, -237,\n",
       "          71, -108, -247,   85, -209,   43,  131,  271,  287,   75,  169,  -21,\n",
       "        -167,   27,   22,  297, -269,  245,  -82,  127, -246,  217,  -29,  117,\n",
       "        -102,  -23, -184,  254,   88,  257,  141,  202,  143,  296,  133,  264,\n",
       "         159,   76,  230, -164,  221, -236,   68, -178,   95,  186, -102,  217,\n",
       "         289, -279,  132,  108, -262,  262,   61,  212,   12, -184, -137, -247,\n",
       "          96,   13,  110,  169,  207,  149, -286,  276, -296,  -17, -263,  294,\n",
       "        -295,  262,  -48,  -24,  272,  -86, -238,   88, -135,   21,  105,  104,\n",
       "        -217,  -45,  102,  -59,    9, -154,   89,  199,  270,  173,   39,  -92,\n",
       "           9,   68,   22,  -19, -234, -160,   64,  167,  104,   -5,  149,  105,\n",
       "         139,  225,   10, -221,  -99,   -4,  -48,  170, -198, -104,  297,  112,\n",
       "          51,   99,  165,   22,  -91,  -11,  -44,  -61,  208, -111,  224, -263,\n",
       "        -293,  -69,  -43,  253,  -82, -230, -131, -289,  -39,  222, -116,   82,\n",
       "         162, -168, -237,  105,   43,   95,  293,  -86,  -44,  -49,  104,  -57,\n",
       "         243,  252,  -10,  -77,  -63, -110,   -4, -120,  191,  217,  -94,  245,\n",
       "         -44,  -20, -273, -199, -105,  298,   83,  154,  246, -161,  -28,  110,\n",
       "         -38,   96,   56, -106,   58,  -48, -113,  109,  128,  248, -182,  147,\n",
       "         229,   -7, -297, -118, -284,   -6,  -69, -105,   -3,   20,  -63,  171,\n",
       "         225, -231,  104,  264, -264,  -73,   43,  256,   29,  -84,  247,  268,\n",
       "          35,  -18, -278,  177,  263, -254,  108, -296,   10, -262, -222, -140,\n",
       "          15, -199, -206,  230,  -53, -256,  193, -246,  212, -260,  263,   18,\n",
       "          16, -230,  132,  252,  240,  199,  -86,  132,   62, -138,  268, -179,\n",
       "        -224,   14,  165, -179,   84,  238, -213,  125,  283, -276,  220,   70,\n",
       "        -116,  137,   96,  271,  -20,  275,  296,  -51, -157,  -23,  -62,   88,\n",
       "         158,  296, -291,  -56,  -56, -265, -229,  -34,  -31,  138,   63, -129,\n",
       "         -29,  256,   44, -224,  -16,  260, -254,  236, -155,  129,  184, -161,\n",
       "         194,  231,   42, -203,  -14, -140,  245,  274,  205,   27,  280,  165,\n",
       "         291,   50, -124,  -95,  -55,  190,  226,   11, -228,  -37, -197,  266,\n",
       "        -175, -165,  152,  221,  275,  209,  144, -266,  -44,  -40,   85, -158,\n",
       "          10,  -37,  109, -215,   88, -106,  -75, -111, -273,   71,  201,   63,\n",
       "         -67,   97, -136,  -43,  -69, -274,   50, -162,  186,   14, -169, -289,\n",
       "          88,  158, -193,  263,  122, -249,   15,  237,  140,  133,  246,   99,\n",
       "         -31, -132,  229, -257,   82,   13,  222,  -51, -220, -189,  277,   50,\n",
       "         217,   87, -101,  279,  -84, -279,  182, -253, -225, -252, -283, -228,\n",
       "          90, -229,  142, -281,   -2,  290,  128, -252,   72, -140,  264,  102,\n",
       "        -282,   17, -219,  -86, -180,  -60,   30,  276, -114,  203,  226,  -99,\n",
       "         -71,  124,  295,  298, -167,  247,  272,   52,  220,   28,  -63,  291,\n",
       "         -84,  149, -135, -103,   74,  164,  160, -260, -189, -239,  103,   64,\n",
       "         -87, -112, -129,   67,   49,  132,  132, -281, -140, -262,   61,   97,\n",
       "        -213,   -6,   59, -203,  293,  218,  -38, -287,  162, -183,   63,  -34,\n",
       "          96, -155,  121,  -53, -110,   -1,  129, -280, -236,  197, -108,   49,\n",
       "        -200,  226, -300,  -83,  191,  169,  122, -262, -151,  126, -220, -255,\n",
       "          66,  214,  149,   85,  206, -252, -196,   77,  270,   -7,  -62,  210,\n",
       "         222,  -88, -211,  -96,  121,  -42,   56, -261, -162,  277,  190,   83,\n",
       "        -266,  -39, -100,  183, -253,  144,   67,  106, -158, -144,  236,  -64,\n",
       "        -143,  193,  185, -288,  -73,   85,  -24,  208, -247,   57,   62, -153,\n",
       "        -131,   16,  135,   36,  253,  154,   77,  -38, -246, -249,  194,  -89,\n",
       "          13, -205,  137, -296,  287,  126,  241,  244,  177,  146, -111, -266,\n",
       "         107, -260, -300,  253, -104,  267,    4,  -65, -129,  -63,  232,  139,\n",
       "        -165,  282,   -6,  262, -116,  230, -259, -272, -140,  161, -189,  171,\n",
       "         114,  129,  -16,  159,   24,  -31, -238, -112, -102,  221, -134,  -45,\n",
       "         121,  -29,   55])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba92d314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:20:38.384406Z",
     "start_time": "2023-04-29T19:20:38.375754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "39e926ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:23:28.142193Z",
     "start_time": "2023-04-29T19:23:28.133480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 173, -150,  -95,  -76,  207,  -62,  -53,  113,  -69,   -7,  172,  288,\n",
       "         294,  251,  162,  -52, -132, -278, -146,  217,  157,  190,   67, -113,\n",
       "          32, -100,  198,   76,  -98,  243,  181,  -32,  139, -113,  198,    4,\n",
       "          68,   97, -213, -176,  157,   81,  -94,   82,  -42, -149, -204,   31,\n",
       "           9,  -56,  147, -149,   41, -251, -229,   83,  281, -134,  211,  279,\n",
       "        -157,  228, -291,   35,  270,   78,  251, -280,  173, -117, -256,    7,\n",
       "         294,  270,   17,  222, -163, -245,  -45,  109,  -33,  132,  231,  186,\n",
       "         265,  106, -264,  258,   79,  120,  182,  113,   55,  285,  273, -126,\n",
       "         242, -145, -189, -159,  152,    3,  128,  227, -218,  221,  -94, -231,\n",
       "          59, -138,   73, -218,  267, -203, -224, -197, -158, -219,   41,  -30,\n",
       "          88,   87,  152, -182,   94,   23, -246,  102, -213, -110,  234,  179,\n",
       "         198,   23, -120, -163,  235,  148, -216,   58,  154,  -68,  149, -293,\n",
       "         -76, -153,  -74,  209, -120,  263, -197, -104, -213,  228,  237, -122,\n",
       "        -177, -219, -300,   48,   -7,  245,  -32,  194,  251,   75, -225,  108,\n",
       "        -227,   36,  -69,   95, -172, -140, -118, -271,  129, -189,  -73, -259,\n",
       "         195, -249, -185,  133,  238,  249, -285, -186,  293,  108, -168,   96,\n",
       "          13, -226,  204,  -56, -147,   -7, -152,   19,  -75,    3,  154,   44,\n",
       "        -127,  244, -219,   43,  -99,  170,  286, -226,  172,  286, -123,   74,\n",
       "         287, -166,  289,  283,  223,  118,   16, -270, -169, -298,  156,  203,\n",
       "        -267,  222,   44,  105, -147,  110,  -26,  261,  169,   87,  165,  113,\n",
       "         149,  162, -227,  280,  235,   47, -123,   43,   94, -147,  181, -123,\n",
       "         266,  140,   32,   83, -162,  -87,   38,   34, -289,  -29, -183, -242,\n",
       "         -66,  140,  -48,  244,  284, -163,   86,  -11,  235,   72,  137,   98,\n",
       "        -248,  102,  137,   -9,  -30,   56,  289, -229, -229,  -23, -196,  113,\n",
       "           5,  193, -141,  233,  157,  170,  -16,   44,   -7,  266,  230, -172,\n",
       "          33,  -68, -123,  129,   92,  297,  296,  139,  -31,  111, -158,   15,\n",
       "        -269,  294,  131,   79,  -15, -160,  -85,   21, -296, -209,  166,  -62,\n",
       "        -273,  122,  295, -275,    6,  139,  102, -137, -143,  166,   26, -127,\n",
       "        -266,  -31,  250,  206,  165,  178,  129,  182, -228,  -11, -176,  254,\n",
       "        -187,  120,  181,   52, -156,  -19,   79,  289,  186, -153,   80, -196,\n",
       "        -260, -134, -207, -174,  117,  -74,  -43,   79,  179, -212,  -45, -219,\n",
       "        -141,  238,  112,  -79,   -8,  288,  294, -106,  238, -267, -204,   81,\n",
       "         240,  -44, -183,  293,  266,  151, -284,   70, -271,   57,  177,   27,\n",
       "        -291, -123,   95, -137,   73, -247,  111,   61,   77,  116,  209,   21,\n",
       "        -107,  -52, -133, -236, -111,   56,   84,  -72,   79, -217,   62, -139,\n",
       "         295,    0,  172,  100,  209,  -85,  101,  -45,  251,  159,  289,  180,\n",
       "        -157, -177,   38, -150, -262, -144,  278, -104,   94,  164,  282, -123,\n",
       "        -110, -246,   89,  117,   54, -174,   -1, -111,  187, -284, -139,  225,\n",
       "         -90,  -71,  211,  201,   59,    0,  -64, -206,  181,   -8,  -61, -161,\n",
       "        -261, -196,   56, -190,  283,  296,  204,  217,  -49, -140, -158, -214,\n",
       "         280,  258,  149,   86,  -24, -131, -219, -178, -102, -275,   60, -119,\n",
       "         281, -297, -199, -283, -118,  -50,  270,  199, -146,  216, -206,  190,\n",
       "        -206, -170,  132,  212,  153,  -76, -241,  -89,  254,  138, -237, -193,\n",
       "        -139,  163,  194,   36,   12,  171, -187, -220, -230,  -26, -268,   49,\n",
       "         131, -151,  -90, -149,  230, -291,  -66,  -64,  -21,   -8,  128, -118,\n",
       "        -142,   24,  -12,  -56, -262,  -47, -100,  188, -280,  -69, -292,   31,\n",
       "         119, -218, -278, -125, -299, -290, -262, -111,  122,  -22, -193, -143,\n",
       "          51,  274, -232, -251, -178,  161,   55, -237,  206, -177,  269,  -64,\n",
       "         -42,  -57,  177, -100, -251,  243, -269,  215, -232,  -19,  -75,   78,\n",
       "         258, -243,  -38,  244,  104,  290,  237, -138,  182, -165,   55,   42,\n",
       "        -101,   18, -258,   60,  133, -160,  196,  278,   -8, -234,  -89,  256,\n",
       "        -211,  149,  243,  -45, -276,   16,  136, -185, -123,  246,  260,   72,\n",
       "        -164,  -66,  156, -150,  244,  199,  -94, -272,   22,  296, -260,  179,\n",
       "          45, -167, -218,  171,   32,  156,  192, -291,  -76,   49,  234,  108,\n",
       "        -290,  112,  243,  268, -118, -281,  -37,  172,  213,   76,  123, -269,\n",
       "        -299,  -35, -287,  209,  279,  135, -187, -198, -139, -166,  283, -176,\n",
       "         159,  118,  -59, -173,   59,  177, -270, -115, -150,   93, -177,  263,\n",
       "         -47,    1,  183,  137,  -14, -202, -263,  188,   62,  -62,   -4,  246,\n",
       "        -139, -133,   16,  127, -250, -282, -185,  283,  226,  -79,  243, -240,\n",
       "         202,  114, -124, -196,  -37,  100,   74, -215,   59,  102,  132, -106,\n",
       "         -25,  101,  -57, -248,  281,  -75, -109,  146,  119,   57,  -77,  241,\n",
       "         281,  -88, -255,  233,  -55,  -23,  139,  146, -243,  -99,  259, -112,\n",
       "         188, -216, -179, -259, -233, -186,  161,  159,  196,  255, -248,   18,\n",
       "         157, -159,   96,  -94,    9, -257,  203,  114, -252, -129, -144, -257,\n",
       "         -80, -101,   67,  156,  133,  124,  -25,  144, -136,    4,   91,  -90,\n",
       "          28,  154,   30, -159,    6,  117, -249,  -33,  -65, -232,   28, -283,\n",
       "         215, -272,  -56,  211,  249, -153, -190, -238,  -78,  280, -174, -197,\n",
       "         104,  -20,  154, -224,  264,  213, -111,  285, -116, -195,  298, -224,\n",
       "        -292, -182,  216,  154,  -74,  169, -175, -170,  261, -279,  -28,   18,\n",
       "        -293,  164, -260, -172,  -65,   -5, -176,   83, -168, -199, -107, -118,\n",
       "        -140, -172,  130,    2,  -21,  -89,   64,  -62,   16,   27, -159,  -12,\n",
       "         116, -291, -296,   65, -193,  229,  -28,   98, -151, -137,  175, -220,\n",
       "        -271,  299,   -9,  166,  -31,  -10,  -91, -105,  221,   62,   26, -167,\n",
       "        -136,  165,  159,  -14,  175,   60,  162,  101, -278,  -29,  249,    8,\n",
       "         246,  157, -262, -160,  222,  -30,  -82,   80, -173, -253,  -42, -176,\n",
       "        -132, -225,  -93, -248, -265, -134,   92, -244, -202,  144,  153,  220,\n",
       "        -185,  -75,  153,  -60,  137,  -82,  270,  190,  286,  262,  -82,  151,\n",
       "         243,   71,  201])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "77dbf7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:23:35.520451Z",
     "start_time": "2023-04-29T19:23:35.505626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 173, -150,  -95,  -76,  207,  -62,  -53,  113,  -69,   -7,  172,  288,\n",
       "         294,  251,  162,  -52, -132, -278, -146,  217,  157,  190,   67, -113,\n",
       "          32, -100,  198,   76,  -98,  243,  181,  -32,  139, -113,  198,    4,\n",
       "          68,   97, -213, -176,  157,   81,  -94,   82,  -42, -149, -204,   31,\n",
       "           9,  -56,  147, -149,   41, -251, -229,   83,  281, -134,  211,  279,\n",
       "        -157,  228, -291,   35,  270,   78,  251, -280,  173, -117, -256,    7,\n",
       "         294,  270,   17,  222, -163, -245,  -45,  109,  -33,  132,  231,  186,\n",
       "         265,  106, -264,  258,   79,  120,  182,  113,   55,  285,  273, -126,\n",
       "         242, -145, -189, -159,  152,    3,  128,  227, -218,  221,  -94, -231,\n",
       "          59, -138,   73, -218,  267, -203, -224, -197, -158, -219,   41,  -30,\n",
       "          88,   87,  152, -182,   94,   23, -246,  102, -213, -110,  234,  179,\n",
       "         198,   23, -120, -163,  235,  148, -216,   58,  154,  -68,  149, -293,\n",
       "         -76, -153,  -74,  209, -120,  263, -197, -104, -213,  228,  237, -122,\n",
       "        -177, -219, -300,   48,   -7,  245,  -32,  194,  251,   75, -225,  108,\n",
       "        -227,   36,  -69,   95, -172, -140, -118, -271,  129, -189,  -73, -259,\n",
       "         195, -249, -185,  133,  238,  249, -285, -186,  293,  108, -168,   96,\n",
       "          13, -226,  204,  -56, -147,   -7, -152,   19,  -75,    3,  154,   44,\n",
       "        -127,  244, -219,   43,  -99,  170,  286, -226,  172,  286, -123,   74,\n",
       "         287, -166,  289,  283,  223,  118,   16, -270, -169, -298,  156,  203,\n",
       "        -267,  222,   44,  105, -147,  110,  -26,  261,  169,   87,  165,  113,\n",
       "         149,  162, -227,  280,  235,   47, -123,   43,   94, -147,  181, -123,\n",
       "         266,  140,   32,   83, -162,  -87,   38,   34, -289,  -29, -183, -242,\n",
       "         -66,  140,  -48,  244,  284, -163,   86,  -11,  235,   72,  137,   98,\n",
       "        -248,  102,  137,   -9,  -30,   56,  289, -229, -229,  -23, -196,  113,\n",
       "           5,  193, -141,  233,  157,  170,  -16,   44,   -7,  266,  230, -172,\n",
       "          33,  -68, -123,  129,   92,  297,  296,  139,  -31,  111, -158,   15,\n",
       "        -269,  294,  131,   79,  -15, -160,  -85,   21, -296, -209,  166,  -62,\n",
       "        -273,  122,  295, -275,    6,  139,  102, -137, -143,  166,   26, -127,\n",
       "        -266,  -31,  250,  206,  165,  178,  129,  182, -228,  -11, -176,  254,\n",
       "        -187,  120,  181,   52, -156,  -19,   79,  289,  186, -153,   80, -196,\n",
       "        -260, -134, -207, -174,  117,  -74,  -43,   79,  179, -212,  -45, -219,\n",
       "        -141,  238,  112,  -79,   -8,  288,  294, -106,  238, -267, -204,   81,\n",
       "         240,  -44, -183,  293,  266,  151, -284,   70, -271,   57,  177,   27,\n",
       "        -291, -123,   95, -137,   73, -247,  111,   61,   77,  116,  209,   21,\n",
       "        -107,  -52, -133, -236, -111,   56,   84,  -72,   79, -217,   62, -139,\n",
       "         295,    0,  172,  100,  209,  -85,  101,  -45,  251,  159,  289,  180,\n",
       "        -157, -177,   38, -150, -262, -144,  278, -104,   94,  164,  282, -123,\n",
       "        -110, -246,   89,  117,   54, -174,   -1, -111,  187, -284, -139,  225,\n",
       "         -90,  -71,  211,  201,   59,    0,  -64, -206,  181,   -8,  -61, -161,\n",
       "        -261, -196,   56, -190,  283,  296,  204,  217,  -49, -140, -158, -214,\n",
       "         280,  258,  149,   86,  -24, -131, -219, -178, -102, -275,   60, -119,\n",
       "         281, -297, -199, -283, -118,  -50,  270,  199, -146,  216, -206,  190,\n",
       "        -206, -170,  132,  212,  153,  -76, -241,  -89,  254,  138, -237, -193,\n",
       "        -139,  163,  194,   36,   12,  171, -187, -220, -230,  -26, -268,   49,\n",
       "         131, -151,  -90, -149,  230, -291,  -66,  -64,  -21,   -8,  128, -118,\n",
       "        -142,   24,  -12,  -56, -262,  -47, -100,  188, -280,  -69, -292,   31,\n",
       "         119, -218, -278, -125, -299, -290, -262, -111,  122,  -22, -193, -143,\n",
       "          51,  274, -232, -251, -178,  161,   55, -237,  206, -177,  269,  -64,\n",
       "         -42,  -57,  177, -100, -251,  243, -269,  215, -232,  -19,  -75,   78,\n",
       "         258, -243,  -38,  244,  104,  290,  237, -138,  182, -165,   55,   42,\n",
       "        -101,   18, -258,   60,  133, -160,  196,  278,   -8, -234,  -89,  256,\n",
       "        -211,  149,  243,  -45, -276,   16,  136, -185, -123,  246,  260,   72,\n",
       "        -164,  -66,  156, -150,  244,  199,  -94, -272,   22,  296, -260,  179,\n",
       "          45, -167, -218,  171,   32,  156,  192, -291,  -76,   49,  234,  108,\n",
       "        -290,  112,  243,  268, -118, -281,  -37,  172,  213,   76,  123, -269,\n",
       "        -299,  -35, -287,  209,  279,  135, -187, -198, -139, -166,  283, -176,\n",
       "         159,  118,  -59, -173,   59,  177, -270, -115, -150,   93, -177,  263,\n",
       "         -47,    1,  183,  137,  -14, -202, -263,  188,   62,  -62,   -4,  246,\n",
       "        -139, -133,   16,  127, -250, -282, -185,  283,  226,  -79,  243, -240,\n",
       "         202,  114, -124, -196,  -37,  100,   74, -215,   59,  102,  132, -106,\n",
       "         -25,  101,  -57, -248,  281,  -75, -109,  146,  119,   57,  -77,  241,\n",
       "         281,  -88, -255,  233,  -55,  -23,  139,  146, -243,  -99,  259, -112,\n",
       "         188, -216, -179, -259, -233, -186,  161,  159,  196,  255, -248,   18,\n",
       "         157, -159,   96,  -94,    9, -257,  203,  114, -252, -129, -144, -257,\n",
       "         -80, -101,   67,  156,  133,  124,  -25,  144, -136,    4,   91,  -90,\n",
       "          28,  154,   30, -159,    6,  117, -249,  -33,  -65, -232,   28, -283,\n",
       "         215, -272,  -56,  211,  249, -153, -190, -238,  -78,  280, -174, -197,\n",
       "         104,  -20,  154, -224,  264,  213, -111,  285, -116, -195,  298, -224,\n",
       "        -292, -182,  216,  154,  -74,  169, -175, -170,  261, -279,  -28,   18,\n",
       "        -293,  164, -260, -172,  -65,   -5, -176,   83, -168, -199, -107, -118,\n",
       "        -140, -172,  130,    2,  -21,  -89,   64,  -62,   16,   27, -159,  -12,\n",
       "         116, -291, -296,   65, -193,  229,  -28,   98, -151, -137,  175, -220,\n",
       "        -271,  299,   -9,  166,  -31,  -10,  -91, -105,  221,   62,   26, -167,\n",
       "        -136,  165,  159,  -14,  175,   60,  162,  101, -278,  -29,  249,    8,\n",
       "         246,  157, -262, -160,  222,  -30,  -82,   80, -173, -253,  -42, -176,\n",
       "        -132, -225,  -93, -248, -265, -134,   92, -244, -202,  144,  153,  220,\n",
       "        -185,  -75,  153,  -60,  137,  -82,  270,  190,  286,  262,  -82,  151,\n",
       "         243,   71,  201])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1364600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:52:31.059669Z",
     "start_time": "2023-04-27T11:52:31.050519Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_weights(weights, feat, target, train_index, test_index):\n",
    "    \n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "    \n",
    "    best_state = weights\n",
    "    Features = features[train_index]\n",
    "    t_Features = features[test_index]\n",
    "    \n",
    "    Target = target[train_index]\n",
    "    t_Target = target[test_index]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "        loss_dos = t_get_rmse(final_preds, Target, xdos, perc = True)\n",
    "        test_loss_dos = t_get_rmse(final_t_preds, t_Target, xdos, perc = True)\n",
    "    \n",
    "    return loss_dos, test_loss_dos, opt_shift_train, opt_shift_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce43a0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:52:31.618630Z",
     "start_time": "2023-04-27T11:52:31.608525Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(weights, feat, target, train_index, test_index):\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "    \n",
    "    best_state = weights\n",
    "    Features = features[train_index]\n",
    "    t_Features = features[test_index]\n",
    "    \n",
    "    Target = target[train_index]\n",
    "    t_Target = target[test_index]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds2 = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds2 = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "        \n",
    "    return final_preds, final_t_preds, opt_shift_train, opt_shift_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ecf5c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:52:32.187758Z",
     "start_time": "2023-04-27T11:52:32.181506Z"
    }
   },
   "outputs": [],
   "source": [
    "def reverse_index(train_align, test_align, train_index, test_index):\n",
    "    total_number = len(train_align) + len(test_align)\n",
    "    original_index = torch.zeros(total_number).float()\n",
    "    \n",
    "    original_index.index_add_(0, train_index, train_align)\n",
    "    original_index.index_add_(0, test_index, test_align)\n",
    "    \n",
    "    return original_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bb29d",
   "metadata": {},
   "source": [
    "### Tests - Randomized starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cec4d3c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T12:56:39.484806Z",
     "start_time": "2023-04-27T11:59:16.270850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1003:   5%|█████▍                                                                                                      | 1003/20000 [11:47<3:43:43,  1.42it/s, lowest_mse=0.0127, pred_loss=23.9, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2005:  10%|██████████▊                                                                                                 | 2005/20000 [22:55<3:20:25,  1.50it/s, lowest_mse=0.0099, pred_loss=21.1, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3007:  15%|████████████████                                                                                           | 3007/20000 [34:15<3:16:33,  1.44it/s, lowest_mse=0.00907, pred_loss=20.2, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4009:  20%|█████████████████████▍                                                                                     | 4009/20000 [45:34<2:59:35,  1.48it/s, lowest_mse=0.00854, pred_loss=19.6, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|██████████████████████████▊                                                                                | 5011/20000 [57:22<2:53:06,  1.44it/s, lowest_mse=0.00822, pred_loss=19.3, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|██████████████████████████▊                                                                                | 5011/20000 [57:22<2:51:38,  1.46it/s, lowest_mse=0.00822, pred_loss=19.3, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Unbiased\n",
      "The train error is 19.25 for SOAP\n",
      "The test error is 21.4 for SOAP\n"
     ]
    }
   ],
   "source": [
    "aweights, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, total_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1136799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T12:56:39.800334Z",
     "start_time": "2023-04-27T12:56:39.488267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is 19.25 for SOAP\n",
      "The test error is 21.4 for SOAP\n"
     ]
    }
   ],
   "source": [
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a17d145d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T14:15:00.735440Z",
     "start_time": "2023-04-27T14:15:00.732317Z"
    }
   },
   "outputs": [],
   "source": [
    "total_train_index, total_test_index = torch.tensor(total_train_index), torch.tensor(total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de751c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:16:23.686543Z",
     "start_time": "2023-04-29T19:16:23.345109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10466/3319986621.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_preds2 = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
      "/tmp/ipykernel_10466/3319986621.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  final_t_preds2 = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
      "/tmp/ipykernel_10466/2403169529.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  first_total_shift = reverse_index(-1 * torch.tensor(train_shift).float(), torch.tensor(-1 * test_shift).float(), total_train_index, total_test_index)\n"
     ]
    }
   ],
   "source": [
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "first_total_shift = reverse_index(-1 * torch.tensor(train_shift).float(), torch.tensor(-1 * test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c355e521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:16:24.707451Z",
     "start_time": "2023-04-29T19:16:24.683490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5.,  -7.,  -5.,  -7.,  -7.,  -8.,  -7.,  -5.,  -5.,  -7.,  -4.,  -7.,\n",
       "         -3.,  -6.,  -8.,  -7.,  -3.,  -7.,  -6.,  -3.,  -1.,  -4.,  -8.,  -4.,\n",
       "         -4.,  -4.,  -1.,  -6.,  -5.,  -2.,  -2.,  -3.,  -4.,  -2.,  -4.,  -4.,\n",
       "         -3.,  -3.,  -3.,  -2.,  -1.,  -2.,  -1.,  -1.,  -3.,  -1.,  -2.,   0.,\n",
       "          0.,   0.,  -2.,   3.,  -1.,  -3.,  -1.,   1.,   1.,   2.,  -1.,   0.,\n",
       "          1.,  -2.,   0.,   1.,   4.,   1.,   2.,   1.,   2.,   2.,   0.,   3.,\n",
       "          2.,   1.,   3.,   0.,   0.,   1.,   5.,   4.,   4.,   3.,   0.,   2.,\n",
       "          2.,   4.,   2.,   2.,   3.,   3.,   5.,   4.,   5.,   4.,   4.,   1.,\n",
       "          9.,   3.,   5.,   5.,   3.,   3.,   3.,   6.,  -6.,  -6.,  -7.,  -7.,\n",
       "         -7.,  -6.,  -6.,  -6.,  -6.,  -7.,  -7.,  -7.,  -6.,  -6.,  -6.,  -6.,\n",
       "         -6.,  -7.,  -7.,  -7.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,\n",
       "         -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,\n",
       "         -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,\n",
       "         -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -3.,  -3.,  -3.,  -3.,\n",
       "         -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,  -3.,\n",
       "         -3.,  -3.,  -3.,  -3.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,\n",
       "         -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,\n",
       "         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,\n",
       "         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   1.,   1.,   1.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   3.,   3.,   3.,   2.,\n",
       "          2.,   1.,   1.,   0.,  -1.,  -1.,  -1.,  -1.,  -1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   4.,   4.,   3.,   3.,   2.,   1.,   1.,   0.,\n",
       "          0.,   0.,  -1.,  -1.,  -1.,  -2.,  -1.,  -2.,  -1.,  -1.,  -1.,  -1.,\n",
       "        -12.,  -8., -13., -18., -13., -14.,  -6., -13., -18., -18., -17., -11.,\n",
       "         -8., -19., -15., -25., -12., -19.,  -1., -13., -13., -20., -18.,  -8.,\n",
       "        -13.,  -7.,  -9., -24., -23., -13., -17., -21., -24., -21., -24., -16.,\n",
       "        -23., -11., -23., -13., -16.,   0., -11., -21., -19., -18., -21., -25.,\n",
       "         -9., -10., -15., -11., -17., -17., -10., -22., -16., -19., -10., -15.,\n",
       "        -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18.,\n",
       "        -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18.,\n",
       "        -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -19., -19.,\n",
       "        -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19.,\n",
       "        -19., -19., -19., -19., -19., -19., -19., -19., -19., -20., -20., -20.,\n",
       "        -20., -20., -16., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "        -15., -15., -15., -15., -15., -15., -15., -15., -16., -16., -16., -16.,\n",
       "        -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16.,\n",
       "        -16., -16., -16., -16., -16., -16., -17., -17., -17., -17., -17., -17.,\n",
       "        -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17.,\n",
       "        -17., -17., -18., -18., -18., -18., -18., -18., -19., -19., -19., -19.,\n",
       "        -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19.,\n",
       "        -19., -20., -20., -19., -19., -19., -19., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -20., -19., -19., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20.,\n",
       "        -20., -20., -20., -20., -10.,  -9.,  -9.,  -8.,  -9.,  -9.,  -9.,  -8.,\n",
       "        -10.,  -9., -10.,  -9.,  -9.,  -9., -10., -10.,  -9.,  -8., -16., -17.,\n",
       "        -18., -17., -18., -18., -18., -18., -18., -18.,  -9., -10., -10., -10.,\n",
       "         -9.,  -9., -10., -10., -10., -10., -10., -11., -10., -10., -10., -10.,\n",
       "         -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9., -19., -19., -20.,\n",
       "        -19., -19., -19., -19., -19., -19., -18., -10., -10.,  -7.,  -8.,  -8.,\n",
       "         -7., -14., -16., -12., -13., -17., -17., -17., -18., -18., -18., -18.,\n",
       "        -18., -17., -17., -18., -17., -18., -18., -18., -18., -18., -17., -17.,\n",
       "        -17., -17., -19., -17., -16., -17., -14., -17., -18., -16., -17., -18.,\n",
       "        -18., -18., -18., -19., -18., -18., -18., -18., -18., -18., -18., -18.,\n",
       "        -18., -17., -16., -18., -17., -17., -17., -17., -18., -18., -12., -10.,\n",
       "        -11., -11., -10., -12., -10., -10., -11., -11., -11., -10., -10., -11.,\n",
       "        -12., -11., -10., -11., -10., -11., -11., -11., -12., -10., -11., -11.,\n",
       "        -11., -12., -11., -11., -11., -11., -11., -11., -12., -12., -12., -11.,\n",
       "        -11., -11., -11., -11., -11., -11., -11., -10., -10., -10.,  -8.,  -9.,\n",
       "         -9., -10., -10.,  -8.,  -8.,  -8.,  -9.,  -9.,  -9.,  -9., -10.,  -9.,\n",
       "        -10.,  -9.,  -8.,  -8.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,\n",
       "         -8.,  -8.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9., -10., -10.,\n",
       "        -10., -10.,  -9., -10., -10., -10.,  -9.,  -9., -10., -10., -10.,  -9.,\n",
       "         -9.,  -9.,  -9.,  -9., -10.,  -9.,  -9.,  -9., -10., -10.,  -9.,  -9.,\n",
       "        -10., -10.,  -9.,  -9., -10.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,\n",
       "         -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9., -10.,  -9., -10., -10.,\n",
       "        -10.,  -9., -10.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9., -10.,\n",
       "         -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9., -10.,  -9.,  -9.,  -9.,  -9.,\n",
       "         -9.,  -9.,  -9.,  -9., -10., -10.,  -9.,  -9.,  -9., -10., -10.,  -9.,\n",
       "         -9.,  -9., -10.,  -9.,  -9.,  -9.,  -9., -10.,  -9.,  -9., -10.,  -9.,\n",
       "        -10.,  -9.,  -8.,  -9.,  -9., -10., -10., -10.,  -9.,  -9.,  -9.,  -9.,\n",
       "         -9.,  -9.,  -9.], dtype=torch.float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_total_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da1b9121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T14:17:02.506898Z",
     "start_time": "2023-04-27T14:17:02.490055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2409, -0.1409, -0.2409, -0.1409, -0.1409, -0.0909, -0.1409, -0.2409,\n",
       "        -0.2409, -0.1409, -0.2909, -0.1409, -0.3409, -0.1909, -0.0909, -0.1409,\n",
       "        -0.3409, -0.1409, -0.1909, -0.3409, -0.4409, -0.2909, -0.0909, -0.2909,\n",
       "        -0.2909, -0.2909, -0.4409, -0.1909, -0.2409, -0.3909, -0.3909, -0.3409,\n",
       "        -0.2909, -0.3909, -0.2909, -0.2909, -0.3409, -0.3409, -0.3409, -0.3909,\n",
       "        -0.4409, -0.3909, -0.4409, -0.4409, -0.3409, -0.4409, -0.3909, -0.4909,\n",
       "        -0.4909, -0.4909, -0.3909, -0.6409, -0.4409, -0.3409, -0.4409, -0.5409,\n",
       "        -0.5409, -0.5909, -0.4409, -0.4909, -0.5409, -0.3909, -0.4909, -0.5409,\n",
       "        -0.6909, -0.5409, -0.5909, -0.5409, -0.5909, -0.5909, -0.4909, -0.6409,\n",
       "        -0.5909, -0.5409, -0.6409, -0.4909, -0.4909, -0.5409, -0.7409, -0.6909,\n",
       "        -0.6909, -0.6409, -0.4909, -0.5909, -0.5909, -0.6909, -0.5909, -0.5909,\n",
       "        -0.6409, -0.6409, -0.7409, -0.6909, -0.7409, -0.6909, -0.6909, -0.5409,\n",
       "        -0.9409, -0.6409, -0.7409, -0.7409, -0.6409, -0.6409, -0.6409, -0.7909,\n",
       "        -0.1909, -0.1909, -0.1409, -0.1409, -0.1409, -0.1909, -0.1909, -0.1909,\n",
       "        -0.1909, -0.1409, -0.1409, -0.1409, -0.1909, -0.1909, -0.1909, -0.1909,\n",
       "        -0.1909, -0.1409, -0.1409, -0.1409, -0.2409, -0.2409, -0.2409, -0.2409,\n",
       "        -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409,\n",
       "        -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409, -0.2409,\n",
       "        -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909,\n",
       "        -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909, -0.2909,\n",
       "        -0.2909, -0.2909, -0.2909, -0.2909, -0.3409, -0.3409, -0.3409, -0.3409,\n",
       "        -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409,\n",
       "        -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409, -0.3409,\n",
       "        -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909,\n",
       "        -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909, -0.3909,\n",
       "        -0.3909, -0.3909, -0.3909, -0.3909, -0.4409, -0.4409, -0.4409, -0.4409,\n",
       "        -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409,\n",
       "        -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409, -0.4409,\n",
       "        -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909,\n",
       "        -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909,\n",
       "        -0.4909, -0.4909, -0.4909, -0.4909, -0.5409, -0.5409, -0.5409, -0.5409,\n",
       "        -0.5409, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409,\n",
       "        -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909,\n",
       "        -0.5909, -0.5909, -0.5909, -0.5909, -0.5909, -0.5909, -0.5909, -0.5909,\n",
       "        -0.5909, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409, -0.5409,\n",
       "        -0.5409, -0.5409, -0.5409, -0.5409, -0.6409, -0.6409, -0.6409, -0.5909,\n",
       "        -0.5909, -0.5409, -0.5409, -0.4909, -0.4409, -0.4409, -0.4409, -0.4409,\n",
       "        -0.4409, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909, -0.4909,\n",
       "        -0.6909, -0.6909, -0.6409, -0.6409, -0.5909, -0.5409, -0.5409, -0.4909,\n",
       "        -0.4909, -0.4909, -0.4409, -0.4409, -0.4409, -0.3909, -0.4409, -0.3909,\n",
       "        -0.4409, -0.4409, -0.4409, -0.4409,  0.1091, -0.0909,  0.1591,  0.4091,\n",
       "         0.1591,  0.2091, -0.1909,  0.1591,  0.4091,  0.4091,  0.3591,  0.0591,\n",
       "        -0.0909,  0.4591,  0.2591,  0.7591,  0.1091,  0.4591, -0.4409,  0.1591,\n",
       "         0.1591,  0.5091,  0.4091, -0.0909,  0.1591, -0.1409, -0.0409,  0.7091,\n",
       "         0.6591,  0.1591,  0.3591,  0.5591,  0.7091,  0.5591,  0.7091,  0.3091,\n",
       "         0.6591,  0.0591,  0.6591,  0.1591,  0.3091, -0.4909,  0.0591,  0.5591,\n",
       "         0.4591,  0.4091,  0.5591,  0.7591, -0.0409,  0.0091,  0.2591,  0.0591,\n",
       "         0.3591,  0.3591,  0.0091,  0.6091,  0.3091,  0.4591,  0.0091,  0.2591,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4091,  0.4091,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.3091,  0.2591,\n",
       "         0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,\n",
       "         0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,  0.2591,\n",
       "         0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,\n",
       "         0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,\n",
       "         0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3091,  0.3591,  0.3591,\n",
       "         0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,\n",
       "         0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,  0.3591,\n",
       "         0.3591,  0.3591,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.5091,  0.5091,  0.4591,  0.4591,  0.4591,  0.4591,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.4591,  0.4591,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,  0.5091,\n",
       "         0.5091,  0.5091,  0.5091,  0.5091,  0.0091, -0.0409, -0.0409, -0.0909,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0909,  0.0091, -0.0409,  0.0091, -0.0409,\n",
       "        -0.0409, -0.0409,  0.0091,  0.0091, -0.0409, -0.0909,  0.3091,  0.3591,\n",
       "         0.4091,  0.3591,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "        -0.0409,  0.0091,  0.0091,  0.0091, -0.0409, -0.0409,  0.0091,  0.0091,\n",
       "         0.0091,  0.0091,  0.0091,  0.0591,  0.0091,  0.0091,  0.0091,  0.0091,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409,  0.4591,  0.4591,  0.5091,  0.4591,  0.4591,  0.4591,  0.4591,\n",
       "         0.4591,  0.4591,  0.4091,  0.0091,  0.0091, -0.1409, -0.0909, -0.0909,\n",
       "        -0.1409,  0.2091,  0.3091,  0.1091,  0.1591,  0.3591,  0.3591,  0.3591,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.3591,  0.3591,  0.4091,\n",
       "         0.3591,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.3591,  0.3591,\n",
       "         0.3591,  0.3591,  0.4591,  0.3591,  0.3091,  0.3591,  0.2091,  0.3591,\n",
       "         0.4091,  0.3091,  0.3591,  0.4091,  0.4091,  0.4091,  0.4091,  0.4591,\n",
       "         0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,  0.4091,\n",
       "         0.4091,  0.3591,  0.3091,  0.4091,  0.3591,  0.3591,  0.3591,  0.3591,\n",
       "         0.4091,  0.4091,  0.1091,  0.0091,  0.0591,  0.0591,  0.0091,  0.1091,\n",
       "         0.0091,  0.0091,  0.0591,  0.0591,  0.0591,  0.0091,  0.0091,  0.0591,\n",
       "         0.1091,  0.0591,  0.0091,  0.0591,  0.0091,  0.0591,  0.0591,  0.0591,\n",
       "         0.1091,  0.0091,  0.0591,  0.0591,  0.0591,  0.1091,  0.0591,  0.0591,\n",
       "         0.0591,  0.0591,  0.0591,  0.0591,  0.1091,  0.1091,  0.1091,  0.0591,\n",
       "         0.0591,  0.0591,  0.0591,  0.0591,  0.0591,  0.0591,  0.0591,  0.0091,\n",
       "         0.0091,  0.0091, -0.0909, -0.0409, -0.0409,  0.0091,  0.0091, -0.0909,\n",
       "        -0.0909, -0.0909, -0.0409, -0.0409, -0.0409, -0.0409,  0.0091, -0.0409,\n",
       "         0.0091, -0.0409, -0.0909, -0.0909, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409, -0.0909, -0.0909, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409,  0.0091,  0.0091,\n",
       "         0.0091,  0.0091, -0.0409,  0.0091,  0.0091,  0.0091, -0.0409, -0.0409,\n",
       "         0.0091,  0.0091,  0.0091, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "         0.0091, -0.0409, -0.0409, -0.0409,  0.0091,  0.0091, -0.0409, -0.0409,\n",
       "         0.0091,  0.0091, -0.0409, -0.0409,  0.0091, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409,  0.0091, -0.0409,  0.0091,  0.0091,\n",
       "         0.0091, -0.0409,  0.0091, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409,  0.0091, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409,  0.0091, -0.0409, -0.0409, -0.0409, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409,  0.0091,  0.0091, -0.0409, -0.0409,\n",
       "        -0.0409,  0.0091,  0.0091, -0.0409, -0.0409, -0.0409,  0.0091, -0.0409,\n",
       "        -0.0409, -0.0409, -0.0409,  0.0091, -0.0409, -0.0409,  0.0091, -0.0409,\n",
       "         0.0091, -0.0409, -0.0909, -0.0409, -0.0409,  0.0091,  0.0091,  0.0091,\n",
       "        -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409, -0.0409],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(first_total_shift - torch.mean(first_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82e6a4fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:22:23.133209Z",
     "start_time": "2023-04-27T14:27:08.052329Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1003:   5%|█████▍                                                                                                      | 1003/20000 [11:16<3:33:40,  1.48it/s, lowest_mse=0.0148, pred_loss=25.9, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2005:  10%|██████████▊                                                                                                 | 2005/20000 [22:02<3:13:21,  1.55it/s, lowest_mse=0.0113, pred_loss=22.6, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3007:  15%|████████████████▏                                                                                           | 3007/20000 [32:28<2:56:45,  1.60it/s, lowest_mse=0.0103, pred_loss=21.6, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4009:  20%|█████████████████████▍                                                                                     | 4009/20000 [43:38<2:58:23,  1.49it/s, lowest_mse=0.00966, pred_loss=20.9, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████                                                                                 | 5011/20000 [55:14<2:52:02,  1.45it/s, lowest_mse=0.0093, pred_loss=20.5, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████                                                                                 | 5011/20000 [55:14<2:45:15,  1.51it/s, lowest_mse=0.0093, pred_loss=20.5, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Unbiased\n",
      "The train error is 20.47 for SOAP\n",
      "The test error is 22.58 for SOAP\n"
     ]
    }
   ],
   "source": [
    "aweights2, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, total_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9038532f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:22:23.444566Z",
     "start_time": "2023-04-27T15:22:23.136515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is 20.47 for SOAP\n",
      "The test error is 22.58 for SOAP\n"
     ]
    }
   ],
   "source": [
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights2, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28dcd93c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:22:23.752388Z",
     "start_time": "2023-04-27T15:22:23.446792Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights2, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "second_total_shift = reverse_index(torch.tensor(train_shift).float(), torch.tensor(test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d25fc8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:31:23.100802Z",
     "start_time": "2023-04-27T15:31:23.071900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([122., 122., 121., 122., 123., 124., 124., 121., 121., 123., 121., 122.,\n",
       "        119., 123., 124., 124., 120., 123., 122., 120., 118., 121., 123., 121.,\n",
       "        121., 121., 119., 122., 122., 119., 119., 120., 121., 119., 121., 121.,\n",
       "        120., 120., 120., 119., 118., 119., 118., 118., 120., 117., 118., 117.,\n",
       "        117., 117., 119., 114., 118., 120., 118., 117., 116., 115., 119., 117.,\n",
       "        116., 119., 117., 116., 113., 116., 115., 116., 115., 115., 117., 114.,\n",
       "        116., 116., 114., 118., 117., 116., 112., 113., 114., 114., 117., 115.,\n",
       "        115., 113., 115., 114., 114., 114., 112., 113., 112., 113., 113., 116.,\n",
       "        108., 114., 112., 111., 113., 114., 114., 110., 123., 123., 123., 123.,\n",
       "        123., 123., 123., 123., 123., 123., 123., 123., 123., 123., 123., 123.,\n",
       "        123., 123., 123., 123., 122., 122., 122., 122., 122., 122., 122., 122.,\n",
       "        122., 122., 122., 122., 122., 122., 122., 122., 122., 122., 122., 122.,\n",
       "        121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
       "        121., 121., 121., 121., 121., 121., 121., 121., 120., 120., 120., 120.,\n",
       "        120., 120., 120., 120., 120., 120., 120., 120., 120., 120., 120., 120.,\n",
       "        120., 120., 120., 120., 119., 119., 119., 119., 119., 119., 119., 119.,\n",
       "        119., 119., 119., 119., 119., 119., 119., 119., 119., 119., 119., 119.,\n",
       "        118., 118., 118., 118., 118., 118., 118., 118., 118., 118., 118., 118.,\n",
       "        118., 118., 118., 118., 118., 118., 118., 118., 117., 117., 117., 117.,\n",
       "        117., 117., 117., 117., 117., 117., 117., 117., 117., 117., 117., 117.,\n",
       "        117., 117., 117., 117., 116., 116., 116., 116., 116., 116., 116., 116.,\n",
       "        116., 116., 116., 117., 117., 117., 117., 117., 117., 117., 117., 117.,\n",
       "        115., 115., 115., 115., 115., 115., 115., 115., 115., 116., 116., 116.,\n",
       "        116., 116., 116., 116., 116., 116., 116., 116., 114., 114., 114., 115.,\n",
       "        115., 116., 116., 117., 117., 118., 118., 118., 117., 117., 117., 117.,\n",
       "        117., 117., 117., 116., 113., 113., 114., 114., 115., 115., 116., 116.,\n",
       "        117., 117., 117., 118., 118., 118., 118., 118., 118., 118., 118., 117.,\n",
       "         83., 108., 100.,  84., 113., 115.,  91., 109., 104., 101., 104., 110.,\n",
       "        103.,  81.,  93.,  93.,  88., 109.,  95.,  64.,  84.,  92.,  58.,  82.,\n",
       "         89.,  81.,  95.,  90.,  86., 103.,  78.,  79.,  92., 102.,  89., 104.,\n",
       "        102.,  77., 100.,  98.,  85.,  71.,  85., 101.,  95.,  89.,  95.,  84.,\n",
       "         73.,  94.,  99.,  93., 104., 100., 105., 108.,  94., 103., 108., 100.,\n",
       "        103., 103., 103., 104., 103., 103., 103., 103., 103., 103., 104., 104.,\n",
       "        104., 104., 102., 102., 102., 102., 102., 102., 102., 103., 103., 103.,\n",
       "        103., 103., 102., 103., 103., 103., 103., 103., 103., 103., 102., 102.,\n",
       "        102., 102., 102., 102., 102., 102., 102., 102., 102., 102., 102., 102.,\n",
       "        102., 102., 102., 102., 102., 102., 101., 101., 101., 101., 101., 100.,\n",
       "        100., 100., 105., 105., 105., 105., 105., 105., 105., 105., 104., 104.,\n",
       "        104., 105., 105., 105., 105., 105., 105., 105., 105., 105., 105., 105.,\n",
       "        105., 105., 105., 105., 105., 104., 105., 104., 105., 105., 105., 105.,\n",
       "        105., 105., 105., 105., 105., 105., 105., 104., 104., 104., 104., 104.,\n",
       "        104., 104., 104., 104., 104., 104., 105., 104., 104., 104., 104., 104.,\n",
       "        104., 104., 104., 103., 104., 104., 104., 104., 101., 101., 101., 101.,\n",
       "        101., 101., 101., 101., 101., 101., 101., 101., 101., 101., 101., 101.,\n",
       "        101., 101., 101., 101., 101., 101., 101., 101., 101., 101., 101., 101.,\n",
       "        101., 101., 101., 101., 101., 101., 101., 100., 100., 100., 100., 100.,\n",
       "        101., 100., 100., 100., 100., 101., 100., 100., 101., 101., 101., 101.,\n",
       "        101., 101., 101., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "        100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "        100., 100., 100., 100., 120., 119., 118., 118., 118., 118., 117., 117.,\n",
       "        119., 119., 119., 118., 118., 118., 119., 119., 117., 117., 118., 119.,\n",
       "        120., 120., 121., 121., 121., 120., 121., 121., 121., 121., 121., 121.,\n",
       "        121., 121., 121., 121., 121., 121., 118., 118., 118., 118., 118., 118.,\n",
       "        118., 118., 118., 117., 121., 121., 121., 121., 121., 121., 121., 122.,\n",
       "        121., 120., 121., 120., 121., 121., 119., 118., 118., 116., 117., 117.,\n",
       "        116., 127., 130., 126., 127., 130., 129., 130., 130., 130., 131., 130.,\n",
       "        130., 130., 130., 130., 130., 131., 130., 130., 131., 130., 129., 129.,\n",
       "        129., 129., 131., 129., 128., 129., 127., 129., 132., 130., 131., 131.,\n",
       "        131., 132., 131., 132., 131., 131., 131., 131., 131., 131., 130., 130.,\n",
       "        130., 129., 128., 129., 129., 129., 129., 129., 130., 130., 124., 122.,\n",
       "        123., 123., 122., 124., 122., 122., 123., 122., 123., 122., 122., 123.,\n",
       "        123., 123., 122., 122., 122., 123., 123., 123., 124., 122., 123., 123.,\n",
       "        123., 124., 123., 123., 124., 124., 124., 124., 124., 124., 124., 124.,\n",
       "        124., 123., 123., 123., 123., 123., 123., 123., 123., 122., 120., 121.,\n",
       "        122., 122., 122., 120., 121., 120., 121., 121., 121., 121., 122., 121.,\n",
       "        122., 121., 120., 120., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
       "        120., 120., 120., 121., 121., 121., 121., 121., 121., 121., 121., 121.,\n",
       "        121., 122., 121., 121., 121., 121., 121., 121., 121., 122., 122., 121.,\n",
       "        121., 120., 120., 121., 121., 120., 120., 121., 121., 121., 120., 121.,\n",
       "        121., 121., 121., 121., 121., 121., 121., 121., 121., 121., 120., 120.,\n",
       "        121., 120., 121., 120., 121., 120., 121., 121., 121., 121., 121., 121.,\n",
       "        121., 121., 121., 121., 120., 120., 120., 121., 120., 121., 121., 121.,\n",
       "        120., 120., 121., 121., 121., 120., 120., 121., 120., 121., 120., 121.,\n",
       "        121., 121., 120., 121., 121., 121., 121., 120., 121., 121., 121., 121.,\n",
       "        121., 121., 121., 120., 121., 121., 121., 121., 120., 121., 121., 121.,\n",
       "        121., 120., 120., 121., 121., 121., 122., 121., 121., 121., 121., 121.,\n",
       "        121., 121., 121.], dtype=torch.float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_total_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b952f70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:22:23.769817Z",
     "start_time": "2023-04-27T15:22:23.754948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3843,  0.3843,  0.3343,  0.3843,  0.4343,  0.4843,  0.4843,  0.3343,\n",
       "         0.3343,  0.4343,  0.3343,  0.3843,  0.2343,  0.4343,  0.4843,  0.4843,\n",
       "         0.2843,  0.4343,  0.3843,  0.2843,  0.1843,  0.3343,  0.4343,  0.3343,\n",
       "         0.3343,  0.3343,  0.2343,  0.3843,  0.3843,  0.2343,  0.2343,  0.2843,\n",
       "         0.3343,  0.2343,  0.3343,  0.3343,  0.2843,  0.2843,  0.2843,  0.2343,\n",
       "         0.1843,  0.2343,  0.1843,  0.1843,  0.2843,  0.1343,  0.1843,  0.1343,\n",
       "         0.1343,  0.1343,  0.2343, -0.0157,  0.1843,  0.2843,  0.1843,  0.1343,\n",
       "         0.0843,  0.0343,  0.2343,  0.1343,  0.0843,  0.2343,  0.1343,  0.0843,\n",
       "        -0.0657,  0.0843,  0.0343,  0.0843,  0.0343,  0.0343,  0.1343, -0.0157,\n",
       "         0.0843,  0.0843, -0.0157,  0.1843,  0.1343,  0.0843, -0.1157, -0.0657,\n",
       "        -0.0157, -0.0157,  0.1343,  0.0343,  0.0343, -0.0657,  0.0343, -0.0157,\n",
       "        -0.0157, -0.0157, -0.1157, -0.0657, -0.1157, -0.0657, -0.0657,  0.0843,\n",
       "        -0.3157, -0.0157, -0.1157, -0.1657, -0.0657, -0.0157, -0.0157, -0.2157,\n",
       "         0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,\n",
       "         0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,\n",
       "         0.4343,  0.4343,  0.4343,  0.4343,  0.3843,  0.3843,  0.3843,  0.3843,\n",
       "         0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,\n",
       "         0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,  0.3843,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.2843,  0.2843,  0.2843,  0.2843,\n",
       "         0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,\n",
       "         0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,  0.2843,\n",
       "         0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,\n",
       "         0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,  0.2343,\n",
       "         0.2343,  0.2343,  0.2343,  0.2343,  0.1843,  0.1843,  0.1843,  0.1843,\n",
       "         0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,\n",
       "         0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,\n",
       "         0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,\n",
       "         0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,\n",
       "         0.1343,  0.1343,  0.1343,  0.1343,  0.0843,  0.0843,  0.0843,  0.0843,\n",
       "         0.0843,  0.0843,  0.0843,  0.0843,  0.0843,  0.0843,  0.0843,  0.1343,\n",
       "         0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,\n",
       "         0.0343,  0.0343,  0.0343,  0.0343,  0.0343,  0.0343,  0.0343,  0.0343,\n",
       "         0.0343,  0.0843,  0.0843,  0.0843,  0.0843,  0.0843,  0.0843,  0.0843,\n",
       "         0.0843,  0.0843,  0.0843,  0.0843, -0.0157, -0.0157, -0.0157,  0.0343,\n",
       "         0.0343,  0.0843,  0.0843,  0.1343,  0.1343,  0.1843,  0.1843,  0.1843,\n",
       "         0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.1343,  0.0843,\n",
       "        -0.0657, -0.0657, -0.0157, -0.0157,  0.0343,  0.0343,  0.0843,  0.0843,\n",
       "         0.1343,  0.1343,  0.1343,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,\n",
       "         0.1843,  0.1843,  0.1843,  0.1343, -1.5657, -0.3157, -0.7157, -1.5157,\n",
       "        -0.0657,  0.0343, -1.1657, -0.2657, -0.5157, -0.6657, -0.5157, -0.2157,\n",
       "        -0.5657, -1.6657, -1.0657, -1.0657, -1.3157, -0.2657, -0.9657, -2.5157,\n",
       "        -1.5157, -1.1157, -2.8157, -1.6157, -1.2657, -1.6657, -0.9657, -1.2157,\n",
       "        -1.4157, -0.5657, -1.8157, -1.7657, -1.1157, -0.6157, -1.2657, -0.5157,\n",
       "        -0.6157, -1.8657, -0.7157, -0.8157, -1.4657, -2.1657, -1.4657, -0.6657,\n",
       "        -0.9657, -1.2657, -0.9657, -1.5157, -2.0657, -1.0157, -0.7657, -1.0657,\n",
       "        -0.5157, -0.7157, -0.4657, -0.3157, -1.0157, -0.5657, -0.3157, -0.7157,\n",
       "        -0.5657, -0.5657, -0.5657, -0.5157, -0.5657, -0.5657, -0.5657, -0.5657,\n",
       "        -0.5657, -0.5657, -0.5157, -0.5157, -0.5157, -0.5157, -0.6157, -0.6157,\n",
       "        -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.5657, -0.5657, -0.5657,\n",
       "        -0.5657, -0.5657, -0.6157, -0.5657, -0.5657, -0.5657, -0.5657, -0.5657,\n",
       "        -0.5657, -0.5657, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157,\n",
       "        -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157,\n",
       "        -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6157, -0.6657, -0.6657,\n",
       "        -0.6657, -0.6657, -0.6657, -0.7157, -0.7157, -0.7157, -0.4657, -0.4657,\n",
       "        -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.5157, -0.5157,\n",
       "        -0.5157, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657,\n",
       "        -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657,\n",
       "        -0.4657, -0.5157, -0.4657, -0.5157, -0.4657, -0.4657, -0.4657, -0.4657,\n",
       "        -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.4657, -0.5157,\n",
       "        -0.5157, -0.5157, -0.5157, -0.5157, -0.5157, -0.5157, -0.5157, -0.5157,\n",
       "        -0.5157, -0.5157, -0.4657, -0.5157, -0.5157, -0.5157, -0.5157, -0.5157,\n",
       "        -0.5157, -0.5157, -0.5157, -0.5657, -0.5157, -0.5157, -0.5157, -0.5157,\n",
       "        -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657,\n",
       "        -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657,\n",
       "        -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657,\n",
       "        -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657,\n",
       "        -0.6657, -0.6657, -0.6657, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157,\n",
       "        -0.6657, -0.7157, -0.7157, -0.7157, -0.7157, -0.6657, -0.7157, -0.7157,\n",
       "        -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.6657, -0.7157,\n",
       "        -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157,\n",
       "        -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157,\n",
       "        -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157,\n",
       "        -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157, -0.7157,\n",
       "        -0.7157, -0.7157, -0.7157, -0.7157,  0.2843,  0.2343,  0.1843,  0.1843,\n",
       "         0.1843,  0.1843,  0.1343,  0.1343,  0.2343,  0.2343,  0.2343,  0.1843,\n",
       "         0.1843,  0.1843,  0.2343,  0.2343,  0.1343,  0.1343,  0.1843,  0.2343,\n",
       "         0.2843,  0.2843,  0.3343,  0.3343,  0.3343,  0.2843,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,  0.1843,\n",
       "         0.1843,  0.1843,  0.1843,  0.1343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3843,  0.3343,  0.2843,  0.3343,  0.2843,\n",
       "         0.3343,  0.3343,  0.2343,  0.1843,  0.1843,  0.0843,  0.1343,  0.1343,\n",
       "         0.0843,  0.6343,  0.7843,  0.5843,  0.6343,  0.7843,  0.7343,  0.7843,\n",
       "         0.7843,  0.7843,  0.8343,  0.7843,  0.7843,  0.7843,  0.7843,  0.7843,\n",
       "         0.7843,  0.8343,  0.7843,  0.7843,  0.8343,  0.7843,  0.7343,  0.7343,\n",
       "         0.7343,  0.7343,  0.8343,  0.7343,  0.6843,  0.7343,  0.6343,  0.7343,\n",
       "         0.8843,  0.7843,  0.8343,  0.8343,  0.8343,  0.8843,  0.8343,  0.8843,\n",
       "         0.8343,  0.8343,  0.8343,  0.8343,  0.8343,  0.8343,  0.7843,  0.7843,\n",
       "         0.7843,  0.7343,  0.6843,  0.7343,  0.7343,  0.7343,  0.7343,  0.7343,\n",
       "         0.7843,  0.7843,  0.4843,  0.3843,  0.4343,  0.4343,  0.3843,  0.4843,\n",
       "         0.3843,  0.3843,  0.4343,  0.3843,  0.4343,  0.3843,  0.3843,  0.4343,\n",
       "         0.4343,  0.4343,  0.3843,  0.3843,  0.3843,  0.4343,  0.4343,  0.4343,\n",
       "         0.4843,  0.3843,  0.4343,  0.4343,  0.4343,  0.4843,  0.4343,  0.4343,\n",
       "         0.4843,  0.4843,  0.4843,  0.4843,  0.4843,  0.4843,  0.4843,  0.4843,\n",
       "         0.4843,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,  0.4343,\n",
       "         0.4343,  0.3843,  0.2843,  0.3343,  0.3843,  0.3843,  0.3843,  0.2843,\n",
       "         0.3343,  0.2843,  0.3343,  0.3343,  0.3343,  0.3343,  0.3843,  0.3343,\n",
       "         0.3843,  0.3343,  0.2843,  0.2843,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.2843,  0.2843,  0.2843,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3843,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3843,  0.3843,  0.3343,  0.3343,  0.2843,  0.2843,  0.3343,\n",
       "         0.3343,  0.2843,  0.2843,  0.3343,  0.3343,  0.3343,  0.2843,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.2843,  0.2843,  0.3343,  0.2843,  0.3343,  0.2843,\n",
       "         0.3343,  0.2843,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.2843,  0.2843,  0.2843,  0.3343,\n",
       "         0.2843,  0.3343,  0.3343,  0.3343,  0.2843,  0.2843,  0.3343,  0.3343,\n",
       "         0.3343,  0.2843,  0.2843,  0.3343,  0.2843,  0.3343,  0.2843,  0.3343,\n",
       "         0.3343,  0.3343,  0.2843,  0.3343,  0.3343,  0.3343,  0.3343,  0.2843,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.2843,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.2843,  0.3343,  0.3343,  0.3343,\n",
       "         0.3343,  0.2843,  0.2843,  0.3343,  0.3343,  0.3343,  0.3843,  0.3343,\n",
       "         0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343,  0.3343],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_total_shift - torch.mean(second_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5019e47f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.523422Z",
     "start_time": "2023-04-27T15:22:23.771528Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 676:   3%|███▋                                                                                                          | 676/20000 [07:56<3:47:04,  1.42it/s, lowest_mse=0.0165, pred_loss=27.3, trigger=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aweights3, loss_dos, test_loss_dos \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_reg_train_Ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_soap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_aligned_dos3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtotal_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_test_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam Unbiased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train error is \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m for SOAP\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss_dos))\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mnormal_reg_train_Ad\u001b[0;34m(feat, target, train_index, test_index, regularization, n_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    134\u001b[0m         loss_i\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss_i\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    139\u001b[0m     preds \u001b[38;5;241m=\u001b[39m Features \u001b[38;5;241m@\u001b[39m weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 183\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    186\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mnormal_reg_train_Ad.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m target_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([Target[i_batch], torch\u001b[38;5;241m.\u001b[39mzeros(n_col, Target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[1;32m    130\u001b[0m pred_i \u001b[38;5;241m=\u001b[39m reg_features_i \u001b[38;5;241m@\u001b[39m weights\n\u001b[0;32m--> 131\u001b[0m opt_shift \u001b[38;5;241m=\u001b[39m \u001b[43mfind_optimal_discrete_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m pred_i[:\u001b[38;5;28mlen\u001b[39m(i_batch)] \u001b[38;5;241m=\u001b[39m shifted_ldos_discrete(pred_i[:\u001b[38;5;28mlen\u001b[39m(i_batch)], xdos, torch\u001b[38;5;241m.\u001b[39mtensor(opt_shift))\n\u001b[1;32m    133\u001b[0m loss_i \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mt_get_mse(pred_i, target_i)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mfind_optimal_discrete_shift\u001b[0;34m(prediction, true)\u001b[0m\n\u001b[1;32m      4\u001b[0m shift \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m----> 6\u001b[0m     corr \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     shift_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(corr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(true[i]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m   \n\u001b[1;32m      8\u001b[0m     shift\u001b[38;5;241m.\u001b[39mappend(shift_i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/signal/signaltools.py:239\u001b[0m, in \u001b[0;36mcorrelate\u001b[0;34m(in1, in2, mode, method)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# this either calls fftconvolve or this function with method=='direct'\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43min1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_reverse_and_conj\u001b[49m\u001b[43m(\u001b[49m\u001b[43min2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# fastpath to faster numpy.correlate for 1d inputs when possible\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _np_conv_ok(in1, in2, mode):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/signal/signaltools.py:1408\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(in1, in2, mode, method)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;66;03m# fastpath to faster numpy.convolve for 1d inputs when possible\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _np_conv_ok(volume, kernel, mode):\n\u001b[0;32m-> 1408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correlate(volume, _reverse_and_conj(kernel), mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:844\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aweights3, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, total_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db52f82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.526377Z",
     "start_time": "2023-04-27T15:30:20.526360Z"
    }
   },
   "outputs": [],
   "source": [
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights3, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fd1bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.527705Z",
     "start_time": "2023-04-27T15:30:20.527689Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights3, total_soap, total_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "third_total_shift = reverse_index(torch.tensor(train_shift).float(), torch.tensor(test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116c029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.528957Z",
     "start_time": "2023-04-27T15:30:20.528941Z"
    }
   },
   "outputs": [],
   "source": [
    "(third_total_shift - torch.mean(third_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c3105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.530196Z",
     "start_time": "2023-04-27T15:30:20.530179Z"
    }
   },
   "outputs": [],
   "source": [
    "shifted_aligned_dos3 = shifted_ldos_discrete(total_aligned_dos3, xdos, torch.ones(len(total_aligned_dos3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a947eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.531429Z",
     "start_time": "2023-04-27T15:30:20.531412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aweights4, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, shifted_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dac34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.532663Z",
     "start_time": "2023-04-27T15:30:20.532647Z"
    }
   },
   "outputs": [],
   "source": [
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights4, total_soap, shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e2b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.533891Z",
     "start_time": "2023-04-27T15:30:20.533875Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights4, total_soap, shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "fourth_total_shift = reverse_index(torch.tensor(train_shift).float(), torch.tensor(test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cd291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T15:30:20.535103Z",
     "start_time": "2023-04-27T15:30:20.535086Z"
    }
   },
   "outputs": [],
   "source": [
    "(fourth_total_shift - torch.mean(fourth_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e7e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e23717ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T18:45:26.093931Z",
     "start_time": "2023-04-27T17:46:17.982357Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1003:   5%|█████▍                                                                                                      | 1003/20000 [12:18<3:44:32,  1.41it/s, lowest_mse=0.0154, pred_loss=17.5, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2005:  10%|██████████▊                                                                                                 | 2005/20000 [23:51<3:26:34,  1.45it/s, lowest_mse=0.0121, pred_loss=15.5, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3007:  15%|████████████████▏                                                                                           | 3007/20000 [35:17<3:07:56,  1.51it/s, lowest_mse=0.0111, pred_loss=14.8, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4009:  20%|█████████████████████▋                                                                                      | 4009/20000 [47:00<3:07:23,  1.42it/s, lowest_mse=0.0105, pred_loss=14.4, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████                                                                                 | 5011/20000 [59:06<3:00:26,  1.38it/s, lowest_mse=0.0101, pred_loss=14.1, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████                                                                                 | 5011/20000 [59:07<2:56:50,  1.41it/s, lowest_mse=0.0101, pred_loss=14.1, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Unbiased\n",
      "The train error is 14.13 for SOAP\n",
      "The test error is 15.45 for SOAP\n",
      "The train error is 14.13 for SOAP\n",
      "The test error is 15.45 for SOAP\n"
     ]
    }
   ],
   "source": [
    "true_alignment = torch.randint(-50, 50, size = (len(total_aligned_dos3),))\n",
    "random_shifted_aligned_dos3 = shifted_ldos_discrete(total_aligned_dos3, xdos, true_alignment)\n",
    "\n",
    "aweights5, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, random_shifted_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))\n",
    "\n",
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights5, total_soap, random_shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))\n",
    "\n",
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights5, total_soap, random_shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "fifth_total_shift = reverse_index(torch.tensor(train_shift).float(), torch.tensor(test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08d791ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T18:45:26.110027Z",
     "start_time": "2023-04-27T18:45:26.096776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3430, -2.1570, -0.1570, -1.0570,  2.0430,  2.0930,  2.0930, -1.5070,\n",
       "         0.4430,  1.8930,  0.4930,  2.2430, -2.6570,  0.3930,  2.1430,  1.4930,\n",
       "         1.8930,  1.6930, -0.6070,  0.9930, -0.8070,  1.9430, -1.5570,  1.3430,\n",
       "        -1.6070,  0.7430, -1.7070, -0.7070,  0.5430,  0.1930,  0.3430, -1.8070,\n",
       "         0.2430, -1.3570, -2.4570, -0.7570,  1.6930, -0.4570, -1.7570,  1.9430,\n",
       "         0.2430, -1.2570,  0.9930,  1.6430,  0.8430, -0.7070, -0.1570,  0.2430,\n",
       "         0.0430,  1.9430, -2.6070, -0.0070,  0.9430,  1.2430,  0.2430, -1.3070,\n",
       "        -0.6070,  0.9930,  0.2430,  1.0430, -2.4070,  0.3430, -0.1070,  0.1930,\n",
       "         1.0430,  1.5430, -2.5070,  1.4930, -1.6570,  0.8930,  1.6930, -0.5070,\n",
       "        -2.0570, -2.4070, -1.8070,  2.0430,  0.2930,  1.8430, -0.9070,  0.9930,\n",
       "        -1.2570,  2.0930, -1.4070,  1.8930,  0.0930, -0.6570,  0.3430, -1.9570,\n",
       "        -0.6070, -2.0570,  0.8930, -0.9570, -0.2570, -0.1070, -1.9070, -2.6570,\n",
       "        -0.3570, -2.1070, -0.3570, -0.1070, -2.2070,  0.3430, -2.0570, -2.5570,\n",
       "        -1.3070, -0.4070, -0.9570, -0.9570, -2.5070,  2.0930,  0.8430,  1.9930,\n",
       "        -0.3570,  1.9930, -0.3070,  1.1930, -2.3570,  1.6930, -2.0070, -0.9070,\n",
       "        -0.9070, -2.5070, -1.5570, -0.2070,  0.7930, -2.4070,  0.1930, -2.5570,\n",
       "         1.1430,  0.9430,  1.2930, -2.7070,  0.0430, -1.1570,  2.1930, -1.4070,\n",
       "         1.7430,  0.4930,  1.2430, -1.7570, -0.5570, -1.4570, -1.1070, -2.0570,\n",
       "         0.5930, -0.4570,  0.3930, -1.1570, -2.3070,  1.5430, -1.4570, -2.4070,\n",
       "         1.4930, -2.2070,  0.2430, -2.7570, -1.8570, -2.1570,  1.6930, -2.5570,\n",
       "        -0.6070, -1.6070, -2.6070, -0.0570,  1.1930,  1.3930, -2.2070,  0.7930,\n",
       "        -1.1070, -1.9070,  0.8430,  0.9930, -1.8570, -0.1070,  1.9430, -1.7070,\n",
       "         0.3930,  1.6930, -1.6570,  0.9430,  1.5430,  0.5930, -0.5070,  0.3430,\n",
       "         1.5430,  0.0930, -2.2070,  0.0430, -2.5570,  1.0930, -2.6570, -0.1570,\n",
       "        -2.6570, -0.3570, -0.2570, -0.0070,  0.4930, -2.6570, -0.3070, -0.4070,\n",
       "        -2.4070, -1.4570,  0.9430, -0.2570, -1.6070,  1.8430, -1.0570,  0.8430,\n",
       "        -2.2570,  1.1430, -0.4070, -1.3070, -0.7070, -0.4070, -2.1570, -2.2070,\n",
       "         0.7430, -1.0070, -1.7070, -0.1070, -1.8570, -1.8570,  1.5430, -2.6570,\n",
       "        -2.4570,  1.9930, -2.1570,  1.5430, -2.7570,  0.7430, -0.8570, -0.2070,\n",
       "         1.8930, -0.8570, -0.9570, -0.3070,  1.9430,  0.1930, -1.1070, -1.1070,\n",
       "        -0.6570, -2.2570,  0.6930,  1.3930, -1.1570, -2.4570,  0.6930, -0.4570,\n",
       "        -2.8070, -1.2070, -1.7070, -2.2070,  1.6930, -0.7570, -2.4570, -0.3570,\n",
       "         0.0930,  1.3930,  0.7930, -1.8070, -0.6570,  1.1430, -2.1570,  0.6930,\n",
       "         0.7430,  0.5430, -0.3070, -1.1070,  0.4430,  1.2430, -2.5570, -0.1570,\n",
       "        -0.0570, -0.0070,  1.0430,  0.1430,  0.4430,  0.3930, -0.1070, -0.3570,\n",
       "        -1.7070, -0.4070,  2.0930, -2.6570, -1.3570,  1.2430, -2.1070, -1.9070,\n",
       "         1.5930, -0.8070,  0.1430,  0.1430, -2.7070, -2.6570, -1.5070, -2.5070,\n",
       "        -2.4570,  1.4430, -2.0070,  0.9930, -2.6570,  0.0430, -2.2570,  1.4430,\n",
       "         1.9930,  1.9930,  1.5930, -1.7070, -0.0070,  1.3430, -1.6070,  0.1430,\n",
       "        -0.4570,  1.1430,  1.9930,  0.1430,  0.4430,  0.7430,  0.6930,  1.5430,\n",
       "        -1.9570,  1.7430, -2.0070, -1.9570, -0.4570, -0.0070,  1.2930,  0.4930,\n",
       "         3.0430, -0.4570,  3.8430,  1.7430, -0.1570,  2.9430,  1.1930,  2.0930,\n",
       "         0.8430,  2.3930,  2.0430,  3.7930,  4.2930,  1.7430,  0.8430,  3.2430,\n",
       "         0.1430,  1.1430,  2.5930,  1.6430,  2.2430,  5.2930,  4.3930,  2.7930,\n",
       "         2.7430,  1.6430,  3.9930,  4.0930,  2.8430, -0.3570,  0.4930,  0.4930,\n",
       "         1.0930,  3.0930,  2.3930,  2.8930, -0.5570,  3.4430,  0.3930,  0.1930,\n",
       "        -0.5570,  0.9930,  3.5930,  0.5930,  4.1930,  4.4430,  2.1430,  4.8430,\n",
       "         0.5430,  2.9430,  0.2430, -0.2570,  2.5930,  0.3430,  3.0930, -0.6570,\n",
       "         2.3430, -0.5570, -1.7070,  0.1930,  0.0930,  0.8930, -0.5070,  2.1930,\n",
       "         0.0430, -0.6070, -0.6570,  0.9930,  2.3430, -2.0570, -1.3570,  1.0930,\n",
       "        -0.5070, -1.6570,  0.5930, -1.8070,  1.4930, -1.3570,  1.8430, -0.6070,\n",
       "        -1.6070,  2.0930, -1.9070,  1.6930,  2.6930, -1.9570, -1.1070,  2.7430,\n",
       "         1.5930, -0.8070, -1.6570, -1.3070, -1.6070,  1.0930, -1.0570, -1.5070,\n",
       "         2.0430,  2.7430,  2.4930, -0.9070,  1.3430, -1.5570, -0.4070,  2.5930,\n",
       "        -1.7070,  0.0430,  2.1930,  1.8430, -1.2070, -0.9570, -1.7070, -1.7070,\n",
       "         1.3430,  1.5930,  0.2930,  0.4430,  2.4930,  1.5430,  0.6930, -0.0570,\n",
       "        -0.1070,  1.7930,  0.6430, -1.3570,  2.3930, -1.3570,  2.0930,  1.6930,\n",
       "        -2.1570, -0.7070, -0.3570,  1.9930, -2.2570,  2.2930, -1.2570,  1.8430,\n",
       "        -0.5570, -0.1070,  1.8930,  0.8930, -2.0070, -1.0070, -0.4570,  1.5930,\n",
       "        -1.0570, -2.2570, -0.1570, -2.2570,  1.0430,  0.9430,  2.3430,  0.3930,\n",
       "         1.8930,  1.2430,  2.1430, -0.2570,  2.1430,  1.7930,  0.2930, -1.7570,\n",
       "        -2.1070, -0.9570,  1.6430, -0.8070, -0.5070, -0.9070, -2.0570, -0.8070,\n",
       "         2.2930,  0.0430,  2.2930, -0.5570,  2.1430,  1.1430, -0.6570, -1.0570,\n",
       "         2.0930,  0.8430, -1.0570,  1.3930,  0.1430,  2.2430,  0.1430,  1.7930,\n",
       "        -1.0070, -1.4570,  2.5930,  0.6930,  2.3430, -1.2070,  0.8930,  1.8430,\n",
       "        -0.0570,  1.0930,  1.7930,  1.6930, -0.9570,  1.5430,  2.5430, -0.6570,\n",
       "        -1.6570,  2.1930, -0.8570, -0.6070, -0.1570,  2.3430, -0.9570, -0.8570,\n",
       "         0.7930, -1.5570,  1.9930,  0.4430, -0.6070, -0.6070, -1.5570, -1.8570,\n",
       "        -0.0070, -0.3070,  1.5930, -1.5070, -1.5570, -1.3570,  1.1930, -1.4070,\n",
       "        -1.5570,  1.7930, -1.9570,  0.9430,  0.4430, -1.2570,  2.1930,  0.1430,\n",
       "         1.3930, -1.3070, -1.2570,  1.7930,  2.3430,  0.0430, -1.8070,  1.2930,\n",
       "         2.6430,  0.8430,  1.4430,  1.1430,  0.8430, -0.9570,  2.4430,  1.3430,\n",
       "        -1.7570,  1.5930,  2.3930,  0.4930, -1.3070,  2.0930,  0.3430,  1.2430,\n",
       "         1.1930, -0.6070,  0.6930,  2.0930,  2.7930,  1.0430,  1.2430, -1.2070,\n",
       "         1.0430, -1.9070,  0.2930, -1.1070, -1.7570, -1.2070,  2.5930,  2.8930,\n",
       "         1.5430,  1.1930,  1.3430, -2.0070,  0.2430,  1.4430, -2.6570, -1.3070,\n",
       "        -1.5070, -2.7070, -0.2070, -0.2570,  1.6930,  1.7430, -2.0570, -1.9070,\n",
       "         1.6930,  0.9930, -2.6570, -0.5570,  0.1930, -2.7070, -0.9570,  1.1930,\n",
       "        -1.3570,  0.1930, -1.0570, -1.6570,  0.7430,  0.4930, -0.6070,  1.6930,\n",
       "         1.5930, -1.3570, -1.4070,  1.5930, -0.5070, -1.1570, -1.3070, -2.1570,\n",
       "        -1.5070, -0.3070, -1.0570,  1.5430,  1.1430,  1.3430, -1.3070, -1.5070,\n",
       "         2.0430,  0.4930, -1.5570, -1.3570,  0.4930, -0.8070, -1.5070,  0.9430,\n",
       "         0.9430, -2.1070,  2.4930, -0.5570,  2.0430, -0.5070,  1.7930,  1.2930,\n",
       "        -0.7570,  1.8930,  0.9430, -2.3070, -0.8570, -1.9570, -0.7570,  1.3430,\n",
       "        -0.3070,  1.1930,  0.8430, -0.3070, -2.0570,  1.2430,  2.5930,  1.5430,\n",
       "         2.0430, -1.5570, -2.0070, -1.8570, -0.2570, -2.2570,  0.7930,  0.2430,\n",
       "        -2.2070, -1.2570,  0.6430, -2.0070, -2.2570,  1.0430,  1.4930, -1.0570,\n",
       "        -0.5070, -1.8070,  0.5930,  2.2930,  2.0430, -1.9070,  2.3430, -1.0570,\n",
       "         1.3930, -0.4570, -0.9070,  1.8930,  2.0430, -0.2070, -1.0570,  0.4930,\n",
       "        -0.4570, -0.1070, -1.5570, -1.5570, -0.2070, -0.3570,  1.3930,  0.6930,\n",
       "        -0.9070,  2.3930,  1.9930,  1.7930,  1.5430, -2.2570,  2.5930, -0.7070,\n",
       "        -1.0070, -0.4070, -1.4570,  1.0430, -2.2570, -2.4570, -1.1070, -1.8070,\n",
       "        -0.0070, -0.5070, -2.0070,  0.7430, -0.9570,  0.1930, -1.6070,  0.7430,\n",
       "         1.5430, -1.7570, -2.3570,  2.2430, -1.5570, -0.9570,  0.6430, -0.0070,\n",
       "         1.7430,  0.0430, -1.0070, -0.2570,  1.7430, -1.6570,  1.3430,  1.1430,\n",
       "        -0.6070,  1.5930,  1.2930, -1.2070, -2.0070,  1.7430,  0.5430,  2.2430,\n",
       "         1.5430,  0.8430, -2.2070, -0.2570, -0.4070, -1.6070,  2.0930,  1.9430,\n",
       "        -1.8570,  0.3930, -2.3570, -0.9570, -2.4070,  0.7430, -0.4570,  0.5930,\n",
       "         1.4430,  1.7430, -2.2570, -2.0570,  0.6430,  1.6430,  1.8430, -1.3570,\n",
       "         0.6430,  1.2930, -0.8070, -2.0570, -0.0570, -1.8570, -2.7570,  0.9930,\n",
       "        -1.1070, -1.7570, -2.4070, -2.3070, -2.1570,  1.4930,  0.0930,  0.8430,\n",
       "         1.7430, -2.5070,  0.4430, -2.3570,  1.2930, -1.1570, -1.0070, -1.7570,\n",
       "         1.4930,  1.7930, -1.0570,  0.9930, -2.5070,  1.8930,  1.7430, -2.4570,\n",
       "        -1.5570,  1.6930, -0.4070, -1.2570,  0.7430, -2.3070,  0.0930,  1.6930,\n",
       "         2.0430, -1.2570,  0.6930, -2.2570, -2.1070, -1.4570,  1.2430, -1.3070,\n",
       "         2.0930,  1.4430, -1.4070,  0.1430,  0.9430,  1.7430,  1.0930,  2.0430,\n",
       "        -0.5070, -1.5070, -1.3570,  1.1930,  0.2430, -1.3570, -0.1070,  0.2430,\n",
       "        -2.3070,  1.1430, -1.4570,  0.4930,  2.2430, -0.6570,  2.0430,  1.0430,\n",
       "         1.3430, -1.0570,  1.5430, -1.6070, -2.6070, -2.2070,  2.0430, -2.3570,\n",
       "        -2.7070, -1.6570,  1.8430,  0.4430, -0.0070,  0.6930, -1.7070, -2.6070,\n",
       "        -1.9570, -1.4570, -0.5570, -0.6070,  1.5930, -2.2070, -0.3570, -1.9070,\n",
       "        -2.2070,  1.7930,  1.1430,  1.8930,  1.0930, -2.1570,  1.8430,  0.0430,\n",
       "         1.6930, -1.1570, -0.1570, -0.9570, -1.5070, -1.8570, -2.4070, -2.2570,\n",
       "         0.3430, -2.6570, -2.2570,  1.3930,  0.5430, -1.3070,  0.6930,  0.5930,\n",
       "        -1.7070, -0.1070, -1.6570, -0.9070, -0.4570,  1.7930, -1.4070, -2.3070,\n",
       "         0.5930, -2.2070, -0.4070, -2.1570,  0.4930,  0.0430,  0.0930],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fifth_total_shift - torch.mean(fifth_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a5bc45e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T13:03:09.565323Z",
     "start_time": "2023-04-25T13:03:09.452804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcbf94bf130>]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 454\n",
    "plt.plot(xdos, train_pred[i])\n",
    "plt.plot(xdos, total_aligned_dos3[total_train_index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "658fa954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T19:32:10.398587Z",
     "start_time": "2023-04-27T18:45:26.111744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1003:   5%|█████▍                                                                                                      | 1003/20000 [11:16<3:32:43,  1.49it/s, lowest_mse=0.0312, pred_loss=37.1, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2005:  10%|██████████▊                                                                                                 | 2005/20000 [22:02<3:13:24,  1.55it/s, lowest_mse=0.0209, pred_loss=30.4, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3007:  15%|████████████████▏                                                                                           | 3007/20000 [30:09<2:15:46,  2.09it/s, lowest_mse=0.0178, pred_loss=28.1, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4009:  20%|█████████████████████▋                                                                                      | 4009/20000 [38:16<2:07:28,  2.09it/s, lowest_mse=0.0162, pred_loss=26.8, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████▌                                                                                  | 5011/20000 [46:42<2:16:41,  1.83it/s, lowest_mse=0.0154, pred_loss=26, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5011:  25%|███████████████████████████▌                                                                                  | 5011/20000 [46:43<2:19:45,  1.79it/s, lowest_mse=0.0154, pred_loss=26, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Unbiased\n",
      "The train error is 26.04 for SOAP\n",
      "The test error is 27.09 for SOAP\n",
      "The train error is 26.04 for SOAP\n",
      "The test error is 27.09 for SOAP\n"
     ]
    }
   ],
   "source": [
    "true_alignment2 = torch.randint(-5, 5, size = (len(total_aligned_dos3),))\n",
    "\n",
    "random_shifted_aligned_dos3 = shifted_ldos_discrete(total_aligned_dos3, xdos, true_alignment2)\n",
    "\n",
    "aweights6, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, random_shifted_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 20000, 60, 1e-3)\n",
    "\n",
    "\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))\n",
    "\n",
    "# adam_opt_weights = weights.clone()\n",
    "loss_dos, test_loss_dos, opt_shift_train, opt_shift_test = evaluate_weights(aweights6, total_soap, random_shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))\n",
    "\n",
    "train_pred, test_pred, train_shift, test_shift = get_predictions(aweights6, total_soap, random_shifted_aligned_dos3, total_train_index, total_test_index)\n",
    "\n",
    "sixth_total_shift = reverse_index(torch.tensor(train_shift).float(), torch.tensor(test_shift).float(), total_train_index, total_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c420b754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T19:32:10.413191Z",
     "start_time": "2023-04-27T19:32:10.401085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7944,  1.1944,  0.8444,  1.0944,  0.7944,  1.1444,  1.0944,  0.5944,\n",
       "         0.6444,  1.1444,  0.7944,  0.9944,  0.8444,  0.7444,  0.9944,  1.1944,\n",
       "         0.8444,  0.9944,  0.7444,  0.7444,  0.6444,  0.6944,  0.9444,  0.4944,\n",
       "         0.7944,  0.8944,  0.6444,  0.9444,  0.7944,  0.4444,  0.7944,  0.5944,\n",
       "         0.8944,  0.7444,  0.8444,  0.8944,  0.5944,  0.6944,  0.5944,  0.4444,\n",
       "         0.6944,  0.3944,  0.3444,  0.6944,  0.5444,  0.1444,  0.5444,  0.4444,\n",
       "         0.5444,  0.4944,  0.3944,  0.2944,  0.4944,  0.3944,  0.4444,  0.4444,\n",
       "         0.2944,  0.2444,  0.5444,  0.3444,  0.4444,  0.4944,  0.0944,  0.4444,\n",
       "        -0.0056,  0.1944,  0.2444,  0.1944,  0.3944,  0.2444,  0.3444,  0.3944,\n",
       "         0.2944,  0.3944,  0.0444,  0.5944,  0.5444,  0.0944, -0.1556,  0.1944,\n",
       "        -0.0556, -0.0056,  0.1444,  0.1444,  0.2944, -0.0056,  0.3444, -0.0556,\n",
       "         0.2444,  0.2444, -0.2056,  0.1444,  0.2444,  0.1444,  0.1444,  0.0444,\n",
       "        -0.1056,  0.3444, -0.0056,  0.1944, -0.0056,  0.0444,  0.3444, -0.3556,\n",
       "         1.0444,  0.9444,  1.0444,  0.8444,  1.0944,  0.7444,  0.6944,  0.9444,\n",
       "         0.7944,  0.8944,  0.7444,  0.7944,  0.9444,  1.0944,  1.0944,  0.8944,\n",
       "         0.8944,  0.7944,  1.0444,  1.1944,  0.9444,  0.7944,  0.6944,  1.0444,\n",
       "         0.5944,  1.0444,  0.6944,  0.8944,  0.5944,  0.5944,  0.7444,  0.7444,\n",
       "         0.7444,  0.8944,  0.5944,  0.9944,  0.8444,  1.0444,  0.5944,  0.8444,\n",
       "         0.6444,  0.5944,  0.9444,  0.7944,  0.8944,  0.7444,  0.8444,  0.8444,\n",
       "         0.7444,  0.5444,  0.9444,  0.6944,  0.8944,  0.5944,  0.5944,  0.8944,\n",
       "         0.6444,  0.4944,  0.5444,  0.8444,  0.7444,  0.4444,  0.4944,  0.7444,\n",
       "         0.7944,  0.5444,  0.6944,  0.8444,  0.3944,  0.6444,  0.5444,  0.6444,\n",
       "         0.4444,  0.5944,  0.7444,  0.5944,  0.4444,  0.7444,  0.3944,  0.5944,\n",
       "         0.7444,  0.2944,  0.7944,  0.6944,  0.7944,  0.4944,  0.5444,  0.5944,\n",
       "         0.7944,  0.6944,  0.3944,  0.3444,  0.5444,  0.3444,  0.5444,  0.3944,\n",
       "         0.7444,  0.3444,  0.5444,  0.4944,  0.5944,  0.2444,  0.4444,  0.4444,\n",
       "         0.6944,  0.5944,  0.6444,  0.2444,  0.6944,  0.2944,  0.3444,  0.5444,\n",
       "         0.5944,  0.4944,  0.6944,  0.5444,  0.5444,  0.2944,  0.2444,  0.5444,\n",
       "         0.2444,  0.2444,  0.2944,  0.1944,  0.1944,  0.6444,  0.4944,  0.2944,\n",
       "         0.3444,  0.4444,  0.4444,  0.6444,  0.3944,  0.1944,  0.5944,  0.6444,\n",
       "         0.2944,  0.1944,  0.3444,  0.3444,  0.2444,  0.2944,  0.5444,  0.1944,\n",
       "         0.4444,  0.2444,  0.5444,  0.4944,  0.1944,  0.2444,  0.4444,  0.3444,\n",
       "         0.5444,  0.3944,  0.2944,  0.2944,  0.1444,  0.4944,  0.4944,  0.4944,\n",
       "         0.4444,  0.1444,  0.3944,  0.4944,  0.0944,  0.0444,  0.1444,  0.1944,\n",
       "         0.3444,  0.4944,  0.3944,  0.1444,  0.4444,  0.1444,  0.4444,  0.0944,\n",
       "         0.3944,  0.3444,  0.4444,  0.5944,  0.3944,  0.3944,  0.1444,  0.1944,\n",
       "         0.1944,  0.0444,  0.0944,  0.3944,  0.1944,  0.6444,  0.3944,  0.3444,\n",
       "         0.5944,  0.5944,  0.4944,  0.3944,  0.5944,  0.1944,  0.2944,  0.2444,\n",
       "         0.3444,  0.1944,  0.2944,  0.0944,  0.1444,  0.3444,  0.4444,  0.4444,\n",
       "         0.3944,  0.1944,  0.5444,  0.6444,  0.4944,  0.2944,  0.2944,  0.3444,\n",
       "         0.5944,  0.6944,  0.4444,  0.3444, -3.0556, -0.7556, -2.0056, -2.3556,\n",
       "        -0.3056, -0.3556, -2.8056, -0.7056, -1.8056, -2.3056, -1.6056, -0.4556,\n",
       "        -1.4556, -3.4056, -2.3056, -1.7556, -2.3056, -1.0056, -1.6056, -4.3556,\n",
       "        -3.3056, -2.2556, -5.0056, -3.1056, -2.7556, -2.4056, -1.8556, -2.3556,\n",
       "        -1.9556, -1.5056, -3.5556, -3.7056, -2.5056, -2.1056, -2.7556, -1.9056,\n",
       "        -1.7556, -3.3556, -1.5556, -2.1556, -2.9556, -3.6556, -2.6056, -2.3056,\n",
       "        -2.0056, -3.1556, -2.3056, -2.8056, -3.7556, -1.6556, -1.5056, -1.2556,\n",
       "        -1.5556, -2.2056, -0.9056, -1.8556, -2.3056, -2.0556, -0.1056, -1.7556,\n",
       "        -1.9556, -1.9056, -2.1056, -2.1056, -1.8056, -2.2056, -2.1556, -2.0056,\n",
       "        -2.2056, -2.1556, -2.1556, -2.2056, -2.0556, -1.8556, -2.2556, -2.1556,\n",
       "        -2.1056, -2.2056, -2.1556, -2.1056, -1.8556, -1.8556, -1.8556, -1.9556,\n",
       "        -2.2056, -2.2056, -2.1556, -2.0556, -1.8056, -1.9556, -2.1056, -1.8556,\n",
       "        -1.8556, -2.2056, -2.0056, -2.1056, -2.1556, -2.0056, -1.8556, -2.1056,\n",
       "        -2.1556, -2.0556, -2.2056, -2.0056, -2.2056, -2.0056, -2.1556, -2.0056,\n",
       "        -2.0556, -2.1556, -2.2556, -2.1556, -2.3056, -2.2056, -2.2056, -1.9556,\n",
       "        -2.0556, -2.0556, -2.1056, -2.4056, -2.3056, -2.0556, -2.1556, -2.1556,\n",
       "        -2.1056, -1.9556, -1.9056, -2.1556, -1.9056, -1.7056, -1.8556, -1.9056,\n",
       "        -2.0556, -2.1556, -1.8056, -2.1056, -2.0556, -1.9056, -1.8056, -1.9056,\n",
       "        -1.9556, -1.9056, -2.1056, -1.8556, -2.1056, -2.1056, -1.7056, -1.7556,\n",
       "        -2.0556, -1.7056, -1.7056, -2.0056, -1.8056, -1.7056, -1.9556, -2.1556,\n",
       "        -2.0556, -1.8556, -1.9556, -1.9556, -1.8556, -1.9056, -1.7056, -2.2056,\n",
       "        -1.8556, -1.9056, -2.0556, -1.8056, -2.2056, -1.8056, -1.9056, -2.0556,\n",
       "        -2.0556, -1.9556, -1.8556, -1.9056, -1.9056, -2.1056, -1.8556, -2.2056,\n",
       "        -2.2056, -2.2056, -2.1556, -1.8556, -1.8556, -1.8556, -2.1056, -2.2056,\n",
       "        -2.2556, -1.9556, -2.3056, -2.0556, -1.8556, -1.9556, -1.8556, -1.8556,\n",
       "        -2.2556, -1.8556, -2.2556, -2.2556, -2.0056, -2.3056, -2.2556, -2.3056,\n",
       "        -2.1556, -1.9556, -2.1056, -1.9556, -2.3556, -2.2556, -2.1056, -2.0056,\n",
       "        -1.9556, -2.2056, -2.1556, -1.9556, -2.0556, -2.3556, -2.1556, -1.9056,\n",
       "        -1.9556, -2.3556, -2.0556, -2.2556, -1.9556, -2.0556, -2.2556, -2.3056,\n",
       "        -2.1056, -2.3556, -1.9556, -2.1556, -2.1056, -2.3556, -1.9556, -2.2056,\n",
       "        -2.0556, -1.9056, -2.0556, -2.2056, -2.3056, -2.0056, -1.9556, -2.1056,\n",
       "        -2.0056, -1.9556, -2.3056, -1.9556, -2.0556, -2.3556, -2.2056, -2.2056,\n",
       "        -2.4056, -2.0056, -2.2556, -2.3556, -2.3556, -2.1556, -2.0056, -2.1556,\n",
       "        -1.9556, -2.2056, -2.0556, -2.1556, -2.3556, -1.9556, -2.0556, -2.2056,\n",
       "        -2.3556, -1.9556, -2.1556, -2.0556, -2.1056, -1.9556, -2.0556, -1.9556,\n",
       "        -2.2056, -2.4056, -2.4056, -2.3056,  1.1944,  0.9444,  1.1944,  0.8944,\n",
       "         1.3444,  0.9944,  0.8444,  0.8944,  1.2944,  0.9944,  1.3444,  0.9444,\n",
       "         1.1444,  1.0944,  1.2944,  1.1944,  1.1444,  1.2444,  0.7444,  0.6444,\n",
       "         0.9444,  0.7444,  0.7944,  0.7944,  0.6944,  1.0444,  0.6444,  0.6944,\n",
       "         1.2444,  1.0944,  1.2944,  1.5444,  1.3444,  1.2444,  1.3944,  1.2944,\n",
       "         1.1944,  1.0944,  0.9944,  1.0444,  0.9444,  1.1444,  0.7944,  0.8444,\n",
       "         1.0944,  0.9944,  0.8444,  0.9944,  1.2944,  1.2944,  1.2944,  1.4944,\n",
       "         1.2944,  0.5444,  0.6944,  0.7444,  0.5944,  0.5944,  0.5444,  0.9444,\n",
       "         0.7944,  0.5944,  0.6944,  0.7444,  1.0444,  1.0444,  1.3444,  0.9944,\n",
       "         1.1444,  1.8444,  1.5944,  1.3444,  1.6444,  1.7444,  1.6944,  1.7944,\n",
       "         1.6944,  1.8944,  1.9444,  1.9944,  1.5944,  1.8944,  1.5944,  1.8444,\n",
       "         1.7444,  1.6944,  1.9444,  1.6944,  1.9944,  1.8444,  1.9444,  1.5944,\n",
       "         1.9944,  1.8444,  1.7944,  1.6944,  1.5444,  1.9944,  1.7444,  1.9944,\n",
       "         1.4444,  1.7944,  1.6944,  1.9944,  1.5944,  1.7944,  1.8444,  1.7444,\n",
       "         1.9444,  1.9444,  1.6444,  1.5444,  1.7444,  1.9944,  1.9444,  1.8944,\n",
       "         1.8944,  1.8444,  1.6444,  1.9944,  1.8444,  1.5444,  1.6944,  1.6444,\n",
       "         1.5444,  1.8444,  1.4944,  1.3444,  1.6944,  1.5944,  1.6444,  1.3944,\n",
       "         1.3444,  1.5944,  1.3444,  1.4944,  1.6444,  1.2944,  1.5444,  1.3444,\n",
       "         1.2944,  1.5444,  1.3944,  1.5444,  1.5944,  1.3444,  1.6944,  1.7444,\n",
       "         1.3444,  1.5444,  1.6444,  1.4444,  1.3444,  1.6944,  1.4944,  1.7944,\n",
       "         1.3444,  1.4944,  1.6944,  1.6444,  1.7444,  1.6444,  1.6944,  1.3444,\n",
       "         1.3444,  1.4944,  1.3944,  1.4444,  1.2944,  1.6944,  1.3444,  1.3944,\n",
       "         1.4444,  1.5944,  1.2944,  1.3944,  1.4944,  1.5944,  1.2444,  1.3444,\n",
       "         1.2944,  1.1944,  1.2444,  1.5444,  1.3944,  1.2444,  1.3444,  1.4944,\n",
       "         1.6944,  1.4444,  1.5444,  1.1944,  1.4944,  1.6444,  1.6444,  1.6444,\n",
       "         1.1944,  1.1944,  1.4444,  1.5944,  1.3944,  1.5944,  1.1444,  1.6444,\n",
       "         1.5944,  1.6444,  1.1944,  1.5944,  1.2944,  1.3444,  1.5944,  1.2944,\n",
       "         1.4944,  1.4944,  1.2944,  1.5444,  1.3444,  1.6444,  1.6444,  1.3944,\n",
       "         1.3944,  1.2944,  1.3944,  1.1444,  1.2944,  1.0444,  1.0444,  0.9944,\n",
       "         0.9944,  0.8944,  0.9444,  1.1944,  1.0444,  0.8944,  0.8444,  0.9944,\n",
       "         1.2444,  1.1444,  0.9444,  1.1444,  1.1944,  1.0944,  1.2944,  0.8944,\n",
       "         1.0444,  1.2444,  1.2444,  0.7944,  0.7944,  0.8944,  0.9444,  0.7944,\n",
       "         0.8444,  0.9444,  1.2444,  0.7944,  0.9444,  0.8444,  1.2944,  1.0944,\n",
       "         1.0444,  0.8444,  1.1944,  0.9444,  0.9944,  0.9444,  0.9944,  1.0444,\n",
       "         0.8944,  1.2444,  1.2944,  1.0944,  1.1444,  0.7944,  1.0944,  0.9944,\n",
       "         0.8944,  1.1944,  1.1444,  0.9444,  1.0944,  1.0944,  1.1944,  1.2444,\n",
       "         1.0444,  1.0944,  0.9444,  1.1444,  0.9944,  1.2944,  1.1444,  1.2444,\n",
       "         1.1444,  1.2444,  0.8944,  0.8944,  1.0444,  0.9944,  0.7944,  1.0444,\n",
       "         1.2944,  0.9944,  0.8944,  0.9444,  1.0444,  1.1444,  1.0944,  0.9944,\n",
       "         0.9944,  0.9944,  1.2444,  1.2444,  1.1444,  0.8944,  0.8944,  0.9944,\n",
       "         0.9944,  1.2944,  1.1444,  1.2944,  1.2444,  1.1944,  1.0444],\n",
       "       dtype=torch.float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sixth_total_shift - torch.mean(sixth_total_shift)) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8215313",
   "metadata": {},
   "source": [
    "### Tests - single structure shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a94482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532f9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c959697d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T14:20:59.059924Z",
     "start_time": "2023-04-27T14:20:59.055172Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964d002",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-27T11:56:21.315Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de87946",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-27T11:56:20.768Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dd988b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:12:43.352218Z",
     "start_time": "2023-04-25T11:12:35.214200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4:   7%|████████                                                                                                                 | 4/60 [00:07<01:50,  1.98s/it, lowest_mse=0.259, pred_loss=193, trigger=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [132]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m U_L_weights3 , loss_dos, test_loss_dos \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_reg_train_L\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurface_soap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_dos3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_test_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBFGS Unbiased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train error is \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m for SOAP\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss_dos))\n",
      "Input \u001b[0;32mIn [126]\u001b[0m, in \u001b[0;36mnormal_reg_train_L\u001b[0;34m(feat, target, train_index, test_index, regularization, n_epochs, lr)\u001b[0m\n\u001b[1;32m     38\u001b[0m     loss_i\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_i\n\u001b[0;32m---> 40\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     43\u001b[0m     preds \u001b[38;5;241m=\u001b[39m Features \u001b[38;5;241m@\u001b[39m weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    429\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:148\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m     insuf_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Evaluate new point\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m ls_func_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    150\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    279\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [126]\u001b[0m, in \u001b[0;36mnormal_reg_train_L.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m pred_i \u001b[38;5;241m=\u001b[39m reg_features \u001b[38;5;241m@\u001b[39m weights\n\u001b[0;32m---> 35\u001b[0m opt_shift \u001b[38;5;241m=\u001b[39m \u001b[43mfind_optimal_discrete_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m pred_i[:\u001b[38;5;28mlen\u001b[39m(index)] \u001b[38;5;241m=\u001b[39m shifted_ldos_discrete(pred_i[:\u001b[38;5;28mlen\u001b[39m(index)], xdos, torch\u001b[38;5;241m.\u001b[39mtensor(opt_shift))\n\u001b[1;32m     37\u001b[0m loss_i \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mt_get_mse(pred_i, reg_target)\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mfind_optimal_discrete_shift\u001b[0;34m(prediction, true)\u001b[0m\n\u001b[1;32m      4\u001b[0m shift \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m----> 6\u001b[0m     corr \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     shift_i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(corr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(true[i]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m   \n\u001b[1;32m      8\u001b[0m     shift\u001b[38;5;241m.\u001b[39mappend(shift_i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/signal/signaltools.py:239\u001b[0m, in \u001b[0;36mcorrelate\u001b[0;34m(in1, in2, mode, method)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# this either calls fftconvolve or this function with method=='direct'\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43min1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_reverse_and_conj\u001b[49m\u001b[43m(\u001b[49m\u001b[43min2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# fastpath to faster numpy.correlate for 1d inputs when possible\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _np_conv_ok(in1, in2, mode):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/scipy/signal/signaltools.py:1408\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(in1, in2, mode, method)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     \u001b[38;5;66;03m# fastpath to faster numpy.convolve for 1d inputs when possible\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _np_conv_ok(volume, kernel, mode):\n\u001b[0;32m-> 1408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correlate(volume, _reverse_and_conj(kernel), mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py:844\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "U_L_weights3 , loss_dos, test_loss_dos = normal_reg_train_L(surface_soap, surface_dos3, surface_train_index, surface_test_index,\n",
    "                                                            1e-2, 60, 1)\n",
    "print (\"LBFGS Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d6fe123c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:21:51.115360Z",
     "start_time": "2023-04-25T11:17:37.519949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 29: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [04:13<00:00,  8.44s/it, lowest_mse=0.0537, pred_loss=49.2, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Unbiased\n",
      "The train error is 49.19 for SOAP\n",
      "The test error is 49.19 for SOAP\n"
     ]
    }
   ],
   "source": [
    "U_L_weights3 , loss_dos, test_loss_dos = normal_reg_train_L(total_soap, total_aligned_dos3,\n",
    "                                                            total_train_index, total_test_index,\n",
    "                                                            1e-2, 30, 1)\n",
    "print (\"LBFGS Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11eeae",
   "metadata": {},
   "source": [
    "22.36, 24.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a4db7317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T11:32:41.829922Z",
     "start_time": "2023-04-25T11:21:51.117999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1005:  10%|███████████                                                                                                   | 1005/10000 [01:53<15:39,  9.57it/s, lowest_mse=0.0152, pred_loss=46.7, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2007:  20%|█████████████████████▉                                                                                       | 2007/10000 [03:28<12:29, 10.66it/s, lowest_mse=0.00871, pred_loss=35.4, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3009:  30%|████████████████████████████████▊                                                                            | 3009/10000 [04:58<10:23, 11.22it/s, lowest_mse=0.00751, pred_loss=32.8, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4011:  40%|███████████████████████████████████████████▋                                                                 | 4010/10000 [06:27<08:41, 11.50it/s, lowest_mse=0.00695, pred_loss=31.6, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5013:  50%|██████████████████████████████████████████████████████▋                                                      | 5013/10000 [07:56<07:13, 11.51it/s, lowest_mse=0.00656, pred_loss=30.7, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6015:  60%|██████████████████████████████████████████████████████████████████▊                                            | 6015/10000 [09:23<05:45, 11.54it/s, lowest_mse=0.00628, pred_loss=30, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7015:  70%|████████████████████████████████████████████████████████████████████████████▍                                | 7015/10000 [10:50<04:36, 10.78it/s, lowest_mse=0.00604, pred_loss=29.4, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  2048\n",
      "Adam Unbiased\n",
      "The train error is 29.43 for SOAP\n",
      "The test error is 38.52 for SOAP\n"
     ]
    }
   ],
   "source": [
    "weights, loss_dos, test_loss_dos = normal_reg_train_Ad(surface_soap, surface_dos3, surface_train_index, surface_test_index, 1e-2, 10000, 16, 1e-3)\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "497.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
