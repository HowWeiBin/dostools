{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db828e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T13:05:52.911575Z",
     "start_time": "2023-04-26T13:05:52.903509Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import time\n",
    "import scipy \n",
    "import ase\n",
    "import ase.io\n",
    "torch.set_default_dtype(torch.float64) \n",
    "# %matplotlib notebook\n",
    "# matplotlib.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dd7bb87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:12:11.572873Z",
     "start_time": "2023-04-26T14:12:11.494119Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.datasets.data as data\n",
    "import dostools.utils.utils as utils\n",
    "\n",
    "with torch.no_grad():\n",
    "#     sigma = 0.3\n",
    "#     structures = data.load_structures(\":\")\n",
    "#     n_structures = len(structures) #total number of structures\n",
    "#     for structure in structures:#implement periodicity\n",
    "#         structure.wrap(eps = 1e-12) \n",
    "#     n_atoms = np.zeros(n_structures, dtype = int) #stores number of atoms in each structures\n",
    "#     for i in range(n_structures):\n",
    "#         n_atoms[i] = len(structures[i])\n",
    "        \n",
    "    xdos = torch.tensor(data.load_xdos())\n",
    "    \n",
    "    total_dos3 = torch.load(\"./total_ldos3.pt\")\n",
    "    total_dos1 = torch.load(\"./total_ldos1.pt\")\n",
    "    \n",
    "    surface_dos3 = torch.load(\"./surface_ldos3.pt\")\n",
    "    surface_dos1 = torch.load(\"./surface_ldos1.pt\")\n",
    "    \n",
    "    surface_aligned_dos3 = torch.load(\"./surface_aligned_dos3.pt\")\n",
    "    surface_aligned_dos1 = torch.load(\"./surface_aligned_dos1.pt\")\n",
    "    \n",
    "    bulk_dos3 = torch.load(\"./bulk_ldos3.pt\")\n",
    "    bulk_dos1 = torch.load(\"./bulk_ldos1.pt\")\n",
    "    \n",
    "    total_aligned_dos3 = torch.load(\"./total_aligned_dos3.pt\")\n",
    "    total_aligned_dos1 = torch.load(\"./total_aligned_dos1.pt\")\n",
    "    \n",
    "    surface_soap = torch.load(\"./surface_soap.pt\")\n",
    "    bulk_soap = torch.load(\"./bulk_soap.pt\")\n",
    "    total_soap = torch.load(\"./total_soap.pt\")\n",
    "    \n",
    "    surface_kernel_30 = torch.load(\"./surface_kernel_30.pt\")\n",
    "    surface_kMM_30 = torch.load(\"./surface_kMM_30.pt\")\n",
    "    \n",
    "    bulk_kernel_200 = torch.load(\"./bulk_kernel_200.pt\")\n",
    "    bulk_kMM_200 = torch.load(\"./bulk_kMM_200.pt\")\n",
    "    \n",
    "    bulk_kernel_100 = torch.load(\"./bulk_kernel_100.pt\")\n",
    "    bulk_kMM_100 = torch.load(\"./bulk_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_100 = torch.load(\"./total_kernel_100.pt\")\n",
    "    total_kMM_100 = torch.load(\"./total_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_150 = torch.load(\"./total_kernel_150.pt\")\n",
    "    total_kMM_150 = torch.load(\"./total_kMM_150.pt\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3b26e208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:09:50.489342Z",
     "start_time": "2023-04-26T14:09:50.423964Z"
    }
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "\n",
    "#     cutoff_index = torch.tensor(487)\n",
    "#     xdos = xdos[:cutoff_index]\n",
    "\n",
    "#     total_dos3 = torch.load(\"./total_ldos3.pt\")[:,:cutoff_index]\n",
    "#     total_dos1 = torch.load(\"./total_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     surface_dos3 = torch.load(\"./surface_ldos3.pt\")[:,:cutoff_index]\n",
    "#     surface_dos1 = torch.load(\"./surface_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     surface_aligned_dos3 = torch.load(\"./surface_aligned_dos3.pt\")[:,:cutoff_index]\n",
    "#     surface_aligned_dos1 = torch.load(\"./surface_aligned_dos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     bulk_dos3 = torch.load(\"./bulk_ldos3.pt\")[:,:cutoff_index]\n",
    "#     bulk_dos1 = torch.load(\"./bulk_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     total_aligned_dos3 = torch.load(\"./total_aligned_dos3.pt\")[:,:cutoff_index]\n",
    "#     total_aligned_dos1 = torch.load(\"./total_aligned_dos1.pt\")[:,:cutoff_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "15107a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:10:41.923619Z",
     "start_time": "2023-04-26T14:10:41.902919Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train_test_split(n_samples):\n",
    "    n_structures = n_samples\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_structures)\n",
    "    train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(train_index)\n",
    "    test_index = train_index[n_train:]\n",
    "    train_index = train_index[:n_train]\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "def generate_biased_train_test_split(n_samples):\n",
    "    #Assumes 100 amorphous structures at the end\n",
    "    n_structures = n_samples\n",
    "    amorph_train = np.arange(n_samples-100, n_samples,1)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(amorph_train)\n",
    "    \n",
    "    amorph_test = amorph_train[:80]\n",
    "    amorph_train = amorph_train[80:]\n",
    "\n",
    "    n_structures = n_samples - 100\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_samples)-20\n",
    "    remaining_train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(remaining_train_index)\n",
    "\n",
    "    remaining_test_index = remaining_train_index[n_train:]\n",
    "    remaining_train_index = remaining_train_index[:n_train]\n",
    "\n",
    "    biased_train_index = np.concatenate([remaining_train_index, amorph_train])\n",
    "    biased_test_index = np.concatenate([remaining_test_index, amorph_test])\n",
    "    \n",
    "    return biased_train_index, biased_test_index\n",
    "\n",
    "def generate_surface_holdout_split(n_samples):\n",
    "    #Assumes that we are using the 110 surfaces for test which are located at 673 + 31st-57th index\n",
    "    #26 structures\n",
    "    \n",
    "    n_test = int(0.2 * n_samples) - 26\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    remaining_indexes = np.concatenate([np.arange(673+31), np.arange(673+57,n_samples,1)])\n",
    "    indexes_110 = np.arange(673+31, 673+57,1)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    np.random.shuffle(remaining_indexes)\n",
    "    \n",
    "    remaining_test_index = remaining_indexes[n_train:]\n",
    "    remaining_train_index = remaining_indexes[:n_train]\n",
    "    \n",
    "    total_train_index = remaining_train_index\n",
    "    total_test_index = np.concatenate([remaining_test_index, indexes_110])\n",
    "    \n",
    "    return total_train_index, total_test_index\n",
    "    \n",
    "def surface_holdout(n_samples):\n",
    "    test_index = np.arange(31,57,1)\n",
    "    train_index = np.concatenate([np.arange(31), np.arange(57, n_samples)])\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "n_surfaces = 154\n",
    "n_bulkstructures = 773\n",
    "n_total_structures = 773 + 154\n",
    "\n",
    "\n",
    "surface_train_index, surface_test_index = generate_train_test_split(n_surfaces)\n",
    "bulk_train_index, bulk_test_index = generate_train_test_split(n_bulkstructures)\n",
    "total_train_index, total_test_index = generate_train_test_split(n_total_structures)\n",
    "surface_holdout_train_index, surface_holdout_test_index = surface_holdout(n_surfaces)\n",
    "bulk_biased_train_index, bulk_biased_test_index = generate_biased_train_test_split(n_bulkstructures)\n",
    "total_biased_train_index, total_biased_test_index = generate_biased_train_test_split(n_total_structures)\n",
    "holdout_train_index, holdout_test_index = generate_surface_holdout_split(n_total_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "11044659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T19:03:36.756599Z",
     "start_time": "2023-04-26T19:03:36.749856Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat = torch.rand(1000).reshape(20, 5, 10)\n",
    "# dog = torch.rand(1000).reshape(20, 5, 10)\n",
    "mouse = torch.sum((cat - dog) **2, axis = 2)\n",
    "mouse =  (mouse.reshape(20,5))\n",
    "\n",
    "a, b = torch.min(mouse, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508a236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "77d384b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T19:17:32.948286Z",
     "start_time": "2023-04-26T19:17:32.930644Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_each_mse(predictions, true, xdos = None):\n",
    "    #takes a 3d array for predictions and true\n",
    "    if xdos is not None:\n",
    "        mse = torch.trapezoid((predictions - true)**2, xdos, axis = 2)\n",
    "    else:\n",
    "        mse = torch.sum((predictions - true)**2 , axis = 2)\n",
    "        \n",
    "    return mse\n",
    "\n",
    "def t_get_rmse(a, b, xdos=None, perc=False): #account for the fact that DOS is continuous but we are training them pointwise\n",
    "    \"\"\" computes  Root Mean Squared Error (RMSE) of array properties (DOS/aofd).\n",
    "         a=pred, b=target, xdos, perc: if False return RMSE else return %RMSE\"\"\"\n",
    "    #MIGHT NOT WORK FOR PC\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=1)).mean())\n",
    "        else:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=0)).mean())\n",
    "        if not perc:\n",
    "            return rmse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.sqrt((torch.trapezoid((b - mean)**2, xdos, axis=1)).mean())\n",
    "            return (100 * rmse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean(dim =0))\n",
    "        else:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean())\n",
    "        if not perc:\n",
    "            return torch.mean(rmse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (rmse / b.std(dim = 0,unbiased=True)), 0)\n",
    "        \n",
    "def t_get_mse(a, b, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=1)).mean()\n",
    "        else:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=0)).mean()\n",
    "        if not perc:\n",
    "            return mse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.trapezoid((b - mean)**2, xdos, axis=1).mean()\n",
    "            return (100 * mse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = ((a - b)**2).mean(dim = 1)\n",
    "        else:\n",
    "            mse = ((a - b)**2).mean()\n",
    "        if len(mse.shape) > 1:\n",
    "            raise ValueError('Loss became 2D')\n",
    "        if not perc:\n",
    "            return torch.mean(mse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (mse / b.std(dim=0, unbiased = True)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "de384dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:30:29.829429Z",
     "start_time": "2023-04-27T09:30:28.514505Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment = torch.rand(927)\n",
    "trueshift = torch.round(alignment/(xdos[1] - xdos[0])).int()\n",
    "test_ldos0 = shifted_ldos_discrete(total_dos1, torch.ones(931))\n",
    "shift_range = torch.arange(2 * full_range + 1) - full_range\n",
    "l, s = t_get_BF_shift_index_mse(test_ldos0[:,:cutoff_index], total_dos1, shift_range, cutoff_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "109023bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:02:04.695783Z",
     "start_time": "2023-04-27T09:02:04.682732Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_BF_shift_index_mse(prediction, true, shift_range, cutoff_index):\n",
    "    #shifts target instead of prediction\n",
    "    if len(prediction.shape) > 1:\n",
    "        shifted_true = shifted_ldos_discrete(true.repeat(shift_range.shape[0], 1, 1), shift_range)[:,:,:cutoff_index]\n",
    "        full_loss = t_get_each_mse(prediction.repeat(shift_range.shape[0], 1, 1), shifted_true)\n",
    "        full_loss = full_loss.reshape(shift_range.shape[0], -1)\n",
    "        min_loss, index = torch.min(full_loss, dim = 0)\n",
    "        \n",
    "    else:\n",
    "        shifted_true = shifted_ldos_discrete(true.repeat(shift_range.shape[0], 1), shift_range)[:,:,:cutoff_index]\n",
    "        full_loss = t_get_each_mse(prediction.repeat(shift_range.shape[0], 1), shifted_true)\n",
    "        min_loss, index = torch.min(full_loss, dim = 0)\n",
    "        min_loss = full_loss[index]\n",
    "    return min_loss, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f483cbd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T18:49:10.571677Z",
     "start_time": "2023-04-26T18:49:10.557362Z"
    }
   },
   "outputs": [],
   "source": [
    "def shifted_ldos_discrete(ldos, shift): \n",
    "    shifted_ldos = torch.zeros_like(ldos)\n",
    "    if len(ldos.shape) == 3:\n",
    "        for i in range(len(ldos)):\n",
    "            if shift[i] > 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i, :, :-1*shift[i]], (shift[i], 0))\n",
    "            elif shift[i] < 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i, :, (-1*shift[i]):], (0, (-1*shift[i])))\n",
    "            else:\n",
    "                shifted_ldos[i] = ldos[i]\n",
    "    elif len(ldos.shape) == 2:\n",
    "        xdos_shift = torch.round(shift).int()\n",
    "        for i in range(len(ldos)):\n",
    "            if xdos_shift[i] > 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,:-1*xdos_shift[i]], (xdos_shift[i],0))\n",
    "            elif xdos_shift[i] < 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,(-1*xdos_shift[i]):], (0,(-1*xdos_shift[i])))\n",
    "            else:\n",
    "                shifted_ldos[i] = ldos[i]\n",
    "    else:        \n",
    "        xdos_shift = int(torch.round(shift))\n",
    "        if xdos_shift > 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[:-1*xdos_shift], (xdos_shift,0))\n",
    "        elif xdos_shift < 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[(-1*xdos_shift):], (0,(-1*xdos_shift)))\n",
    "        else:\n",
    "            shifted_ldos = ldos\n",
    "    return shifted_ldos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "447e16bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T15:18:36.732312Z",
     "start_time": "2023-04-24T15:18:36.728676Z"
    }
   },
   "outputs": [],
   "source": [
    "# alignment = torch.rand(927)\n",
    "# trueshift = torch.round(alignment/(xdos[1] - xdos[0])).int()\n",
    "# test_ldos0 = shifted_ldos(total_dos1, xdos, torch.ones(931))\n",
    "# optshift = find_optimal_discrete_shift(np.array(test_ldos0),np.array(total_dos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "8a5fb5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:07:45.183433Z",
     "start_time": "2023-04-27T09:07:45.151970Z"
    }
   },
   "outputs": [],
   "source": [
    "def normal_reg_train_L(cutoff_index, shift_range, feat, target, train_index, test_index, regularization, n_epochs, lr):\n",
    "    \n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "    reg_features = torch.vstack([Features, reg])\n",
    "    reg_target = torch.vstack([Target, torch.zeros(n_col,Target.shape[1])])\n",
    "    \n",
    "    alignment = torch.zeros(len(index))\n",
    "\n",
    "    weights = torch.nn.Parameter(torch.rand(Features.shape[1], cutoff_index)- 0.5)\n",
    "    \n",
    "    opt = torch.optim.LBFGS([weights], lr = lr, line_search_fn = \"strong_wolfe\", tolerance_grad = 1e-20, tolerance_change = 1-20, history_size = 200)\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        def closure():\n",
    "            print (\"--------HEY------\")\n",
    "            opt.zero_grad()\n",
    "            pred_i = reg_features @ weights\n",
    "            loss_i, jitter = t_get_BF_shift_index_mse(pred_i[:len(index)], reg_target[:len(index)], shift_range, cutoff_index)\n",
    "            print (loss_i) #FIND OUT WHY THE LOSS_i is not the same as the bottom one\n",
    "            print (torch.mean(loss_i))\n",
    "            #change this code such that we dont use the loss but we get the jitter and shift it again\n",
    "            jitter -= shift_range[0]\n",
    "            reg_target_i = reg_target.clone()\n",
    "            reg_target_i[:len(index)] = shifted_ldos_discrete(reg_target_i[:len(index)], jitter)\n",
    "            loss_i = t_get_mse(pred_i, reg_target_i[:, :cutoff_index])\n",
    "            print (loss_i)\n",
    "            \n",
    "            loss_i.backward()\n",
    "            return loss_i\n",
    "        opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            target = Target.clone()\n",
    "#             target = shifted_ldos_discrete(target, -1 * alignment)\n",
    "            epoch_mse, jitter = t_get_BF_shift_index_mse(preds, target, shift_range, cutoff_index)\n",
    "            jitter -= shift_range[0]\n",
    "            alignment = -1 * jitter\n",
    "            \n",
    "            epoch_mse = torch.mean(epoch_mse)\n",
    "            target = Target.clone()\n",
    "            target = shifted_ldos_discrete(target, -1 * jitter)\n",
    "            \n",
    "            epoch_rmse = t_get_rmse(preds, target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "            \n",
    "            \n",
    "            pred_loss = torch.mean(epoch_rmse)\n",
    "\n",
    "            if epoch_mse < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            epoch_mse = prev_loss\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print (\"NOT FIXED YET\")\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        shifted_true = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "        loss_dos = loss.t_get_rmse(final_preds, Target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "        test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "        return best_state, loss_dos, test_loss_dos\n",
    "        \n",
    "\n",
    "def normal_reg_train_Ad(cutoff_index, shift_range, feat, target, train_index, test_index, regularization, n_epochs, batch_size, lr):\n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "\n",
    "    Sampler = torch.utils.data.RandomSampler(index, replacement = False)\n",
    "    Batcher = torch.utils.data.BatchSampler(Sampler, batch_size, False)\n",
    "\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "\n",
    "\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "\n",
    "\n",
    "    # reg_features = torch.vstack([Features, reg])\n",
    "    # reg_target = torch.vstack([Target, torch.zeros(n_col,Target.shape[1])])\n",
    "\n",
    "\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "    \n",
    "#     alignment = torch.zeros(len(index))\n",
    "\n",
    "    weights = torch.nn.Parameter((torch.rand(Features.shape[1], Target.shape[1])- 0.5))\n",
    "    opt = torch.optim.Adam([weights], lr = lr, weight_decay = 0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = 500, threshold = 1e-7, min_lr = 1e-8)\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        for i_batch in Batcher:\n",
    "            def closure():\n",
    "                opt.zero_grad()\n",
    "                reg_features_i = torch.vstack([Features[i_batch], reg])\n",
    "                pred_i = reg_features_i @ weights\n",
    "#                 shifted_target_i = shifted_ldos_discrete(Target[i_batch].clone(), xdos, -1 * alignment)\n",
    "                loss_i, jitter = t_get_BF_shift_index_mse(pred_i[:len(index)], reg_target_n[:len(index)], shift_range, cutoff_index)\n",
    "                target_i = torch.vstack([shifted_target_i, torch.zeros(n_col, Target.shape[1])])\n",
    "                loss_i = loss.t_get_mse(pred_i, target_i)\n",
    "                loss_i.backward()\n",
    "                return loss_i\n",
    "            opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            opt_shift = find_optimal_discrete_shift(np.array(preds),np.array(Target))\n",
    "            preds = shifted_ldos_discrete(preds, xdos, torch.tensor(opt_shift))\n",
    "            epoch_rmse = loss.t_get_rmse(preds, Target, xdos, perc = True)\n",
    "            epoch_mse = loss.t_get_mse(preds, Target, xdos)\n",
    "\n",
    "\n",
    "            pred_loss = torch.mean(epoch_rmse)\n",
    "\n",
    "            if pred_loss < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            prev_loss = pred_loss\n",
    "\n",
    "            scheduler.step(epoch_mse)\n",
    "\n",
    "            if Batcher.batch_size > 1024:\n",
    "                break\n",
    "\n",
    "            if opt.param_groups[0]['lr'] < 1e-4:\n",
    "                Batcher.batch_size *= 2\n",
    "                opt.param_groups[0]['lr'] = lr\n",
    "                print (\"The batch_size is now: \", Batcher.batch_size)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_preds = Features @ best_state \n",
    "        final_t_preds = t_Features @ best_state\n",
    "\n",
    "        opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "        final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "        opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "        final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "        loss_dos = loss.t_get_rmse(final_preds, Target, xdos, perc = True)\n",
    "        test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target, xdos, perc = True)\n",
    "        return best_state, loss_dos, test_loss_dos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e74dc451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:42:56.198029Z",
     "start_time": "2023-04-27T08:42:56.190359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xdos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d770a109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:46:05.682484Z",
     "start_time": "2023-04-27T08:46:05.677015Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_index = torch.tensor(487)\n",
    "full_range = 200\n",
    "shift_range = torch.arange(2 * full_range + 1) - full_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "76dc5987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T18:59:15.732831Z",
     "start_time": "2023-04-26T18:59:15.575905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|                                                                                                                                | 0/10000 [00:00<?, ?it/s, lowest_mse=100, pred_loss=100, trigger=0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "shifted_ldos_discrete() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [217]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights, loss_dos, test_loss_dos \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_reg_train_Ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_soap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_aligned_dos3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mtotal_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_test_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam Unbiased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train error is \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m for SOAP\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss_dos))\n",
      "Input \u001b[0;32mIn [215]\u001b[0m, in \u001b[0;36mnormal_reg_train_Ad\u001b[0;34m(feat, target, train_index, test_index, regularization, n_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    142\u001b[0m         loss_i\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss_i\n\u001b[0;32m--> 144\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    147\u001b[0m     preds \u001b[38;5;241m=\u001b[39m Features \u001b[38;5;241m@\u001b[39m weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 183\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    186\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [215]\u001b[0m, in \u001b[0;36mnormal_reg_train_Ad.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m pred_i \u001b[38;5;241m=\u001b[39m reg_features_i \u001b[38;5;241m@\u001b[39m weights\n\u001b[1;32m    139\u001b[0m opt_shift \u001b[38;5;241m=\u001b[39m find_optimal_discrete_shift(np\u001b[38;5;241m.\u001b[39marray(pred_i[:\u001b[38;5;28mlen\u001b[39m(i_batch)]\u001b[38;5;241m.\u001b[39mdetach()),np\u001b[38;5;241m.\u001b[39marray(target_i[:\u001b[38;5;28mlen\u001b[39m(i_batch)]\u001b[38;5;241m.\u001b[39mdetach()))\n\u001b[0;32m--> 140\u001b[0m pred_i[:\u001b[38;5;28mlen\u001b[39m(i_batch)] \u001b[38;5;241m=\u001b[39m \u001b[43mshifted_ldos_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_shift\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m loss_i \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mt_get_mse(pred_i, target_i)\n\u001b[1;32m    142\u001b[0m loss_i\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mTypeError\u001b[0m: shifted_ldos_discrete() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "weights, loss_dos, test_loss_dos = normal_reg_train_Ad(total_soap, total_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 10000, 16, 1e-3)\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2934bc68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:07:51.900406Z",
     "start_time": "2023-04-27T09:07:48.318436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|                                                                                                                                    | 0/6 [00:00<?, ?it/s, lowest_mse=100, pred_loss=100, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(125.4775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(125.1251, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0509, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(121.9773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0495, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(92.8213, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0369, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.2189, grad_fn=<MeanBackward0>)\n",
      "tensor(8.8324e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.2186, grad_fn=<MeanBackward0>)\n",
      "tensor(8.8190e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.2158, grad_fn=<MeanBackward0>)\n",
      "tensor(8.6987e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.1891, grad_fn=<MeanBackward0>)\n",
      "tensor(7.5529e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.0513, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8201e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.0517, grad_fn=<MeanBackward0>)\n",
      "tensor(1.5635e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.0507, grad_fn=<MeanBackward0>)\n",
      "tensor(1.1940e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.0469, grad_fn=<MeanBackward0>)\n",
      "tensor(1.1425e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n",
      "torch.Size([123])\n",
      "tensor(6.0441, grad_fn=<MeanBackward0>)\n",
      "tensor(1.1339e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   0%|                                                                                                                                    | 0/6 [00:03<?, ?it/s, lowest_mse=100, pred_loss=100, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([123])\n",
      "tensor(6.0437, grad_fn=<MeanBackward0>)\n",
      "tensor(1.1336e-05, grad_fn=<MeanBackward1>)\n",
      "--------HEY------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [272]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m U_L_weights3 , loss_dos, test_loss_dos \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_reg_train_L\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_soap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_dos3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_train_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_test_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBFGS Unbiased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train error is \u001b[39m\u001b[38;5;132;01m{:.4}\u001b[39;00m\u001b[38;5;124m for SOAP\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss_dos))\n",
      "Input \u001b[0;32mIn [271]\u001b[0m, in \u001b[0;36mnormal_reg_train_L\u001b[0;34m(cutoff_index, shift_range, feat, target, train_index, test_index, regularization, n_epochs, lr)\u001b[0m\n\u001b[1;32m     45\u001b[0m     loss_i\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_i\n\u001b[0;32m---> 47\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     50\u001b[0m     preds \u001b[38;5;241m=\u001b[39m Features \u001b[38;5;241m@\u001b[39m weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    429\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:50\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    279\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [271]\u001b[0m, in \u001b[0;36mnormal_reg_train_L.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m pred_i \u001b[38;5;241m=\u001b[39m reg_features \u001b[38;5;241m@\u001b[39m weights\n\u001b[0;32m---> 35\u001b[0m loss_i, jitter \u001b[38;5;241m=\u001b[39m \u001b[43mt_get_BF_shift_index_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m (loss_i\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmean(loss_i))\n",
      "Input \u001b[0;32mIn [264]\u001b[0m, in \u001b[0;36mt_get_BF_shift_index_mse\u001b[0;34m(prediction, true, shift_range, cutoff_index)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mt_get_BF_shift_index_mse\u001b[39m(prediction, true, shift_range, cutoff_index):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#shifts target instead of prediction\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         shifted_true \u001b[38;5;241m=\u001b[39m \u001b[43mshifted_ldos_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_range\u001b[49m\u001b[43m)\u001b[49m[:,:,:cutoff_index]\n\u001b[1;32m      5\u001b[0m         full_loss \u001b[38;5;241m=\u001b[39m t_get_each_mse(prediction\u001b[38;5;241m.\u001b[39mrepeat(shift_range\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), shifted_true)\n\u001b[1;32m      6\u001b[0m         full_loss \u001b[38;5;241m=\u001b[39m full_loss\u001b[38;5;241m.\u001b[39mreshape(shift_range\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "U_L_weights3 , loss_dos, test_loss_dos = normal_reg_train_L(cutoff_index, shift_range, surface_soap, surface_dos3, surface_train_index, surface_test_index,\n",
    "                                                            1e-2, 6, 1)\n",
    "print (\"LBFGS Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a4961b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T16:14:49.848726Z",
     "start_time": "2023-04-26T16:14:49.841240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f74e219bb20>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813544fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
