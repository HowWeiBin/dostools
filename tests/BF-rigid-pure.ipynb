{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db828e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T13:05:52.911575Z",
     "start_time": "2023-04-26T13:05:52.903509Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import time\n",
    "import scipy \n",
    "import ase\n",
    "import ase.io\n",
    "torch.set_default_dtype(torch.float64) \n",
    "# %matplotlib notebook\n",
    "# matplotlib.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dd7bb87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:12:11.572873Z",
     "start_time": "2023-04-26T14:12:11.494119Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.datasets.data as data\n",
    "import dostools.utils.utils as utils\n",
    "\n",
    "with torch.no_grad():\n",
    "#     sigma = 0.3\n",
    "#     structures = data.load_structures(\":\")\n",
    "#     n_structures = len(structures) #total number of structures\n",
    "#     for structure in structures:#implement periodicity\n",
    "#         structure.wrap(eps = 1e-12) \n",
    "#     n_atoms = np.zeros(n_structures, dtype = int) #stores number of atoms in each structures\n",
    "#     for i in range(n_structures):\n",
    "#         n_atoms[i] = len(structures[i])\n",
    "        \n",
    "    xdos = torch.tensor(data.load_xdos())\n",
    "    \n",
    "    total_dos3 = torch.load(\"./total_ldos3.pt\")\n",
    "    total_dos1 = torch.load(\"./total_ldos1.pt\")\n",
    "    \n",
    "    surface_dos3 = torch.load(\"./surface_ldos3.pt\")\n",
    "    surface_dos1 = torch.load(\"./surface_ldos1.pt\")\n",
    "    \n",
    "    surface_aligned_dos3 = torch.load(\"./surface_aligned_dos3.pt\")\n",
    "    surface_aligned_dos1 = torch.load(\"./surface_aligned_dos1.pt\")\n",
    "    \n",
    "    bulk_dos3 = torch.load(\"./bulk_ldos3.pt\")\n",
    "    bulk_dos1 = torch.load(\"./bulk_ldos1.pt\")\n",
    "    \n",
    "    total_aligned_dos3 = torch.load(\"./total_aligned_dos3.pt\")\n",
    "    total_aligned_dos1 = torch.load(\"./total_aligned_dos1.pt\")\n",
    "    \n",
    "    surface_soap = torch.load(\"./surface_soap.pt\")\n",
    "    bulk_soap = torch.load(\"./bulk_soap.pt\")\n",
    "    total_soap = torch.load(\"./total_soap.pt\")\n",
    "    \n",
    "    surface_kernel_30 = torch.load(\"./surface_kernel_30.pt\")\n",
    "    surface_kMM_30 = torch.load(\"./surface_kMM_30.pt\")\n",
    "    \n",
    "    bulk_kernel_200 = torch.load(\"./bulk_kernel_200.pt\")\n",
    "    bulk_kMM_200 = torch.load(\"./bulk_kMM_200.pt\")\n",
    "    \n",
    "    bulk_kernel_100 = torch.load(\"./bulk_kernel_100.pt\")\n",
    "    bulk_kMM_100 = torch.load(\"./bulk_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_100 = torch.load(\"./total_kernel_100.pt\")\n",
    "    total_kMM_100 = torch.load(\"./total_kMM_100.pt\")\n",
    "    \n",
    "    total_kernel_150 = torch.load(\"./total_kernel_150.pt\")\n",
    "    total_kMM_150 = torch.load(\"./total_kMM_150.pt\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3b26e208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:09:50.489342Z",
     "start_time": "2023-04-26T14:09:50.423964Z"
    }
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "\n",
    "#     cutoff_index = torch.tensor(487)\n",
    "#     xdos = xdos[:cutoff_index]\n",
    "\n",
    "#     total_dos3 = torch.load(\"./total_ldos3.pt\")[:,:cutoff_index]\n",
    "#     total_dos1 = torch.load(\"./total_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     surface_dos3 = torch.load(\"./surface_ldos3.pt\")[:,:cutoff_index]\n",
    "#     surface_dos1 = torch.load(\"./surface_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     surface_aligned_dos3 = torch.load(\"./surface_aligned_dos3.pt\")[:,:cutoff_index]\n",
    "#     surface_aligned_dos1 = torch.load(\"./surface_aligned_dos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     bulk_dos3 = torch.load(\"./bulk_ldos3.pt\")[:,:cutoff_index]\n",
    "#     bulk_dos1 = torch.load(\"./bulk_ldos1.pt\")[:,:cutoff_index]\n",
    "\n",
    "#     total_aligned_dos3 = torch.load(\"./total_aligned_dos3.pt\")[:,:cutoff_index]\n",
    "#     total_aligned_dos1 = torch.load(\"./total_aligned_dos1.pt\")[:,:cutoff_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "15107a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T14:10:41.923619Z",
     "start_time": "2023-04-26T14:10:41.902919Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train_test_split(n_samples):\n",
    "    n_structures = n_samples\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_structures)\n",
    "    train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(train_index)\n",
    "    test_index = train_index[n_train:]\n",
    "    train_index = train_index[:n_train]\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "def generate_biased_train_test_split(n_samples):\n",
    "    #Assumes 100 amorphous structures at the end\n",
    "    n_structures = n_samples\n",
    "    amorph_train = np.arange(n_samples-100, n_samples,1)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(amorph_train)\n",
    "    \n",
    "    amorph_test = amorph_train[:80]\n",
    "    amorph_train = amorph_train[80:]\n",
    "\n",
    "    n_structures = n_samples - 100\n",
    "    np.random.seed(0)\n",
    "    n_train = int(0.8 * n_samples)-20\n",
    "    remaining_train_index = np.arange(n_structures)\n",
    "    np.random.shuffle(remaining_train_index)\n",
    "\n",
    "    remaining_test_index = remaining_train_index[n_train:]\n",
    "    remaining_train_index = remaining_train_index[:n_train]\n",
    "\n",
    "    biased_train_index = np.concatenate([remaining_train_index, amorph_train])\n",
    "    biased_test_index = np.concatenate([remaining_test_index, amorph_test])\n",
    "    \n",
    "    return biased_train_index, biased_test_index\n",
    "\n",
    "def generate_surface_holdout_split(n_samples):\n",
    "    #Assumes that we are using the 110 surfaces for test which are located at 673 + 31st-57th index\n",
    "    #26 structures\n",
    "    \n",
    "    n_test = int(0.2 * n_samples) - 26\n",
    "    n_train = n_samples - n_test\n",
    "    \n",
    "    remaining_indexes = np.concatenate([np.arange(673+31), np.arange(673+57,n_samples,1)])\n",
    "    indexes_110 = np.arange(673+31, 673+57,1)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    np.random.shuffle(remaining_indexes)\n",
    "    \n",
    "    remaining_test_index = remaining_indexes[n_train:]\n",
    "    remaining_train_index = remaining_indexes[:n_train]\n",
    "    \n",
    "    total_train_index = remaining_train_index\n",
    "    total_test_index = np.concatenate([remaining_test_index, indexes_110])\n",
    "    \n",
    "    return total_train_index, total_test_index\n",
    "    \n",
    "def surface_holdout(n_samples):\n",
    "    test_index = np.arange(31,57,1)\n",
    "    train_index = np.concatenate([np.arange(31), np.arange(57, n_samples)])\n",
    "    \n",
    "    return train_index, test_index\n",
    "\n",
    "n_surfaces = 154\n",
    "n_bulkstructures = 773\n",
    "n_total_structures = 773 + 154\n",
    "\n",
    "\n",
    "surface_train_index, surface_test_index = generate_train_test_split(n_surfaces)\n",
    "bulk_train_index, bulk_test_index = generate_train_test_split(n_bulkstructures)\n",
    "total_train_index, total_test_index = generate_train_test_split(n_total_structures)\n",
    "surface_holdout_train_index, surface_holdout_test_index = surface_holdout(n_surfaces)\n",
    "bulk_biased_train_index, bulk_biased_test_index = generate_biased_train_test_split(n_bulkstructures)\n",
    "total_biased_train_index, total_biased_test_index = generate_biased_train_test_split(n_total_structures)\n",
    "holdout_train_index, holdout_test_index = generate_surface_holdout_split(n_total_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "11044659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T19:03:36.756599Z",
     "start_time": "2023-04-26T19:03:36.749856Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat = torch.rand(1000).reshape(20, 5, 10)\n",
    "# dog = torch.rand(1000).reshape(20, 5, 10)\n",
    "mouse = torch.sum((cat - dog) **2, axis = 2)\n",
    "mouse =  (mouse.reshape(20,5))\n",
    "\n",
    "a, b = torch.min(mouse, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508a236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "77d384b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:44:24.911656Z",
     "start_time": "2023-04-27T09:44:24.893880Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_each_mse(predictions, true, xdos = None):\n",
    "    #takes a 3d array for predictions and true\n",
    "    if xdos is not None:\n",
    "        mse = torch.trapezoid((predictions - true)**2, xdos, axis = 2)\n",
    "    else:\n",
    "        mse = torch.mean((predictions - true)**2 , axis = 2)\n",
    "        \n",
    "    return mse\n",
    "\n",
    "def t_get_rmse(a, b, xdos=None, perc=False): #account for the fact that DOS is continuous but we are training them pointwise\n",
    "    \"\"\" computes  Root Mean Squared Error (RMSE) of array properties (DOS/aofd).\n",
    "         a=pred, b=target, xdos, perc: if False return RMSE else return %RMSE\"\"\"\n",
    "    #MIGHT NOT WORK FOR PC\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=1)).mean())\n",
    "        else:\n",
    "            rmse = torch.sqrt((torch.trapezoid((a - b)**2, xdos, axis=0)).mean())\n",
    "        if not perc:\n",
    "            return rmse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.sqrt((torch.trapezoid((b - mean)**2, xdos, axis=1)).mean())\n",
    "            return (100 * rmse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean(dim =0))\n",
    "        else:\n",
    "            rmse = torch.sqrt(((a - b)**2).mean())\n",
    "        if not perc:\n",
    "            return torch.mean(rmse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (rmse / b.std(dim = 0,unbiased=True)), 0)\n",
    "        \n",
    "def t_get_mse(a, b, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=1)).mean()\n",
    "        else:\n",
    "            mse = (torch.trapezoid((a - b)**2, xdos, axis=0)).mean()\n",
    "        if not perc:\n",
    "            return mse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.trapezoid((b - mean)**2, xdos, axis=1).mean()\n",
    "            return (100 * mse / std)\n",
    "    else:\n",
    "        if len(a.size()) > 1:\n",
    "            mse = ((a - b)**2).mean(dim = 1)\n",
    "        else:\n",
    "            mse = ((a - b)**2).mean()\n",
    "        if len(mse.shape) > 1:\n",
    "            raise ValueError('Loss became 2D')\n",
    "        if not perc:\n",
    "            return torch.mean(mse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (mse / b.std(dim=0, unbiased = True)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "de384dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:49:14.306591Z",
     "start_time": "2023-04-27T09:49:12.934297Z"
    }
   },
   "outputs": [],
   "source": [
    "alignment = torch.rand(927)\n",
    "trueshift = torch.round(alignment/(xdos[1] - xdos[0])).int()\n",
    "test_ldos0 = shifted_ldos_discrete(total_dos3, torch.zeros(931)-15)\n",
    "shift_range = torch.arange(2 * full_range + 1) - full_range\n",
    "l, s = t_get_BF_shift_index_mse(test_ldos0[:,:cutoff_index], total_dos1, shift_range, cutoff_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "535832f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:49:28.794498Z",
     "start_time": "2023-04-27T09:49:28.780292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15, -15,\n",
       "        -15, -15, -15])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s + shift_range[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7b8170aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:49:19.721004Z",
     "start_time": "2023-04-27T09:49:19.704118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.2083e-04, 7.5757e-04, 7.2594e-04, 7.8010e-04, 8.8274e-04, 1.1556e-03,\n",
       "        1.0576e-03, 1.0472e-03, 1.0117e-03, 6.4176e-04, 9.5548e-04, 9.5521e-04,\n",
       "        5.2911e-04, 8.7031e-04, 1.0467e-03, 1.0460e-03, 6.2723e-04, 9.1996e-04,\n",
       "        9.2358e-04, 6.2153e-04, 9.0146e-04, 8.3584e-04, 8.0291e-04, 8.3456e-04,\n",
       "        1.3616e-03, 9.2762e-04, 1.0747e-03, 9.6573e-04, 9.5462e-04, 7.4888e-04,\n",
       "        6.9765e-04, 9.2473e-04, 8.7710e-04, 7.5673e-04, 7.4101e-04, 6.3273e-04,\n",
       "        7.0046e-04, 6.9757e-04, 5.9642e-04, 7.4645e-04, 7.3755e-04, 7.8690e-04,\n",
       "        7.6836e-04, 1.0769e-03, 1.1486e-03, 6.8294e-04, 4.7939e-04, 5.2257e-04,\n",
       "        8.9669e-04, 1.1813e-03, 9.9098e-04, 9.3089e-04, 8.7537e-04, 6.1713e-04,\n",
       "        7.0604e-04, 9.0446e-04, 8.0403e-04, 6.2777e-04, 7.2641e-04, 5.6946e-04,\n",
       "        1.0171e-03, 6.0419e-04, 5.7601e-04, 5.0010e-04, 4.9846e-04, 5.5036e-04,\n",
       "        9.1764e-04, 7.2040e-04, 5.1013e-04, 6.3412e-04, 7.6596e-04, 8.7071e-04,\n",
       "        8.6423e-04, 5.0732e-04, 9.3173e-04, 7.6583e-04, 1.0458e-03, 1.1250e-03,\n",
       "        6.7376e-04, 5.3579e-04, 1.3162e-03, 6.7504e-04, 7.8685e-04, 9.7850e-04,\n",
       "        1.2148e-03, 7.1261e-04, 4.9145e-04, 5.5156e-04, 5.4744e-04, 5.3152e-04,\n",
       "        7.8573e-04, 7.2583e-04, 8.5648e-04, 4.4060e-04, 4.3175e-04, 9.8821e-04,\n",
       "        7.8691e-04, 3.6783e-04, 5.5601e-04, 3.4464e-04, 5.5407e-04, 5.0485e-04,\n",
       "        1.4311e-03, 6.7011e-04, 2.2098e-03, 2.3339e-03, 2.4660e-03, 2.3329e-03,\n",
       "        2.2007e-03, 2.4052e-03, 2.3317e-03, 2.2233e-03, 2.1462e-03, 2.1238e-03,\n",
       "        2.1768e-03, 2.3642e-03, 1.8969e-03, 2.0970e-03, 1.9552e-03, 2.2113e-03,\n",
       "        1.9436e-03, 2.1071e-03, 1.9608e-03, 1.9514e-03, 2.4631e-03, 2.8766e-03,\n",
       "        2.4232e-03, 2.3876e-03, 2.6358e-03, 2.3619e-03, 2.3307e-03, 2.5193e-03,\n",
       "        2.3528e-03, 2.3587e-03, 2.3287e-03, 2.4795e-03, 2.5124e-03, 2.4282e-03,\n",
       "        2.4348e-03, 2.3376e-03, 2.3651e-03, 2.3986e-03, 2.3177e-03, 2.2387e-03,\n",
       "        2.9299e-03, 2.3784e-03, 2.6116e-03, 2.2904e-03, 2.5559e-03, 2.0865e-03,\n",
       "        2.5978e-03, 2.0556e-03, 2.2265e-03, 2.3615e-03, 2.4696e-03, 2.5861e-03,\n",
       "        2.5408e-03, 2.6472e-03, 2.5093e-03, 2.5730e-03, 2.3717e-03, 2.4841e-03,\n",
       "        2.3546e-03, 2.4404e-03, 2.7230e-03, 2.5713e-03, 2.5515e-03, 2.4562e-03,\n",
       "        2.4158e-03, 2.4746e-03, 2.1004e-03, 2.4780e-03, 2.1465e-03, 2.5252e-03,\n",
       "        2.1737e-03, 2.2850e-03, 2.4562e-03, 2.2768e-03, 2.4453e-03, 2.3195e-03,\n",
       "        2.6104e-03, 2.3129e-03, 2.5083e-03, 2.1187e-03, 2.6945e-03, 2.5389e-03,\n",
       "        2.4251e-03, 2.2790e-03, 2.3280e-03, 2.3426e-03, 2.2624e-03, 2.3439e-03,\n",
       "        2.2061e-03, 2.3952e-03, 2.1938e-03, 2.2031e-03, 2.1454e-03, 2.1368e-03,\n",
       "        1.9442e-03, 2.0103e-03, 1.8702e-03, 2.0559e-03, 2.0843e-03, 1.9964e-03,\n",
       "        2.1713e-03, 2.3763e-03, 2.2987e-03, 2.0707e-03, 2.1437e-03, 1.9990e-03,\n",
       "        1.8426e-03, 2.0765e-03, 1.8187e-03, 2.0937e-03, 2.0914e-03, 2.1079e-03,\n",
       "        2.0687e-03, 2.0163e-03, 2.0616e-03, 1.9067e-03, 1.9746e-03, 2.0656e-03,\n",
       "        1.8745e-03, 1.9547e-03, 2.5786e-03, 2.1534e-03, 2.2414e-03, 1.9803e-03,\n",
       "        2.1332e-03, 1.9924e-03, 2.0538e-03, 1.9573e-03, 2.0276e-03, 1.9113e-03,\n",
       "        1.8766e-03, 1.8036e-03, 1.9002e-03, 1.6697e-03, 1.9232e-03, 1.8193e-03,\n",
       "        2.0342e-03, 2.0259e-03, 2.1439e-03, 2.0176e-03, 2.2567e-03, 2.4870e-03,\n",
       "        2.4234e-03, 2.1181e-03, 2.3622e-03, 2.0370e-03, 2.0825e-03, 1.8848e-03,\n",
       "        1.9364e-03, 1.9431e-03, 1.8360e-03, 1.8862e-03, 1.7762e-03, 1.7577e-03,\n",
       "        1.7968e-03, 1.6295e-03, 1.8831e-03, 1.7306e-03, 1.8768e-03, 1.9464e-03,\n",
       "        2.5799e-03, 2.4297e-03, 2.4874e-03, 2.2219e-03, 2.3181e-03, 2.2052e-03,\n",
       "        2.2151e-03, 2.0959e-03, 2.3044e-03, 2.1297e-03, 1.9278e-03, 2.0034e-03,\n",
       "        1.8104e-03, 1.9367e-03, 1.7720e-03, 1.8391e-03, 1.8924e-03, 1.9090e-03,\n",
       "        2.0320e-03, 1.8570e-03, 2.5603e-03, 2.5699e-03, 2.2578e-03, 2.1910e-03,\n",
       "        2.0964e-03, 1.8959e-03, 1.7133e-03, 1.6396e-03, 1.6667e-03, 1.6588e-03,\n",
       "        1.7765e-03, 1.8600e-03, 1.9565e-03, 1.9211e-03, 1.9186e-03, 1.9226e-03,\n",
       "        1.7197e-03, 1.7981e-03, 1.7625e-03, 1.6050e-03, 2.5573e-03, 2.6035e-03,\n",
       "        2.1069e-03, 2.0586e-03, 1.9274e-03, 1.8439e-03, 1.6105e-03, 1.7053e-03,\n",
       "        1.7172e-03, 1.6285e-03, 1.6263e-03, 1.6347e-03, 1.5088e-03, 1.5468e-03,\n",
       "        1.3998e-03, 1.5594e-03, 1.4316e-03, 1.5886e-03, 1.5187e-03, 1.4733e-03,\n",
       "        1.8260e-04, 1.8999e-04, 2.3663e-04, 1.7314e-04, 2.0692e-04, 1.9746e-04,\n",
       "        1.6053e-04, 2.9133e-04, 2.2384e-04, 2.3771e-04, 2.2070e-04, 1.9080e-04,\n",
       "        1.7052e-04, 2.1118e-04, 1.8926e-04, 2.3595e-04, 1.5074e-04, 2.0512e-04,\n",
       "        1.5341e-04, 1.8833e-04, 2.2801e-04, 1.8816e-04, 1.5434e-04, 1.1639e-04,\n",
       "        2.0798e-04, 1.5960e-04, 1.6279e-04, 2.5530e-04, 1.3878e-04, 1.3613e-04,\n",
       "        1.5971e-04, 2.2799e-04, 1.9714e-04, 1.9922e-04, 1.5629e-04, 2.9168e-04,\n",
       "        1.3133e-04, 2.0104e-04, 2.0703e-04, 1.6139e-04, 1.6023e-04, 1.8483e-04,\n",
       "        1.8374e-04, 1.8871e-04, 1.8395e-04, 1.9448e-04, 1.5311e-04, 1.7665e-04,\n",
       "        1.0951e-04, 1.7596e-04, 1.8648e-04, 1.8239e-04, 2.2346e-04, 2.1026e-04,\n",
       "        2.1867e-04, 2.3135e-04, 1.9864e-04, 2.8010e-04, 2.3058e-04, 1.6159e-04,\n",
       "        4.8258e-04, 4.5941e-04, 4.5452e-04, 4.5420e-04, 4.6931e-04, 4.8460e-04,\n",
       "        4.7854e-04, 4.4329e-04, 4.1032e-04, 4.0711e-04, 4.1339e-04, 4.1636e-04,\n",
       "        4.2496e-04, 4.5044e-04, 5.0492e-04, 4.6722e-04, 4.6895e-04, 4.4826e-04,\n",
       "        4.2085e-04, 4.2884e-04, 4.1516e-04, 4.0869e-04, 4.1486e-04, 4.2390e-04,\n",
       "        4.2072e-04, 4.2749e-04, 4.3453e-04, 4.5172e-04, 4.5482e-04, 4.4231e-04,\n",
       "        4.3740e-04, 4.2126e-04, 3.9617e-04, 4.0863e-04, 4.0092e-04, 4.1505e-04,\n",
       "        4.0269e-04, 3.8974e-04, 3.9794e-04, 3.8791e-04, 3.8935e-04, 4.0664e-04,\n",
       "        4.0102e-04, 4.0599e-04, 4.0302e-04, 3.7504e-04, 3.7006e-04, 4.0504e-04,\n",
       "        3.9190e-04, 3.8756e-04, 4.0495e-04, 3.9884e-04, 4.1781e-04, 4.0866e-04,\n",
       "        4.1000e-04, 4.0243e-04, 3.4485e-04, 3.7556e-04, 3.6336e-04, 3.1140e-04,\n",
       "        3.5651e-04, 4.1623e-04, 5.4216e-04, 4.9187e-04, 4.8313e-04, 4.8258e-04,\n",
       "        4.6161e-04, 4.2756e-04, 4.1450e-04, 4.1712e-04, 4.1508e-04, 4.0947e-04,\n",
       "        4.1506e-04, 4.2559e-04, 4.3436e-04, 4.4776e-04, 4.7231e-04, 4.9900e-04,\n",
       "        5.0732e-04, 4.9729e-04, 4.7747e-04, 4.7369e-04, 5.4477e-04, 5.0308e-04,\n",
       "        4.9978e-04, 4.9451e-04, 4.7862e-04, 4.9238e-04, 5.0848e-04, 4.8918e-04,\n",
       "        4.5492e-04, 4.5417e-04, 4.6755e-04, 4.6701e-04, 4.5614e-04, 4.4588e-04,\n",
       "        4.3766e-04, 4.5315e-04, 4.7680e-04, 4.8744e-04, 4.8064e-04, 4.4986e-04,\n",
       "        5.3776e-04, 4.8455e-04, 4.7276e-04, 4.7912e-04, 4.7011e-04, 4.5338e-04,\n",
       "        4.5193e-04, 4.6937e-04, 4.5795e-04, 4.5383e-04, 4.6509e-04, 4.6654e-04,\n",
       "        4.5451e-04, 4.5272e-04, 4.6310e-04, 4.4766e-04, 4.3610e-04, 4.3754e-04,\n",
       "        4.4167e-04, 4.4062e-04, 4.5980e-04, 4.7135e-04, 4.4561e-04, 4.1546e-04,\n",
       "        4.3514e-04, 4.5555e-04, 4.0462e-04, 3.8907e-04, 4.2959e-04, 3.7324e-04,\n",
       "        3.7411e-04, 3.8769e-04, 3.6956e-04, 3.6996e-04, 3.8831e-04, 3.9488e-04,\n",
       "        3.8935e-04, 3.7804e-04, 3.5518e-04, 3.6563e-04, 3.5839e-04, 3.5538e-04,\n",
       "        3.5480e-04, 4.1590e-04, 3.8850e-04, 3.6240e-04, 3.3056e-04, 3.4119e-04,\n",
       "        3.4173e-04, 3.5103e-04, 3.5837e-04, 3.9094e-04, 3.5916e-04, 3.7158e-04,\n",
       "        3.7407e-04, 3.6613e-04, 3.6849e-04, 3.4547e-04, 3.4422e-04, 3.5196e-04,\n",
       "        3.5873e-04, 4.0726e-04, 4.3242e-04, 3.9830e-04, 4.1100e-04, 3.8084e-04,\n",
       "        3.6647e-04, 3.7717e-04, 3.3733e-04, 3.3877e-04, 3.3520e-04, 3.4642e-04,\n",
       "        3.5618e-04, 3.5497e-04, 3.5821e-04, 3.7960e-04, 3.7094e-04, 3.5360e-04,\n",
       "        3.7400e-04, 3.6804e-04, 3.4067e-04, 4.2017e-04, 4.3405e-04, 4.0722e-04,\n",
       "        3.7858e-04, 3.7949e-04, 3.5969e-04, 3.4620e-04, 3.3094e-04, 3.4026e-04,\n",
       "        3.6082e-04, 3.9302e-04, 4.2099e-04, 3.9907e-04, 3.5104e-04, 3.5355e-04,\n",
       "        3.5889e-04, 3.5733e-04, 3.5496e-04, 4.3432e-04, 3.9838e-04, 3.8959e-04,\n",
       "        3.7133e-04, 3.5158e-04, 3.8441e-04, 3.8965e-04, 3.9472e-04, 3.8577e-04,\n",
       "        4.0194e-04, 4.2321e-04, 4.3445e-04, 4.3745e-04, 4.2166e-04, 4.2509e-04,\n",
       "        3.9749e-04, 4.0904e-04, 4.1314e-04, 4.2304e-04, 2.1336e-04, 2.2455e-04,\n",
       "        1.4332e-04, 1.6262e-04, 1.5668e-04, 1.7833e-04, 1.8169e-04, 1.2446e-04,\n",
       "        1.6859e-04, 1.4879e-04, 1.7490e-04, 1.7194e-04, 1.1521e-04, 1.5093e-04,\n",
       "        2.4404e-04, 2.5192e-04, 1.2143e-04, 1.4340e-04, 1.4847e-04, 1.5744e-04,\n",
       "        1.2250e-04, 1.5947e-04, 1.2437e-04, 1.3510e-04, 1.3724e-04, 1.2305e-04,\n",
       "        1.1716e-04, 1.4651e-04, 1.9090e-04, 2.1331e-04, 1.7159e-04, 2.8886e-04,\n",
       "        2.5012e-04, 2.0273e-04, 2.3816e-04, 1.8497e-04, 2.1347e-04, 2.2282e-04,\n",
       "        1.4322e-04, 1.6275e-04, 1.8106e-04, 1.3605e-04, 1.4309e-04, 1.1998e-04,\n",
       "        1.4365e-04, 1.5030e-04, 9.4377e-05, 1.6236e-04, 3.0465e-04, 2.6338e-04,\n",
       "        2.1530e-04, 1.9907e-04, 3.1994e-04, 1.6860e-04, 1.9600e-04, 1.3386e-04,\n",
       "        1.6588e-04, 1.9911e-04, 1.7907e-04, 1.5038e-04, 1.9400e-04, 1.3737e-04,\n",
       "        1.7177e-04, 1.0773e-04, 1.0582e-04, 1.5262e-04, 1.3816e-04, 2.5315e-04,\n",
       "        1.4743e-04, 1.5580e-03, 1.0462e-03, 2.6122e-03, 1.1565e-03, 3.8154e-04,\n",
       "        2.8412e-04, 3.0569e-04, 3.1521e-04, 3.0181e-04, 3.3471e-04, 5.1742e-04,\n",
       "        4.2729e-04, 4.6267e-04, 4.0146e-04, 4.5219e-04, 4.9564e-04, 4.7028e-04,\n",
       "        5.8351e-04, 4.7275e-04, 4.9174e-04, 4.9875e-04, 1.4086e-04, 1.4969e-03,\n",
       "        2.4022e-04, 2.0536e-04, 2.6700e-04, 2.6700e-04, 1.8760e-04, 1.0554e-03,\n",
       "        9.3487e-04, 1.7448e-04, 1.8031e-03, 9.4978e-04, 4.5294e-04, 3.9466e-04,\n",
       "        3.9736e-04, 5.0526e-04, 3.3065e-04, 4.1904e-04, 4.3015e-04, 4.9587e-04,\n",
       "        3.6889e-04, 3.7794e-04, 4.2655e-04, 4.6912e-04, 1.5024e-04, 1.7030e-04,\n",
       "        2.0091e-04, 2.9207e-04, 1.5630e-04, 1.9427e-04, 2.1320e-04, 1.7754e-04,\n",
       "        1.7890e-04, 1.8181e-04, 1.7711e-04, 1.7304e-04, 6.5982e-04, 3.8529e-04,\n",
       "        3.7637e-04, 3.9881e-04, 4.4050e-04, 5.1751e-04, 3.8134e-04, 3.9152e-04,\n",
       "        4.9632e-04, 3.9424e-04, 5.6006e-04, 3.5685e-04, 4.5832e-04, 4.0443e-04,\n",
       "        4.2480e-04, 3.9346e-04, 4.9505e-04, 6.4426e-04, 4.5494e-04, 5.6722e-04,\n",
       "        4.7983e-04, 5.3058e-04, 3.8085e-04, 4.0687e-04, 4.5333e-04, 5.2159e-04,\n",
       "        5.2365e-04, 4.3220e-04, 1.4790e-03, 1.3547e-03, 1.1837e-03, 1.2074e-03,\n",
       "        1.1317e-03, 1.0630e-03, 9.7610e-04, 8.0167e-04, 7.5949e-04, 8.0917e-04,\n",
       "        8.1210e-04, 7.9605e-04, 8.1469e-04, 9.9159e-04, 9.4168e-04, 9.6637e-04,\n",
       "        1.0466e-03, 1.1969e-03, 1.2560e-03, 1.5907e-03, 8.9411e-04, 5.0114e-04,\n",
       "        5.4171e-04, 4.4174e-04, 3.8158e-04, 4.4673e-04, 4.4405e-04, 2.7709e-04,\n",
       "        3.3159e-04, 3.5026e-04, 3.8297e-04, 3.4363e-04, 5.2567e-04, 3.3783e-04,\n",
       "        5.2557e-04, 4.4374e-04, 3.4722e-04, 4.3416e-04, 6.5052e-04, 4.6422e-04,\n",
       "        4.0205e-04, 4.0843e-04, 5.8437e-04, 4.5955e-04, 5.6224e-04, 4.5284e-04,\n",
       "        3.7213e-04, 4.0811e-04, 3.3215e-04, 4.3871e-04, 3.7136e-04, 4.3947e-04,\n",
       "        5.0648e-04, 3.3456e-04, 6.0438e-04, 3.8952e-04, 3.8710e-04, 6.4315e-04,\n",
       "        3.9680e-04, 4.8523e-04, 3.3644e-04, 3.4957e-04, 4.5261e-04, 4.1260e-04,\n",
       "        4.6617e-04, 4.5516e-04, 3.1092e-04, 4.0800e-04, 3.6524e-04, 4.2689e-04,\n",
       "        4.3722e-04, 4.5686e-04, 4.2272e-04, 5.7148e-04, 3.7504e-04, 3.9650e-04,\n",
       "        4.2941e-04, 4.9048e-04, 3.5744e-04, 4.0090e-04, 4.2617e-04, 3.8414e-04,\n",
       "        3.3062e-04, 4.0777e-04, 3.8443e-04, 4.6136e-04, 3.4593e-04, 6.7835e-04,\n",
       "        5.5437e-04, 3.9411e-04, 4.0008e-04, 3.1593e-04, 4.6253e-04, 4.3006e-04,\n",
       "        5.2197e-04, 3.6958e-04, 4.1572e-04, 3.7094e-04, 4.3143e-04, 4.5490e-04,\n",
       "        3.4993e-04, 3.8039e-04, 3.5302e-04, 4.8709e-04, 4.4578e-04, 3.3042e-04,\n",
       "        4.9659e-04, 3.6062e-04, 4.1009e-04, 4.1395e-04, 4.2873e-04, 3.8727e-04,\n",
       "        4.4674e-04, 4.1294e-04, 5.0555e-04, 4.7851e-04, 4.7648e-04, 3.9065e-04,\n",
       "        4.2841e-04, 3.3709e-04, 4.9739e-04, 5.5546e-04, 3.9363e-04, 4.1652e-04,\n",
       "        4.0042e-04, 3.0053e-04, 3.0136e-04, 3.7001e-04, 3.8784e-04, 3.3921e-04,\n",
       "        4.5490e-04, 4.3149e-04, 4.3109e-04, 4.2047e-04, 3.6887e-04, 4.6838e-04,\n",
       "        4.1418e-04, 4.2948e-04, 3.9991e-04, 4.1417e-04, 3.5074e-04, 4.6970e-04,\n",
       "        4.2025e-04, 3.8392e-04, 4.7928e-04, 4.6817e-04, 3.3865e-04, 4.6541e-04,\n",
       "        4.4071e-04, 3.4316e-04, 5.1812e-04, 4.4521e-04, 4.3277e-04, 5.4535e-04,\n",
       "        3.7317e-04, 4.0710e-04, 4.4151e-04, 4.1320e-04, 4.1312e-04, 4.8740e-04,\n",
       "        4.6359e-04, 4.5889e-04, 6.4067e-04, 3.9224e-04, 3.6569e-04, 4.3446e-04,\n",
       "        4.7296e-04, 3.3661e-04, 3.3683e-04])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b1e2d7d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:39:40.897966Z",
     "start_time": "2023-04-27T09:39:40.890531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0b3f3165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:42:24.973028Z",
     "start_time": "2023-04-27T09:42:24.887726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = shifted_ldos_discrete(total_dos1, (s-200))\n",
    "t_get_mse(test_ldos0[:,:cutoff_index], new[:,:cutoff_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "5d51e278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:42:11.600077Z",
     "start_time": "2023-04-27T09:42:11.592601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(((test_ldos0[:,:cutoff_index]- new[:,:cutoff_index])**2).mean(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "da51a054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:43:43.614943Z",
     "start_time": "2023-04-27T09:43:43.606758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-200)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_range[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f0346beb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:32:11.274061Z",
     "start_time": "2023-04-27T09:32:11.260670Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s- 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109023bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:28:49.279Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_BF_shift_index_mse(prediction, true, shift_range, cutoff_index):\n",
    "    #shifts target instead of prediction\n",
    "    if len(prediction.shape) > 1:\n",
    "        shifted_true = shifted_ldos_discrete(true.repeat(shift_range.shape[0], 1, 1), shift_range)[:,:,:cutoff_index]\n",
    "        full_loss = t_get_each_mse(prediction.repeat(shift_range.shape[0], 1, 1), shifted_true)\n",
    "        full_loss = full_loss.reshape(shift_range.shape[0], -1)\n",
    "        index = torch.argmin(full_loss, dim = 0)\n",
    "        \n",
    "    else:\n",
    "        shifted_true = shifted_ldos_discrete(true.repeat(shift_range.shape[0], 1), shift_range)[:,:,:cutoff_index]\n",
    "        full_loss = t_get_each_mse(prediction.repeat(shift_range.shape[0], 1), shifted_true)\n",
    "        index = torch.argmin(full_loss, dim = 0)\n",
    "#         min_loss = full_loss[index]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f483cbd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:44:28.404835Z",
     "start_time": "2023-04-27T09:44:28.393673Z"
    }
   },
   "outputs": [],
   "source": [
    "def shifted_ldos_discrete(ldos, shift): \n",
    "    shifted_ldos = torch.zeros_like(ldos)\n",
    "    if len(ldos.shape) == 3:\n",
    "        for i in range(len(ldos)):\n",
    "            if shift[i] > 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i, :, :-1*shift[i]], (shift[i], 0))\n",
    "            elif shift[i] < 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i, :, (-1*shift[i]):], (0, (-1*shift[i])))\n",
    "            else:\n",
    "                shifted_ldos[i] = ldos[i]\n",
    "    elif len(ldos.shape) == 2:\n",
    "        xdos_shift = torch.round(shift).int()\n",
    "        for i in range(len(ldos)):\n",
    "            if xdos_shift[i] > 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,:-1*xdos_shift[i]], (xdos_shift[i],0))\n",
    "            elif xdos_shift[i] < 0:\n",
    "                shifted_ldos[i] = torch.nn.functional.pad(ldos[i,(-1*xdos_shift[i]):], (0,(-1*xdos_shift[i])))\n",
    "            else:\n",
    "                shifted_ldos[i] = ldos[i]\n",
    "    else:        \n",
    "        xdos_shift = int(torch.round(shift))\n",
    "        if xdos_shift > 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[:-1*xdos_shift], (xdos_shift,0))\n",
    "        elif xdos_shift < 0:\n",
    "            shifted_ldos = torch.nn.functional.pad(ldos[(-1*xdos_shift):], (0,(-1*xdos_shift)))\n",
    "        else:\n",
    "            shifted_ldos = ldos\n",
    "    return shifted_ldos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "447e16bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T15:18:36.732312Z",
     "start_time": "2023-04-24T15:18:36.728676Z"
    }
   },
   "outputs": [],
   "source": [
    "# alignment = torch.rand(927)\n",
    "# trueshift = torch.round(alignment/(xdos[1] - xdos[0])).int()\n",
    "# test_ldos0 = shifted_ldos(total_dos1, xdos, torch.ones(931))\n",
    "# optshift = find_optimal_discrete_shift(np.array(test_ldos0),np.array(total_dos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8a5fb5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T20:19:39.119276Z",
     "start_time": "2023-04-27T20:19:39.090255Z"
    }
   },
   "outputs": [],
   "source": [
    "def normal_reg_train_L(cutoff_index, shift_range, feat, target, train_index, test_index, regularization, n_epochs, lr):\n",
    "    \n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "    reg_features = torch.vstack([Features, reg])\n",
    "    reg_target = torch.vstack([Target[:,:cutoff_index], torch.zeros(n_col,cutoff_index)])\n",
    "    \n",
    "    alignment = torch.zeros(len(index))\n",
    "\n",
    "    weights = torch.nn.Parameter(torch.rand(Features.shape[1], cutoff_index)- 0.5)\n",
    "    \n",
    "    opt = torch.optim.LBFGS([weights], lr = lr, line_search_fn = \"strong_wolfe\", tolerance_grad = 1e-20, tolerance_change = 1-20, history_size = 200)\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            pred_i = reg_features @ weights\n",
    "            jitter = t_get_BF_shift_index_mse(pred_i[:len(index)], reg_target[:len(index)], shift_range, cutoff_index)\n",
    "            jitter += shift_range[0]\n",
    "            reg_target_i = reg_target.clone()\n",
    "            reg_target_i[:len(index)] = shifted_ldos_discrete(reg_target_i[:len(index)], jitter)\n",
    "            loss_i = t_get_mse(pred_i, reg_target_i[:, :cutoff_index])\n",
    "            \n",
    "            loss_i.backward()\n",
    "            return loss_i\n",
    "        opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            target = Target.clone()\n",
    "#             target = shifted_ldos_discrete(target, -1 * alignment)\n",
    "            jitter = t_get_BF_shift_index_mse(preds, target, shift_range, cutoff_index)\n",
    "            jitter += shift_range[0]\n",
    "            alignment = jitter\n",
    "            \n",
    "            target = Target.clone()\n",
    "            target = shifted_ldos_discrete(target, jitter)\n",
    "            \n",
    "            epoch_rmse = t_get_rmse(preds, target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "            epoch_mse = t_get_mse(preds, target[:,:cutoff_index], xdos[:cutoff_index])\n",
    "            \n",
    "            pred_loss = torch.mean(epoch_rmse)\n",
    "\n",
    "            if epoch_mse < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            epoch_mse = prev_loss\n",
    "\n",
    "\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         print (\"NOT FIXED YET\")\n",
    "#         final_preds = Features @ best_state \n",
    "#         final_t_preds = t_Features @ best_state\n",
    "\n",
    "#         shifted_true = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "#         final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "#         opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "#         final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "#         loss_dos = loss.t_get_rmse(final_preds, Target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "#         test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "    return best_state, alignment, 1.0, 1.0#, loss_dos, test_loss_dos\n",
    "        \n",
    "\n",
    "def normal_reg_train_Ad(cutoff_index, shift_range, feat, target, train_index, test_index, regularization, n_epochs, batch_size, lr):\n",
    "    patience = 20\n",
    "    index = train_index\n",
    "    t_index = test_index\n",
    "\n",
    "    features = torch.hstack([feat, torch.ones(feat.shape[0]).view(-1,1)])\n",
    "\n",
    "    Sampler = torch.utils.data.RandomSampler(index, replacement = False)\n",
    "    Batcher = torch.utils.data.BatchSampler(Sampler, batch_size, False)\n",
    "\n",
    "    Features = features[index]\n",
    "    t_Features = features[t_index]\n",
    "    n_col = Features.shape[1]\n",
    "\n",
    "\n",
    "    Target = target[index]\n",
    "    t_Target = target[t_index]\n",
    "\n",
    "\n",
    "    # reg_features = torch.vstack([Features, reg])\n",
    "    # reg_target = torch.vstack([Target, torch.zeros(n_col,Target.shape[1])])\n",
    "\n",
    "\n",
    "    reg = regularization * torch.eye(n_col)\n",
    "    reg[-1, -1] = 0\n",
    "    \n",
    "    alignment = torch.zeros(len(index))\n",
    "\n",
    "    weights = torch.nn.Parameter((torch.rand(Features.shape[1], cutoff_index)- 0.5))\n",
    "    opt = torch.optim.Adam([weights], lr = lr, weight_decay = 0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = 500, threshold = 1e-7, min_lr = 1e-8)\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    best_state = weights.clone()\n",
    "    current_rmse = torch.tensor(100)\n",
    "    pred_loss = torch.tensor(100)\n",
    "    prev_loss = torch.tensor(100)\n",
    "    best_mse = torch.tensor(100)\n",
    "    trigger = 0\n",
    "    for epoch in pbar:\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "        pbar.set_postfix(pred_loss = pred_loss.item(), lowest_mse = best_mse.item(), trigger = trigger)\n",
    "        for i_batch in Batcher:\n",
    "            def closure():\n",
    "                opt.zero_grad()\n",
    "                reg_features_i = torch.vstack([Features[i_batch], reg])\n",
    "                pred_i = reg_features_i @ weights\n",
    "#                 shifted_target_i = shifted_ldos_discrete(Target[i_batch].clone(), xdos, -1 * alignment)\n",
    "                loss_i, jitter = t_get_BF_shift_index_mse(pred_i[:len(i_batch)], Target[i_batch,:].clone(), shift_range, cutoff_index)\n",
    "                jitter += shift_range[0]\n",
    "        \n",
    "                shifted_target_i =  shifted_ldos_discrete(Target[i_batch].clone(), jitter)\n",
    "                target_i = torch.vstack([shifted_target_i[:,:cutoff_index], torch.zeros(n_col, cutoff_index)])\n",
    "                \n",
    "                loss_i = t_get_mse(pred_i, target_i)\n",
    "                loss_i.backward()\n",
    "                return loss_i\n",
    "            opt.step(closure)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = Features @ weights\n",
    "            loss_i, jitter = t_get_BF_shift_index_mse(preds, Target.clone(), shift_range, cutoff_index)\n",
    "            jitter += shift_range[0]\n",
    "            \n",
    "            shifted_target = shifted_ldos_discrete(Target.clone(), jitter)\n",
    "\n",
    "            epoch_rmse = t_get_rmse(preds, shifted_target[:,:cutoff_index], xdos[:cutoff_index], perc = True)\n",
    "            epoch_mse = t_get_mse(preds, shifted_target[:,:cutoff_index], xdos[:cutoff_index])\n",
    "\n",
    "\n",
    "            pred_loss = torch.mean(epoch_rmse)\n",
    "\n",
    "            if pred_loss < best_mse:\n",
    "                best_mse = epoch_mse\n",
    "                best_state = weights.clone()\n",
    "\n",
    "            if epoch_mse < prev_loss * ( 1 + 1e-3):\n",
    "                trigger =0\n",
    "            else:\n",
    "                trigger +=1 \n",
    "                if trigger >= patience:\n",
    "                    weights = best_state\n",
    "                    opt = torch.optim.Adam([weights], lr = opt.param_groups[0]['lr'], weight_decay = 0)\n",
    "\n",
    "            prev_loss = pred_loss\n",
    "\n",
    "            scheduler.step(epoch_mse)\n",
    "\n",
    "            if Batcher.batch_size > 1024:\n",
    "                break\n",
    "\n",
    "            if opt.param_groups[0]['lr'] < 1e-4:\n",
    "                Batcher.batch_size *= 2\n",
    "                opt.param_groups[0]['lr'] = lr\n",
    "                print (\"The batch_size is now: \", Batcher.batch_size)\n",
    "\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         final_preds = Features @ best_state \n",
    "#         final_t_preds = t_Features @ best_state\n",
    "\n",
    "#         opt_shift_train = find_optimal_discrete_shift(np.array(final_preds),np.array(Target))\n",
    "#         final_preds = shifted_ldos_discrete(final_preds, xdos, torch.tensor(opt_shift_train))\n",
    "#         opt_shift_test = find_optimal_discrete_shift(np.array(final_t_preds),np.array(t_Target))\n",
    "#         final_t_preds = shifted_ldos_discrete(final_t_preds, xdos, torch.tensor(opt_shift_test))\n",
    "\n",
    "#         loss_dos = loss.t_get_rmse(final_preds, Target, xdos, perc = True)\n",
    "#         test_loss_dos = loss.t_get_rmse(final_t_preds, t_Target, xdos, perc = True)\n",
    "    return best_state, alignment, 1.0, 1.0# loss_dos, test_loss_dos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "76309b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:42:56.198029Z",
     "start_time": "2023-04-27T08:42:56.190359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xdos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d770a109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:46:05.682484Z",
     "start_time": "2023-04-27T08:46:05.677015Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_index = torch.tensor(487)\n",
    "full_range = 200\n",
    "shift_range = torch.arange(2 * full_range + 1) - full_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc5987",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-28T14:26:36.487Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3687:   1%|█▎                                                                                                     | 3687/300000 [3:08:51<247:51:35,  3.01s/it, lowest_mse=0.0232, pred_loss=15.5, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9041:   3%|███                                                                                                    | 9041/300000 [6:51:34<196:46:54,  2.43s/it, lowest_mse=0.0232, pred_loss=14.9, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11524:   4%|███▉                                                                                                 | 11524/300000 [8:32:58<194:04:33,  2.42s/it, lowest_mse=0.0232, pred_loss=14.3, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17225:   6%|█████▋                                                                                              | 17225/300000 [12:39:31<208:53:25,  2.66s/it, lowest_mse=0.0232, pred_loss=13.7, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 82088:  27%|███████████████████████████▎                                                                        | 82088/300000 [57:11:43<147:50:11,  2.44s/it, lowest_mse=0.0232, pred_loss=13.3, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch_size is now:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 118184:  39%|██████████████████████████████████████▌                                                           | 118184/300000 [80:39:28<120:50:13,  2.39s/it, lowest_mse=0.0232, pred_loss=13.2, trigger=0]"
     ]
    }
   ],
   "source": [
    "weights, alignment, loss_dos, test_loss_dos = normal_reg_train_Ad(cutoff_index, shift_range, total_soap, total_aligned_dos3,\n",
    "                                                       total_train_index, total_test_index,\n",
    "                                                       1e-2, 300000, 16, 1e-3)\n",
    "print (\"Adam Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2934bc68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T11:20:40.951458Z",
     "start_time": "2023-04-27T11:16:42.159249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 39: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [03:58<00:00,  5.97s/it, lowest_mse=2.64e-5, pred_loss=40.9, trigger=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Unbiased\n",
      "The train error is 1.0 for SOAP\n",
      "The test error is 1.0 for SOAP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "U_L_weights3 , alignment, loss_dos, test_loss_dos = normal_reg_train_L(cutoff_index, shift_range, surface_soap, surface_dos3, surface_train_index, surface_test_index,\n",
    "                                                            1e-2, 40, 1)\n",
    "print (\"LBFGS Unbiased\")\n",
    "print (\"The train error is {:.4} for SOAP\".format(loss_dos))\n",
    "print (\"The test error is {:.4} for SOAP\".format(test_loss_dos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813544fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
