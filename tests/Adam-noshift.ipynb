{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84565b74",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdaeca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:56:56.480605Z",
     "start_time": "2023-02-22T16:56:52.870373Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import dostools\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import time\n",
    "torch.set_default_dtype(torch.float64) \n",
    "%matplotlib notebook\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 10)\n",
    "sys.modules['dostools.src'] = dostools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccbf26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:57:00.086543Z",
     "start_time": "2023-02-22T16:56:56.483942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldos shape is torch.Size([1039, 778])\n",
      "mean dos shape is torch.Size([778])\n",
      "Variance covered with 10 PCs is = 0.9871211778950163\n"
     ]
    }
   ],
   "source": [
    "import dostools.datasets.data as data\n",
    "import dostools.utils.utils as utils\n",
    "\n",
    "n_structures = 1039\n",
    "np.random.seed(0)\n",
    "n_train = int(0.8 * n_structures)\n",
    "train_index = np.arange(n_structures)\n",
    "np.random.shuffle(train_index)\n",
    "test_index = train_index[n_train:]\n",
    "train_index = train_index[:n_train]\n",
    "\n",
    "with torch.no_grad():\n",
    "    structures = data.load_structures(\":\")\n",
    "    n_structures = len(structures) #total number of structures\n",
    "    for structure in structures:#implement periodicity\n",
    "        structure.wrap(eps = 1e-12) \n",
    "    n_atoms = np.zeros(n_structures, dtype = int) #stores number of atoms in each structures\n",
    "    for i in range(n_structures):\n",
    "        n_atoms[i] = len(structures[i])\n",
    "\n",
    "    #eigenergies, emin, emax = dostools.src.datasets.data.load_eigenenergies(unpack = True, n_structures = len(structures))\n",
    "    xdos = torch.tensor(data.load_xdos())\n",
    "    ldos = torch.tensor(data.load_ldos())\n",
    "    ldos *= 2\n",
    "\n",
    "    print (\"ldos shape is {}\".format(ldos.shape))\n",
    "    mean_dos_per_atom = ldos[train_index].mean(axis = 0) #only calculated for train set to prevent data leakage\n",
    "    print (\"mean dos shape is {}\".format(mean_dos_per_atom.shape))\n",
    "    \n",
    "    \n",
    "    y_pw = ldos - mean_dos_per_atom\n",
    "    y_lcdf = torch.cumsum(y_pw, dim = 1)\n",
    "    _, pc_vectors = utils.build_pc(ldos[train_index], mean_dos_per_atom[None,:], n_pc = 10)\n",
    "    y_pc = utils.build_coeffs(ldos - mean_dos_per_atom[None,:], pc_vectors)\n",
    "    Silicon = data.load_features()\n",
    "    kMM = data.load_kMM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c7af2",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfe1751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:57:00.146021Z",
     "start_time": "2023-02-22T16:57:00.090994Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.evaluation.evaluation as evaluation\n",
    "importlib.reload(evaluation)\n",
    "import dostools.models.training as training\n",
    "importlib.reload(training)\n",
    "\n",
    "targets = {\n",
    "    'pw' : ldos,\n",
    "    'lcdf' : y_lcdf,\n",
    "    'pc' : y_pc\n",
    "}\n",
    "evaluator = evaluation.Evaluator(targets, xdos, mean_dos_per_atom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2997b794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:58:07.926919Z",
     "start_time": "2023-02-22T16:58:07.866280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.89371661e-15, 2.91656217e-15, 2.89787701e-15, 1.70171194e-17,\n",
       "       1.77631607e-17, 1.71690667e-17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.GetTargetRMSE(y_pw, \"pw\", train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ebc76",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7efa3cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:33:03.340699Z",
     "start_time": "2023-02-15T18:33:03.313675Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.datasets.dataset as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import dostools.consistency.consistency as consistency\n",
    "\n",
    "device = 'cpu'\n",
    "kwargs = {\"pin_memory\":True} if device == \"cuda:0\" else {}\n",
    "#Dataset\n",
    "y_shifted = copy.deepcopy(y_pw)\n",
    "#y_shifted[train_index][:100] = consistency.shifted_ldos(y_shifted[:100], xdos, torch.zeros(100)-10)\n",
    "train_data_soap = TensorDataset(Silicon.Features[\"structure_avedescriptors\"][train_index].double(), y_shifted[train_index].double())\n",
    "train_data_kernel = TensorDataset(Silicon.Features[\"structure_avekerneldescriptors\"][train_index].double(), y_shifted[train_index].double())\n",
    "\n",
    "test_data_soap = TensorDataset(Silicon.Features[\"structure_avedescriptors\"][test_index].double(), y_shifted[test_index].double())\n",
    "test_data_kernel = TensorDataset(Silicon.Features[\"structure_avekerneldescriptors\"][test_index].double(), y_shifted[test_index].double())\n",
    "\n",
    "#Dataloader\n",
    "\n",
    "train_dataloader_soap = DataLoader(train_data_soap, batch_size = len(train_data_soap), shuffle = False, **kwargs)\n",
    "train_dataloader_kernel = DataLoader(train_data_kernel, batch_size = len(train_data_kernel), shuffle = False, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774fe1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:09:26.245547Z",
     "start_time": "2023-02-22T17:09:26.192708Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.datasets.dataset as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import dostools.consistency.consistency as consistency\n",
    "\n",
    "\n",
    "soap_mean = torch.mean(Silicon.Features[\"structure_avedescriptors\"][train_index], dim = 0)\n",
    "soap_total = Silicon.Features[\"structure_avedescriptors\"] - soap_mean\n",
    "\n",
    "kernel_mean = torch.mean(Silicon.Features[\"structure_avekerneldescriptors\"][train_index], dim = 0)\n",
    "kernel_total = Silicon.Features[\"structure_avekerneldescriptors\"] - kernel_mean\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "kwargs = {\"pin_memory\":True} if device == \"cuda:0\" else {}\n",
    "#Dataset\n",
    "y_shifted = copy.deepcopy(y_pw)\n",
    "#y_shifted[train_index][:100] = consistency.shifted_ldos(y_shifted[:100], xdos, torch.zeros(100)-10)\n",
    "train_data_soap = TensorDataset(soap_total[train_index].double(), y_shifted[train_index].double())\n",
    "train_data_kernel = TensorDataset(kernel_total[train_index].double(), y_shifted[train_index].double())\n",
    "\n",
    "test_data_soap = TensorDataset(soap_total[test_index].double(), y_shifted[test_index].double())\n",
    "test_data_kernel = TensorDataset(kernel_total[test_index].double(), y_shifted[test_index].double())\n",
    "\n",
    "#Dataloader\n",
    "\n",
    "train_dataloader_soap = DataLoader(train_data_soap, batch_size = 8, shuffle = False, **kwargs)\n",
    "train_dataloader_kernel = DataLoader(train_data_kernel, batch_size = 8, shuffle = False, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7691fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T16:19:17.672020Z",
     "start_time": "2023-01-27T16:19:17.669361Z"
    }
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd3ea42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:09:30.726205Z",
     "start_time": "2023-02-22T17:09:30.706814Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.src.consistency.consistency as consistency\n",
    "import dostools.src.loss.loss as loss\n",
    "importlib.reload(loss)\n",
    "importlib.reload(consistency)\n",
    "\n",
    "def t_get_mse(a, b, xdos = None, perc = False):\n",
    "    if xdos is not None:\n",
    "        mse = (torch.trapezoid((a - b)**2, xdos, axis=1)).mean()\n",
    "        if not perc:\n",
    "            return mse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.sqrt(torch.trapezoid((b - mean)**2, xdos, axis=1)).mean()\n",
    "            return (100 * mse / std)\n",
    "    else:\n",
    "        mse = ((a - b)**2).mean(dim = 1)\n",
    "        if len(mse.shape) > 1:\n",
    "            raise ValueError('Loss became 2D')\n",
    "        if not perc:\n",
    "            return torch.mean(mse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (mse / b.std(dim=0, unbiased = True)),0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfaf523",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a54f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:09:33.195884Z",
     "start_time": "2023-02-22T17:09:33.184061Z"
    }
   },
   "outputs": [],
   "source": [
    "def t_get_rmse(a, b, xdos=None, perc=False): #account for the fact that DOS is continuous but we are training them pointwise\n",
    "    \"\"\" computes  Root Mean Squared Error (RMSE) of array properties (DOS/aofd).\n",
    "         a=pred, b=target, xdos, perc: if False return RMSE else return %RMSE\"\"\"\n",
    "    #MIGHT NOT WORK FOR PC\n",
    "    if xdos is not None:\n",
    "        rmse = torch.sqrt(torch.trapezoid((a - b)**2, xdos, axis=1)).mean()\n",
    "        if not perc:\n",
    "            return rmse\n",
    "        else:\n",
    "            mean = b.mean(axis = 0)\n",
    "            std = torch.sqrt(torch.trapezoid((b - mean)**2, xdos, axis=1)).mean()\n",
    "            return (100 * rmse / std)\n",
    "    else:\n",
    "        rmse = torch.sqrt(((a - b)**2).mean(dim =0))\n",
    "        if not perc:\n",
    "            return torch.mean(rmse, 0)\n",
    "        else:\n",
    "            return torch.mean(100 * (rmse / b.std(dim = 0,unbiased=True)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6462d",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5bb960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:15:32.407846Z",
     "start_time": "2023-02-22T17:15:32.378010Z"
    }
   },
   "outputs": [],
   "source": [
    "import dostools.src.models.models as models\n",
    "import dostools.src.models.training as training\n",
    "import dostools.src.models.architectures as architecture\n",
    "import dostools.src.loss.loss as loss\n",
    "import torch.nn as nn\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(training)\n",
    "importlib.reload(architecture)\n",
    "importlib.reload(loss)\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, xdos, device):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(inputSize, outputSize, bias = False)\n",
    "        self.xdos = xdos\n",
    "        self.device = device\n",
    "        #self.alignment = torch.zeros(train_size, device = self.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the transformations to the features based on the model\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): input features\n",
    "        \n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f298779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:34:14.274520Z",
     "start_time": "2023-02-22T17:34:14.261843Z"
    }
   },
   "outputs": [],
   "source": [
    "soap_model = LinearModel(448, 778, xdos, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e40a39ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T21:35:08.192959Z",
     "start_time": "2023-02-22T18:32:16.032652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21873:  22%|████████████████████▌                                                                         | 21873/100000 [3:02:52<10:53:10,  1.99it/s, lowest_loss=4.79e-5, pred_loss=2.96e-5, trigger=1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 4.7857860638692426e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "n_epochs = 100000\n",
    "\n",
    "opt = torch.optim.Adam(soap_model.parameters(), lr = lr, weight_decay = 0)\n",
    "threshold = 1000\n",
    "scheduler_threshold = 100\n",
    "tol = 1e-7\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = scheduler_threshold)#0.5)\n",
    "best_state = copy.deepcopy(soap_model.state_dict())\n",
    "lowest_loss = torch.tensor(9999)\n",
    "pred_loss = torch.tensor(0)\n",
    "trigger = 0\n",
    "loss_history =[]\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(f\"Epoch: {epoch}\")\n",
    "    pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "\n",
    "    for x_data, y_data in train_dataloader_soap:\n",
    "        opt.zero_grad()\n",
    "        x_data, y_data = x_data.to(soap_model.device), y_data.to(soap_model.device)\n",
    "        pred = soap_model.forward(x_data)\n",
    "        pred_loss = t_get_mse(pred, y_data)#, self.xdos, perc = True)\n",
    "        new_loss = 1E7 * pred_loss\n",
    "        new_loss.backward()\n",
    "        opt.step()\n",
    "        if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "            print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "            soap_model.load_state_dict(best_state)\n",
    "            opt = torch.optim.Adam(soap_model.parameters(), lr = lr, weight_decay = 0)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_pred = soap_model.forward(train_dataloader_soap.dataset.tensors[0])\n",
    "        total_loss = t_get_mse(total_pred, train_dataloader_soap.dataset.tensors[1])\n",
    "        rel_loss = t_get_rmse(total_pred, train_dataloader_soap.dataset.tensors[1], soap_model.xdos, perc = True)\n",
    "        new_loss = total_loss\n",
    "        if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "            best_state = copy.deepcopy(soap_model.state_dict())\n",
    "            lowest_loss = new_loss\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger +=1\n",
    "\n",
    "        if trigger > threshold:\n",
    "            soap_model.load_state_dict(best_state)\n",
    "            print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "            break\n",
    "        if epoch %1000 == 1:\n",
    "            loss_history.append(lowest_loss.item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9ea0ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T21:35:08.259237Z",
     "start_time": "2023-02-22T21:35:08.199107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.34178037 13.08973676  8.87244966  0.04317491  0.07972232  0.05256665]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    soap_preds = soap_model(soap_total)\n",
    "    RMSES = evaluator.GetTargetRMSE(soap_preds, \"pw\", train_index, test_index)\n",
    "    print (RMSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83f7794b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T21:35:08.302332Z",
     "start_time": "2023-02-22T21:35:08.261465Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_model = LinearModel(1000, 778, xdos, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efb41383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:04:32.417676Z",
     "start_time": "2023-02-22T21:35:08.305400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2339:   2%|██▏                                                                                             | 2339/100000 [29:24<20:27:36,  1.33it/s, lowest_loss=0.000539, pred_loss=0.000456, trigger=1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.0005390271828621282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "n_epochs = 100000\n",
    "\n",
    "opt = torch.optim.Adam(kernel_model.parameters(), lr = lr, weight_decay = 0)\n",
    "threshold = 1000\n",
    "scheduler_threshold = 100\n",
    "tol = 1e-4\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor = 0.1, patience = scheduler_threshold)#0.5)\n",
    "best_state = copy.deepcopy(kernel_model.state_dict())\n",
    "lowest_loss = torch.tensor(9999)\n",
    "pred_loss = torch.tensor(0)\n",
    "trigger = 0\n",
    "loss_history =[]\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(f\"Epoch: {epoch}\")\n",
    "    pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "\n",
    "    for x_data, y_data in train_dataloader_kernel:\n",
    "        opt.zero_grad()\n",
    "        x_data, y_data = x_data.to(kernel_model.device), y_data.to(kernel_model.device)\n",
    "        pred = kernel_model.forward(x_data)\n",
    "        pred_loss = t_get_mse(pred, y_data)#, self.xdos, perc = True)\n",
    "        new_loss = 1E7 * pred_loss\n",
    "        new_loss.backward()\n",
    "        opt.step()\n",
    "        if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "            print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "            kernel_model.load_state_dict(best_state)\n",
    "            opt = torch.optim.Adam(kernel_model.parameters(), lr = lr, weight_decay = 0)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_pred = kernel_model.forward(train_dataloader_kernel.dataset.tensors[0])\n",
    "        total_loss = t_get_mse(total_pred, train_dataloader_kernel.dataset.tensors[1])\n",
    "        rel_loss = t_get_rmse(total_pred, train_dataloader_kernel.dataset.tensors[1], kernel_model.xdos, perc = True)\n",
    "        new_loss = total_loss\n",
    "        if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "            best_state = copy.deepcopy(kernel_model.state_dict())\n",
    "            lowest_loss = new_loss\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger +=1\n",
    "\n",
    "        if trigger > threshold:\n",
    "            kernel_model.load_state_dict(best_state)\n",
    "            print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "            break\n",
    "        if epoch %1000 == 1:\n",
    "            loss_history.append(lowest_loss.item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7ecdab0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T22:04:32.464211Z",
     "start_time": "2023-02-22T22:04:32.420434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.6393655  23.17641564 24.33127686  0.14489706  0.14115468  0.14415564]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    kernel_preds = kernel_model(kernel_total)\n",
    "    Kernel_RMSES = evaluator.GetTargetRMSE(kernel_preds, \"pw\", train_index, test_index)\n",
    "    print (Kernel_RMSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bef2ce2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T17:10:30.468695Z",
     "start_time": "2023-02-16T17:10:30.328325Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399319b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "759px",
    "left": "25px",
    "top": "159px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
